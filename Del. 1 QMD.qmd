---
title: "1. Del Raw data & Transformation"
author: "Jan Maurycy Pedersen, Jolene Kaye Jensen, Lukas Beslic Christiansen"

format:
  pdf:
    toc: true
    toc-depth: 4
    number-sections: true

execute:
  eval: false
  message: false
  warning: false
  error: false

editor: visual
---

# Datagrundlag og data management

Dette dokument beskriver de centrale datah√•ndterings- og transformationsprocesser, der danner grundlag for projektets efterf√∏lgende analyser og modeller. Fokus er rettet mod dokumentation af datagrundlag, behandlingslogik og kvalitetssikring frem for fuld teknisk reproducerbarhed. Dette skyldes, at v√¶sentlige dele af datagrundlaget er placeret i et adgangsbegr√¶nset og betalt SQL-baseret datalager, som suppleres af eksterne offentlige datakilder.

Af hensyn til databeskyttelse, systemadgang og omkostningsstyring stilles der ikke direkte adgang til SQL-databasen eller det samlede r√• datagrundlag til r√•dighed. Dokumentationen skal derfor l√¶ses som en faglig og metodisk redeg√∏relse for, hvordan data indsamles, behandles, transformeres og klarg√∏res til analyse. Redeg√∏relsen underst√∏ttes af dokumenterede resultater, strukturelle outputs og beskrivende eksempler, hvor det er relevant.

## PBA01_Raw

I dette afsnit pr√¶senteres opbygningen af projektets datagrundlag, som er struktureret efter principper inspireret af klassisk data management og data warehousing. I stedet for at arbejde direkte p√• webscrapede tabeller og ad hoc-datas√¶t er datagrundlaget opdelt i tydelige og adskilte lag, der hver har et klart og afgr√¶nset form√•l i det samlede dataflow.

Databasen er organiseret i tre overordnede schemaer: PBA01_Raw, PBA02_Clean og PBA03_JoinReady. RAW-laget fungerer som et landingslag, hvor data gemmes t√¶t p√• kilden og uden analytisk fortolkning. CLEAN-laget anvendes til n√∏dvendig rensning, standardisering og validering af data, mens JOIN-READY-laget indeholder datas√¶t, der er struktureret og konsolideret med henblik p√• analyse og modellering.

Denne lagdelte struktur muligg√∏r en robust og transparent datah√•ndtering, hvor fejl kan isoleres, enkelte trin kan genk√∏res, og √¶ndringer i eksterne datakilder h√•ndteres uden at p√•virke hele datagrundlaget. Samtidig underst√∏tter arkitekturen en klar adskillelse mellem r√• data, behandlet data og analyseklar data, hvilket er centralt i et projekt med b√•de kvantitative analyser og maskinl√¶ringsbaserede modeller.

### Webscrapping af Superligaens kampprogram ([superstats.dk](https://superstats.dk/program?season=))

Denne kode udg√∏r et fuldt RAW-scrape af Superligaens kampprogram direkte fra SuperStats‚Äô HTML-sider. Fokus er bevidst p√• at udtr√¶kke data s√• t√¶t p√• kilden som muligt, med kun minimal teknisk strukturering, s√• alle fortolkninger, beregninger og joins kan foretages konsekvent i senere clean, join-lag, og analyse lag.

#### Pakker

Dette afsnit indl√¶ser de n√∏dvendige R-pakker til webscraping og basal datamanipulation. Pakkerne er valgt ud fra faktisk anvendelse i dette RAW-scrape for at sikre et let, gennemsigtigt og reproducerbart setup.

```{r}
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  
  pacman::p_load(
    rvest, dplyr, stringr, purrr, tibble
  )
});  cat("Pakker klar og loadedüôÇ\n")
```

#### Hj√¶lpefunktion: RAW-parse af √©n runde-tabel

Denne funktion udtr√¶kker data fra √©n HTML-tabel for en given runde og gemmer felterne i et RAW-format t√¶t p√• kilden. Der foretages kun minimal strukturering (fx udtr√¶k af runde-nummer og trim af tekst), mens al egentlig fortolkning og konvertering flyttes til senere clean- og join-lag.

```{r}
# ===================================================================
# 1) Hj√¶lpefunktion til at parse √âN runde-tabel (RAW-niveau)
# =============================================================================
# VIGTIGT: Dette er et RAW-parse:
#  - Vi tr√¶kker kun det ud, som fremg√•r direkte af HTML-tabellen.
#  - Vi laver KUN den manipulation, der er n√∏dvendig for overhovedet
#    at f√• data ud i kolonner (ikke beregninger som ugedag, dato-objekter osv.).
#
# Output-kolonner (t√¶t p√• kildens struktur):
#  - season_value   : s√¶son-parameteren fra URL‚Äôen (fx "2000")
#  - runde_header   : r√• tekst i tabel-header, fx "Runde 1"
#  - runde_nr       : selve runde-nummeret (int) udtrukket fra header
#  - ugedag_raw     : tekst som st√•r i f√∏rste kolonne (mandag/tirsdag/ el.lign.)
#  - dato_tid_raw   : ren tekst fra 2. kolonne (fx "22/07 15:30")
#  - hold_raw       : tekst fra 3. kolonne (fx "BIF-AGF") ‚Äì vi splitter IKKE her
#  - resultat_raw   : tekst/score fra 4. kolonne (fx "2-1") ‚Äì ingen split til m√•l
#  - tilskuertal_raw: r√• tekst fra 5. kolonne (fx "12.345") ‚Äì ingen konvertering
#  - dommer_raw     : dommertekst fra 6. kolonne
#  - tv_kanal_raw   : alt-tekst p√• TV-logoet, hvis det findes ‚Äì ellers NA

parse_runde_tabel_raw <- function(tbl_node, season_value) {
  
  # ---------------------------------------------------------------------------
  # L√¶s runde-overskriften fra thead
  # ---------------------------------------------------------------------------
  runde_header <- tbl_node %>%
    html_element("thead tr th") %>%
    html_text2() %>%
    str_squish()
  
  # Udtr√¶k runde-nummer som heltal, fx "Runde 1" ‚Üí 1
  runde_nr <- str_extract(runde_header, "\\d+") %>% as.integer()
  
  # ---------------------------------------------------------------------------
  # Find alle r√¶kker i tbody
  # ---------------------------------------------------------------------------
  rows <- tbl_node %>% html_elements("tbody tr")
  if (length(rows) == 0) return(tibble())
  
  # ---------------------------------------------------------------------------
  # Map over hver r√¶kke og tr√¶k r√• v√¶rdier ud
  # ---------------------------------------------------------------------------
  map_dfr(rows, function(row) {
    
    # Alle celler i r√¶kken
    tds <- row %>% html_elements("td")
    
    # Vi forventer mindst 6 kolonner (ugedag, dato/tid, hold, resultat,
    # tilskuertal, dommer). TV-kolonne er typisk nr. 7 og kan mangle.
    if (length(tds) < 6) return(tibble())
    
    # R√• tekster, direkte fra HTML-tabellen, kun trimmet for whitespace
    ugedag_raw   <- html_text2(tds[1]) %>% str_squish()
    dato_tid_raw <- html_text2(tds[2]) %>% str_squish()
    hold_raw     <- html_text2(tds[3]) %>% str_squish()
    
    # Resultat ligger ofte i en <a> inde i 4. kolonne
    res_node <- tds[4] %>% html_element("a")
    resultat_raw <- if (!is.null(res_node)) {
      res_node %>% html_text2() %>% str_squish()
    } else {
      html_text2(tds[4]) %>% str_squish()
    }
    
    tilskuertal_raw <- html_text2(tds[5]) %>% str_squish()
    dommer_raw      <- html_text2(tds[6]) %>% str_squish()
    
    # TV-kanal (kan mangle) ‚Äì vi gemmer alt-teksten p√• billedet som RAW
    tv_node <- tryCatch(
      tds[7] %>% html_element("img"),
      error = function(e) NULL
    )
    
    tv_kanal_raw <- if (!is.null(tv_node)) {
      html_attr(tv_node, "alt") %>% str_squish()
    } else {
      NA_character_
    }
    
    # Returner √©n r√¶kke som tibble ‚Äì 100 % RAW + let strukturering
    tibble(
      season_value    = season_value,   # parameteren vi scraper s√¶sonen med
      runde_header    = runde_header,   # fuld header-tekst
      runde_nr        = runde_nr,       # ekstra hj√¶lp senere, men stadig direkte afledt
      ugedag_raw      = ugedag_raw,
      dato_tid_raw    = dato_tid_raw,
      hold_raw        = hold_raw,
      resultat_raw    = resultat_raw,
      tilskuertal_raw = tilskuertal_raw,
      dommer_raw      = dommer_raw,
      tv_kanal_raw    = tv_kanal_raw
    )
  })
}
```

#### Scrape af √©n s√¶son i RAW-format

Denne funktion henter kampprogrammet for en given s√¶son fra SuperStats, identificerer alle runde-tabeller p√• siden og parser dem systematisk til et samlet RAW-datas√¶t. Udv√¶lgelsen sker udelukkende p√• baggrund af tabelheaders, s√• strukturen f√∏lger kilden t√¶t uden fortolkning.

```{r}

# ====================================================================
# Scrape √©n s√¶son (RAW)
# ====================================================================
# Denne funktion:
#  - bygger URL til en bestemt s√¶son,
#  - finder alle tabeller,
#  - v√¶lger dem, hvis headeren matcher "Runde X",
#  - parser hver runde-tabel via parse_runde_tabel_raw.

scrape_superstats_saeson_raw <- function(season_value) {
  
  url <- paste0("https://superstats.dk/program?season=", season_value)
  cat("Henter s√¶son", season_value, "fra", url, "\n")
  
  pg <- read_html(url)
  
  # Find alle <table>-elementer p√• siden
  alle_tabeller <- pg %>% html_elements("table")
  
  # Udv√¶lg kun dem, hvor f√∏rste <th> i thead starter med "Runde <tal>"
  er_runde <- vapply(
    alle_tabeller,
    function(tb) {
      head_node <- tb %>% html_element("thead tr th")
      
      # Hvis der ikke er noget header-node, er det ikke en runde-tabel
      if (is.null(head_node) || length(head_node) == 0) return(FALSE)
      
      txt <- tryCatch(
        head_node %>% html_text2() %>% str_squish(),
        error = function(e) ""
      )
      
      str_detect(txt, "^Runde\\s+\\d+")
    },
    logical(1)
  )
  
  runde_tabeller <- alle_tabeller[er_runde]
  
  if (length(runde_tabeller) == 0) {
    cat("Ingen runde-tabeller fundet for s√¶son", season_value, "\n")
    return(tibble())
  }
  
  # Parse alle runde-tabeller for denne s√¶son og bind dem sammen
  map_dfr(runde_tabeller, ~parse_runde_tabel_raw(.x, season_value))
}

```

#### Samling af RAW-data p√• tv√¶rs af s√¶soner

I dette trin k√∏res scrape-funktionen for alle valgte s√¶soner, hvorefter resultaterne samles i √©t samlet RAW-datas√¶t. Afslutningsvis foretages et simpelt struktur- og indholdstjek for at validere, at data er indl√¶st korrekt.

```{r}

# ====================================================================
# Loop over flere s√¶soner ‚Üí samlet RAW-datas√¶t
# ====================================================================

# Her v√¶lger du selv hvilke s√¶soner, du vil have med.
# Eksempel: 2000‚Äì2026 som i din oprindelige kode.
saesoner_vec <- 2000:2026

superliga_program_raw <- map_dfr(saesoner_vec, scrape_superstats_saeson_raw)

cat("\nSamlet antal r√¶kker i superliga_program_raw:", nrow(superliga_program_raw), "\n\n")

# Hurtig struktur- og indholdstjek i R:
# glimpse(superliga_program_raw)
str(superliga_program_raw)


```

![](images/2026-01-03_23h32_18.png)

#### Upload af kampprogram til RAW-laget i Azure SQL

Her uploades det samlede RAW-scrape til Azure SQL som en faktatabel i schemaet PBA01_Raw, s√• data lander centralt og kan genbruges i de efterf√∏lgende lag. Uploaden sker med fuld overskrivning for at sikre en konsistent version, og der kan efterf√∏lgende laves et hurtigt sanity check direkte fra databasen.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Upload til Azure som RAW-fact (PBA01_Raw)
# ====================================================================
# Nu hvor vi har et rent RAW-scrape i superliga_program_raw, skal det op som
# en RAW-tabel i Azure SQL. Det passer ind i din lagdeling:
#  - PBA01_Raw     : landings-/RAW-lag (ingen business-logik, minimal parsing)
#  - PBA02_Clean   : rensning, parsing af datoer, split af hold, m√•l osv.
#  - PBA03_JoinReady / marts-lag: join med billetdata, vejr, dim_dato, osv.
#
# Vi bruger samme connection-setup som i de andre scripts, men skriver eksplicit
# til schema = "PBA01_Raw".

library(DBI)
library(odbc)

con_azure <- dbConnect(
  odbc::odbc(),
  driver   = "ODBC Driver 18 for SQL Server",
  server   = Sys.getenv("AZURE_SQL_SERVER"),
  database = Sys.getenv("AZURE_SQL_DB"),
  uid      = Sys.getenv("AZURE_SQL_UID"),
  pwd      = Sys.getenv("AZURE_SQL_PWD"),
  port     = 1433,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  ConnectionTimeout      = 30
)

cat("Forbundet til Azure (", Sys.getenv("AZURE_SQL_DB"), ") ‚Äì skriver til PBA01_Raw\n\n", sep = "")

# Skriv/overskriv RAW-tabellen
DBI::dbWriteTable(
  conn      = con_azure,
  name      = DBI::Id(schema = "PBA01_Raw", table = "fact_superliga_program_raw"),
  value     = superliga_program_raw,
  overwrite = TRUE
)

cat("Tabel PBA01_Raw.fact_superliga_program_raw er nu opdateret.\n")

# (valgfrit) lille sanity check fra databasen
head_db <- DBI::dbReadTable(
  con_azure,
  DBI::Id(schema = "PBA01_Raw", table = "fact_superliga_program_raw")
)
print(head_db)

dbDisconnect(con_azure)
cat("üîö Forbindelse til Azure lukket.\n")

```

### Webscrapping VFF rundeplacering samlet pipeline

Dette script afviger bevidst fra vores normale lagdeling, fordi vi her har prioriteret en enkel og driftssikker ‚Äú√©n-knaps‚Äù-pipeline. Det scraper rundeplaceringer, gemmer dem i RAW (overwrite), transformer dem til CLEAN-format (overwrite) og bygger til sidst en kamp-n√∏glebaseret feature-tabel til PBA03_JoinReady, s√• den kan joines direkte p√• baseline via kamp_id.

#### Pakker

```{r}

suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(
    rvest, dplyr, tidyr, stringr, purrr, tibble,
    DBI, odbc, rstudioapi
  )
});cat("Pakker klar og loadedüôÇ\n")

```

#### Safeguard .Renviron (login-check)

Afsnittet validerer, at alle n√∏dvendige Azure SQL‚Äìmilj√∏variabler findes, f√∏r scriptet forts√¶tter. Hvis noget mangler, stoppes k√∏rslen tidligt, s√• vi undg√•r fejlk√∏rsler og uklare forbindelsesfejl senere.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Safeguard .Renviron
# ====================================================================
required_vars <- c("AZURE_SQL_SERVER", "AZURE_SQL_DB", "AZURE_SQL_UID", "AZURE_SQL_PWD")
values <- Sys.getenv(required_vars)
missing_vars <- required_vars[values == ""]

if (length(missing_vars) > 0) {
  cat("‚ö†Ô∏è F√∏lgende milj√∏variabler mangler i .Renviron:\n")
  print(missing_vars)
  cat("\n√Öbner ~/.Renviron ‚Äì udfyld v√¶rdierne og gem filen.\n")
  file.edit("~/.Renviron")
  if (rstudioapi::isAvailable()) rstudioapi::restartSession()
  stop("Stopper her. Genstart R og k√∏r scriptet igen.")
} else {
  cat("ü´° Alle n√∏dvendige .Renviron-variabler er sat.\n\n")
}

```

#### Funktion: hent VFF‚Äôs rundeplaceringer pr. s√¶son

Denne funktion scraper SuperStats for √©n s√¶son og finder den tabelr√¶kke, der matcher ‚ÄúVFF‚Äù. Output gemmes i et ‚Äúwide‚Äù RAW-format (Runde‚ÇÅ‚Ä¶Runde‚Çô) med s√¶sonmetadata, s√• vi senere kan oms√¶tte det til CLEAN/FEATURE uden at gen-scrape.

```{r}
# ====================================================================
# Funktion: hent VFF placering pr. runde for √©n s√¶son (RAW wide)
# ====================================================================
hent_vff_alle_runder <- function(aarstal) {
  
  url <- paste0(
    "https://superstats.dk/stilling/pladser-runde?season=",
    aarstal,
    "&klub=vff"
  )
  
  cat("üåê Scraper season_code:", aarstal, "‚Äî URL:", url, "\n")
  
  tryCatch({
    
    webpage <- read_html(url)
    
    # 2A) l√¶s s√¶son-label fra <h2>
    h2_node <- webpage %>% html_element("h2")
    if (length(h2_node) == 0) {
      cat("  ‚ùå Ingen <h2> fundet. Springer over.\n")
      return(NULL)
    }
    
    h2_txt <- h2_node %>% html_text2() %>% str_squish()
    saeson_label <- str_extract(h2_txt, "\\d{4}/\\d{4}")
    saeson_url   <- if (!is.na(saeson_label)) str_replace(saeson_label, "/", "%2F") else NA_character_
    
    if (is.na(saeson_label)) {
      cat("  ‚ùå Kunne ikke udlede s√¶son-label. Springer over.\n")
      return(NULL)
    }
    
    # 2B) l√¶s alle tabeller
    tabeller <- html_table(webpage, header = FALSE, fill = TRUE)
    if (length(tabeller) == 0) {
      cat("  ‚ùå Ingen tabeller fundet. Springer over.\n")
      return(NULL)
    }
    
    # 2C) find tabel som indeholder "VFF"
    idx <- which(purrr::map_lgl(
      tabeller,
      function(tbl) {
        cell_values <- tbl %>%
          mutate(across(everything(), as.character)) %>%
          unlist(use.names = FALSE) %>%
          str_squish()
        any(cell_values == "VFF")
      }
    ))
    
    if (length(idx) == 0) {
      cat("  ‚ÑπÔ∏è Ingen tabel med VFF (VFF ikke i ligaen?). Springer over.\n")
      return(NULL)
    }
    
    data <- tabeller[[idx[1]]]
    first_col_name <- names(data)[1]
    
    vff_row <- data %>%
      mutate(Klub = str_squish(as.character(.data[[first_col_name]]))) %>%
      filter(Klub == "VFF")
    
    if (nrow(vff_row) == 0) {
      cat("  ‚ÑπÔ∏è VFF-r√¶kke ikke fundet i valgt tabel. Springer over.\n")
      return(NULL)
    }
    
    # 2D) fjern klub-kolonne, cast til numeric
    vff_round_data <- vff_row[, -1, drop = FALSE] %>%
      mutate(across(everything(), ~ as.numeric(str_squish(as.character(.x)))))
    
    # 2E) navngiv Runde_1..Runde_n
    new_names <- paste0("Runde_", seq_len(ncol(vff_round_data)))
    colnames(vff_round_data) <- new_names
    
    # 2F) add s√¶son-info
    out <- vff_round_data %>%
      mutate(
        SeasonCode         = aarstal,
        Saeson_label       = saeson_label,
        Saeson_url_encoded = saeson_url
      ) %>%
      relocate(SeasonCode, Saeson_label, Saeson_url_encoded)
    
    cat("  ‚úÖ OK:", saeson_label, "(runder:", ncol(vff_round_data), ")\n")
    out
    
  }, error = function(e) {
    cat("  ‚ùå Fejl i season_code", aarstal, ":", e$message, "\n")
    NULL
  })
}
```

#### Scrape alle s√¶soner til √©t samlet RAW-datas√¶t

Her looper vi alle s√¶sonkoder (2000‚Äì2026) og kalder scrape-funktionen for hver s√¶son, hvorefter resultaterne bindes sammen til √©n RAW wide-tabel. Til sidst logger vi antal r√¶kker/kolonner og stopper scriptet, hvis der ikke blev hentet data (fail-fast).

```{r}
#| warning: false
# ====================================================================
# Scrape alle s√¶soner
# ====================================================================
season_koder <- 2000:2026

cat("\n--- Starter scraping af VFF placering pr. runde ---\n")

vff_runde_placering_raw_wide <- season_koder %>%
  map(hent_vff_alle_runder) %>%
  list_rbind()

cat("\n--- F√¶rdig med scraping ---\n")
cat("‚úÖ RAW wide r√¶kker:", nrow(vff_runde_placering_raw_wide), "\n")
cat("‚úÖ RAW wide kolonner:", ncol(vff_runde_placering_raw_wide), "\n\n")

if (nrow(vff_runde_placering_raw_wide) == 0) {
  stop("Ingen RAW-data scraped. Stopper her.")
}; str(vff_runde_placering_raw_wide)
```

#### Forbindelse til Azure SQL

Her oprettes √©n f√¶lles databaseforbindelse, som genbruges i resten af scriptet. Det sikrer en stabil og kontrolleret adgang til Azure SQL under hele eksekveringen.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Connect til Azure (√©n gang)
# ====================================================================
cat("üîå Forbinder til Azure SQL\n")

con <- DBI::dbConnect(
  odbc::odbc(),
  driver   = "ODBC Driver 18 for SQL Server",
  server   = Sys.getenv("AZURE_SQL_SERVER"),
  database = Sys.getenv("AZURE_SQL_DB"),
  uid      = Sys.getenv("AZURE_SQL_UID"),
  pwd      = Sys.getenv("AZURE_SQL_PWD"),
  port     = 1433,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  ConnectionTimeout      = 180
)

cat("‚úî Forbundet til:", Sys.getenv("AZURE_SQL_DB"), "\n\n")

```

#### Overwrite af RAW-laget

Her overskrives RAW-tabellen med det fulde, ny-scrapede datas√¶t for VFF‚Äôs rundeplaceringer. Det sikrer, at RAW-laget altid afspejler den aktuelle kilde uden historiske dubletter.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# OVERWRITE RAW i SQL
# ====================================================================
raw_table_id <- DBI::Id(schema = "PBA01_Raw", table = "fact_vff_rundeplaceringer_raw")

cat("‚¨Ü Overwriter RAW:", "[PBA01_Raw].[fact_vff_rundeplaceringer_raw]\n")

DBI::dbWriteTable(
  conn      = con,
  name      = raw_table_id,
  value     = vff_runde_placering_raw_wide,
  overwrite = TRUE
)

cat("‚úÖ RAW overwrite OK.\n\n")

```

![](images/2026-01-03_23h54_14.png)

#### Byg CLEAN-laget (wide ‚Üí long + kamp_id)

Her transformerer vi RAW-tabellen fra bredt format til langt format, s√• hver r√¶kke repr√¶senterer √©n s√¶son og √©n runde. Samtidig konstruerer vi kamp_id og beregner placering f√∏r kamp, placering efter runde samt √¶ndringen, og til sidst laver vi et safeguard der stopper scriptet, hvis kamp_id ikke er unik.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Byg CLEAN (long + kamp_id + f√∏r/efter + √¶ndring)
# ====================================================================
VFF_KLUB_ID <- "KLUB046"

cat("--- Starter CLEAN (wide ‚Üí long) ---\n")

vff_runde_placering_clean <- vff_runde_placering_raw_wide %>%
  select(-any_of("Saeson_url_encoded")) %>%
  pivot_longer(
    cols = starts_with("Runde_"),
    names_to  = "runde",
    values_to = "placering",
    values_drop_na = TRUE
  ) %>%
  mutate(
    runde     = as.integer(str_remove(as.character(runde), "Runde_")),
    placering = as.integer(placering),
    s√¶sonkode = as.integer(SeasonCode),
    s√¶son     = str_squish(as.character(Saeson_label)),
    kamp_id   = paste0(s√¶son, "R", runde, VFF_KLUB_ID)
  ) %>%
  arrange(s√¶sonkode, runde) %>%
  group_by(s√¶sonkode) %>%
  mutate(
    placering_efter_runde = placering,
    placering_f√∏r_kamp    = lag(placering, 1),
    √¶ndring_placering     = placering_efter_runde - placering_f√∏r_kamp
  ) %>%
  ungroup() %>%
  select(
    kamp_id, s√¶sonkode, s√¶son, runde,
    placering_f√∏r_kamp, placering_efter_runde, √¶ndring_placering
  )

# safeguard: kamp_id unik
dup <- vff_runde_placering_clean %>%
  count(kamp_id) %>%
  filter(n > 1)

if (nrow(dup) > 0) {
  print(dup)
  DBI::dbDisconnect(con)
  stop("‚ùå kamp_id er ikke unik i CLEAN. Stop.")
}

cat("‚úÖ CLEAN bygget. R√¶kker:", nrow(vff_runde_placering_clean), "\n\n"); vff_runde_placering_clean

```

![](images/2026-01-03_23h48_11-01.png)

#### Overwrite CLEAN-laget i databasen

Det f√¶rdige CLEAN-datas√¶t skrives nu til PBA02_Clean og erstatter eventuelle tidligere versioner. P√• den m√•de sikres, at efterf√∏lgende joins og analyser altid arbejder p√• √©n konsistent og opdateret sandhedskilde.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# 7) OVERWRITE CLEAN i SQL
# ====================================================================
clean_table_id <- DBI::Id(schema = "PBA02_Clean", table = "fact_vff_rundeplaceringer_clean")

cat("‚¨Ü Overwriter CLEAN:", "[PBA02_Clean].[fact_vff_rundeplaceringer_clean]\n")

DBI::dbWriteTable(
  conn      = con,
  name      = clean_table_id,
  value     = vff_runde_placering_clean,
  overwrite = TRUE
)

cat("‚úÖ CLEAN overwrite OK.\n\n")

```

#### Feature-tabel til modellering

Her kondenseres CLEAN-data til √©n r√¶kke pr. kamp via kamp_id og gemmes som en feature-tabel. Denne tabel anvendes direkte i baseline- og analyse-datas√¶t og g√∏r joinet mod modeller enkelt og stabilt.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)


# ====================================================================
# 8) FEATURE-tabel til baseline (√©n r√¶kke pr kamp_id)
#    (Den her er det du joiner ind i analysis_df/baseline p√• kamp_id)
# ====================================================================
feat_tbl <- vff_runde_placering_clean %>%
  transmute(
    kamp_id,
    vff_placering_f√∏r_kamp = placering_f√∏r_kamp,
    vff_placering_efter_runde = placering_efter_runde,
    vff_√¶ndring_placering = √¶ndring_placering
  )

feat_table_id <- DBI::Id(schema = "PBA03_JoinReady", table = "feat_vff_rundeplacering_kamp")

cat("‚¨Ü Overwriter FEATURE:", "[PBA03_JoinReady].[feat_vff_rundeplacering_kamp]\n")

DBI::dbWriteTable(
  conn      = con,
  name      = feat_table_id,
  value     = feat_tbl,
  overwrite = TRUE
); tibble(feat_tbl)

cat("‚úÖ FEATURE overwrite OK.\n\n")

```

![](images/2026-01-03_23h56_32.png)

#### Luk forbindelse

Forbindelsen til Azure SQL lukkes kontrolleret for at sikre, at ressourcer frigives korrekt, og at scriptet afsluttes p√¶nt.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

DBI::dbDisconnect(con)
cat("üîå Azure connection lukket. Done.\n")
```

### VFF interne data (3 datas√¶t)

I dette script indl√¶ser vi VFF‚Äôs interne data fra b√•de en lokal SQLite-database og to RDS-filer, laver et kort kvalitetstjek (klubnavne + kolonnematch), og uploader derefter tre separate RAW-tabeller til Azure SQL i PBA01_Raw.\

Det giver os en stabil ‚Äúlanding zone‚Äù, hvor vi b√•de har klub-dimensionen samt billetsalget fra to kilder (SQLite og RDS), s√• rensning og senere modellering kan bygges ovenp√• et ensartet RAW-lag.

#### Pakker

Her indl√¶ses de n√∏dvendige R-pakker til databaseadgang, datah√•ndtering og upload til Azure SQL. Pacman bruges for at sikre, at alle pakker b√•de er installeret og loadet konsistent.

```{r}
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, RSQLite, dplyr, tidyverse, odbc, rstudioapi)
}); cat("Pakker klar og loadedüëç\n")

```

#### Filstier

Her defineres de lokale filstier til de datakilder, som indl√¶ses i scriptet.\
Stierne peger p√• b√•de SQLite-databasen og de tilh√∏rende RDS-filer, som tilsammen udg√∏r grundlaget for RAW-uploadet.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
{
sqlite_path <- "C:/Users/janpe/OneDrive/Skrivebord/PBA Dataanlyse/01_F√∏rste semester/1 Semester projekt/Database SQL/Upload af RAW data til SQL Azure/VFF_udl_SQL_RDS/fodbolddata (3).sqlite"

rds_klubber_path <- "C:/Users/janpe/OneDrive/Skrivebord/PBA Dataanlyse/01_F√∏rste semester/1 Semester projekt/Database SQL/Upload af RAW data til SQL Azure/VFF_udl_SQL_RDS/fcidk (1).rds"

rds_billetsalg_path <- "C:/Users/janpe/OneDrive/Skrivebord/PBA Dataanlyse/01_F√∏rste semester/1 Semester projekt/Database SQL/Upload af RAW data til SQL Azure/VFF_udl_SQL_RDS/vffkort01 (2).rds"}
cat("Alt VFF data fundet üëç\n")

```

#### Hent data fra SQLite

I dette afsnit oprettes forbindelse til den lokale SQLite-database, hvorefter klub- og billetsalgsdata indl√¶ses som r√• datas√¶t. Form√•let er at f√• et f√∏rste overblik over struktur og indhold, inden data senere uploades til RAW-laget i Azure SQL.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Hent data fra SQLite
# --------------------------------------------------------------------
con_sqlite <- DBI::dbConnect(RSQLite::SQLite(), sqlite_path)

tabeller <- DBI::dbListTables(con_sqlite)
cat("Tabeller i sqlite:\n")
print(tabeller)

# tabeller[1] = klubber, tabeller[2] = billetsalg
dim_vff_klubber     <- DBI::dbReadTable(con_sqlite, tabeller[1])
fact_vff_billetsalg <- DBI::dbReadTable(con_sqlite, tabeller[2])

DBI::dbDisconnect(con_sqlite)

# Overblik + valgfri view
{
  glimpse(fact_vff_billetsalg); str(fact_vff_billetsalg)
  glimpse(dim_vff_klubber);     str(dim_vff_klubber)
  
  if (interactive()) {
    View(fact_vff_billetsalg)
    View(dim_vff_klubber)
  }
}

```

![](images/2026-01-04_00h00_17.png)

![](images/2026-01-04_00h07_56.png)

#### Hent data fra RDS-filer

Her indl√¶ses supplerende VFF-data fra RDS-filer, som bruges til validering og sammenligning med data fra SQLite. Form√•let er at sikre konsistens i klubnavne og struktur, f√∏r datas√¶ttene uploades som RAW-tabeller.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false

RDSfil1 <- readRDS(rds_klubber_path)
df_1    <- as_tibble(RDSfil1)

tjek_ens <- df_1 |>
  mutate(findes_i_dim = ifelse(navn %in% dim_vff_klubber$navn, "ja", "nej"))

afvigelser <- tjek_ens |>
  filter(findes_i_dim == "nej")

if (interactive()) {
  View(tjek_ens)
  View(afvigelser)
}

RDSfil2 <- readRDS(rds_billetsalg_path)
df_2    <- as_tibble(RDSfil2)               # billetsalg fra RDS
df_fact <- as_tibble(fact_vff_billetsalg)   # billetsalg fra SQLite

tibble(tjek_ens)

```

![](images/2026-01-04_00h09_23.png)

#### Struktur- og kolonnekontrol

Her sammenlignes kolonnerne i billetsalgsdata fra RDS og SQLite for at identificere strukturelle forskelle. Form√•let er at afd√¶kke afvigelser tidligt, s√• RAW-tabellerne dokumenterer kilden korrekt og konsistent.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# ------------------------------------------------------------------
# Sammenligning af kolonner (struktur-tjek)
# ------------------------------------------------------------------
kolonner_kun_i_df2  <- setdiff(names(df_2),    names(df_fact))
kolonner_kun_i_fact <- setdiff(names(df_fact), names(df_2))

message("Kolonner i df_2 (RDS) som IKKE er i fact_vff_billetsalg (SQLite):")
print(kolonner_kun_i_df2)

message("Kolonner i fact_vff_billetsalg (SQLite) som IKKE er i df_2 (RDS):")
print(kolonner_kun_i_fact)
```

#### Milj√∏-sikring (.Renviron)

Dette afsnit sikrer, at alle n√∏dvendige login-oplysninger til Azure SQL er sat korrekt. Hvis noget mangler, stoppes scriptet tidligt for at undg√• fejlslagen upload eller ufuldst√¶ndige RAW-data.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# --------------------------------------------------------------------
# Safeguard: tjek at .Renviron er sat korrekt op
# --------------------------------------------------------------------
required_vars <- c("AZURE_SQL_SERVER", "AZURE_SQL_DB", "AZURE_SQL_UID", "AZURE_SQL_PWD")
values <- Sys.getenv(required_vars)
missing_vars <- required_vars[values == ""]

if (length(missing_vars) > 0) {
  cat("‚ö†Ô∏è  F√∏lgende milj√∏variabler mangler i .Renviron:\n")
  print(missing_vars)
  cat("\n√Öbner ~/.Renviron ‚Äì udfyld v√¶rdierne og gem filen.\n")
  
  file.edit("~/.Renviron")
  cat("üîÑ R genstartes nu via RStudio ‚Äì k√∏r scriptet igen bagefter.\n")
  
  if (rstudioapi::isAvailable()) {
    rstudioapi::restartSession()
  } else {
    stop("rstudioapi er ikke tilg√¶ngelig ‚Äì genstart R manuelt og k√∏r scriptet igen.")
  }
} else {
  cat("‚úî Alle n√∏dvendige .Renviron-variabler er sat.\n\n")
}

```

#### Azure-forbindelse med retry

Denne del opretter forbindelse til Azure SQL med automatisk genfors√∏g og stigende timeout. Form√•let er at sikre en stabil forbindelse, selv ved midlertidige netv√¶rks- eller serverproblemer.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# -------------------------------------------------------------------
# Azure forbindelse med automatisk retry
# -------------------------------------------------------------------
fors√∏g_max <- 6
timeouts  <- c(60, 180, 200, 260, 360, 600)
delay_sec <- 10
fors√∏g    <- 1
con_azure <- NULL

while (fors√∏g <= fors√∏g_max && is.null(con_azure)) {
  
  timeout_brug <- timeouts[min(fors√∏g, length(timeouts))]
  
  cat("Fors√∏g", fors√∏g, "p√• at forbinde (ConnectionTimeout =", timeout_brug, "sekunder)\n")
  
  con_try <- try(
    DBI::dbConnect(
      odbc::odbc(),
      driver   = "ODBC Driver 18 for SQL Server",
      server   = Sys.getenv("AZURE_SQL_SERVER"),
      database = Sys.getenv("AZURE_SQL_DB"),
      uid      = Sys.getenv("AZURE_SQL_UID"),
      pwd      = Sys.getenv("AZURE_SQL_PWD"),
      port     = 1433,
      Encrypt  = "yes",
      TrustServerCertificate = "no",
      ConnectionTimeout      = timeout_brug
    ),
    silent = TRUE
  )
  
  if (!inherits(con_try, "try-error")) {
    con_azure <- con_try
    cat("‚úÖ Forbundet til:", Sys.getenv("AZURE_SQL_DB"), "p√• fors√∏g", fors√∏g, "\n\n")
  } else {
    cat("‚ùå Forbindelsen fejlede p√• fors√∏g", fors√∏g, "\n")
    
    if (fors√∏g == fors√∏g_max) {
      stop("Kunne ikke forbinde til Azure SQL efter ", fors√∏g_max, " fors√∏g.\n")
    }
    
    cat("Venter", delay_sec, "sekunder f√∏r n√¶ste fors√∏g\n\n")
    Sys.sleep(delay_sec)
    fors√∏g <- fors√∏g + 1
  }
}
```

#### Upload af data til Azure SQL database

I dette afsnit uploades de tre datas√¶t til RAW-laget i Azure SQL under schemaet PBA01_Raw. Tabellerne overskrives bevidst, da de fungerer som rene landings-/kildetabeller, som senere danner grundlag for clean- og join-ready lagene.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# --------------------------------------------------------------------
# Upload til Azure SQL (skema PBA01_Raw)
# --------------------------------------------------------------------
schema <- "PBA01_Raw"

DBI::dbWriteTable(
  conn      = con_azure,
  name      = DBI::Id(schema = schema, table = "dim_vff_klubber_raw"),
  value     = dim_vff_klubber,
  overwrite = TRUE
)

DBI::dbWriteTable(
  conn      = con_azure,
  name      = DBI::Id(schema = schema, table = "fact_vff_billetsalg_raw_SQLite"),
  value     = df_fact,
  overwrite = TRUE
)

DBI::dbWriteTable(
  conn      = con_azure,
  name      = DBI::Id(schema = schema, table = "fact_vff_billetsalg_raw_RDS"),
  value     = df_2,
  overwrite = TRUE
)

```

#### Hurtigt sanity check og lukning

Her laver vi et hurtigt opslag i de tre uploadede RAW-tabeller for at bekr√¶fte, at data faktisk ligger i Azure som forventet.\
Til sidst lukkes forbindelsen til databasen, s√• scriptet afslutter p√¶nt og uden √•bne sessions.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)


head(DBI::dbReadTable(con_azure, DBI::Id(schema = schema, table = "dim_vff_klubber_raw")))
head(DBI::dbReadTable(con_azure, DBI::Id(schema = schema, table = "fact_vff_billetsalg_raw_SQLite")))
head(DBI::dbReadTable(con_azure, DBI::Id(schema = schema, table = "fact_vff_billetsalg_raw_RDS")))

DBI::dbDisconnect(con_azure)

```

### Websrapping H√•ndbold kampprogram SAH([tophaandbold.dk](https://tophaandbold.dk/kampprogram/herreligaen))

I dette afsnit indsamles kampprogrammet for **SAH**, som er det h√•ndboldhold, der vurderes mest relevant i relation til Viborg og projektets kontekst. Form√•let er at kortl√¶gge, hvorn√•r SAH afvikler hjemmekampe, da disse kan fungere som en konkurrerende begivenhed i forhold til publikums tilstedev√¶relse. Kampprogrammet bruges som et kontekstuelt datas√¶t, der kan indg√• i analysen af tilskuertal.

#### Pakker

```{r}
# ====================================================================
# Pakker
# ====================================================================
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(
    rvest, httr, dplyr, stringr, purrr, tibble, tidyr,
    DBI, odbc, rstudioapi, dbplyr, rlang
  )
}); cat("Pakker loaded og klar til anvendelse üëå\n")
```

#### Hj√¶lpefunktion ‚Äì opbygning af kampprogram-URL

Denne funktion konstruerer en korrekt foresp√∏rgsels-URL til toph√•ndbold.dk baseret p√• s√¶son, hold og hjemme/udekamp.\
Form√•let er at sikre ensartede og reproducerbare kald til kampprogrammet i det efterf√∏lgende scrape.

```{r}

# ====================================================================
# Hj√¶lpefunktion ‚Äì byg URL til kampprogram
# ====================================================================
byg_kampprogram_url <- function(saeson_id,
                                hold_id,
                                hjemmekamp = 1,
                                udekamp   = 0) {
  grund_url <- "https://tophaandbold.dk/kampprogram/herreligaen"
  
  forespoergsel <- list(
    year      = saeson_id,
    team      = hold_id,
    home_game = hjemmekamp,
    away_game = udekamp
  )
  
  query_streng <- paste0(
    names(forespoergsel), "=", forespoergsel,
    collapse = "&"
  )
  
  paste0(grund_url, "?", query_streng)
}
```

#### Hj√¶lpefunktion ‚Äì parsing af kampr√¶kker fra HTML

Denne funktion udtr√¶kker r√• kampinformation direkte fra HTML-strukturen p√• kampprogramsiden. Form√•let er at oms√¶tte ustruktureret webindhold til et konsistent RAW-datas√¶t uden fortolkning eller berigelse.

```{r}
# ====================================================================
# Hj√¶lpefunktion ‚Äì parse kamp-r√¶kker fra HTML
# ====================================================================
parse_kampr√¶kker <- function(side_html) {
  
  kampr√¶kker <- side_html |>
    html_elements(".match-program .row")
  
  if (length(kampr√¶kker) == 0) {
    message("Ingen kamp-r√¶kker fundet p√• siden.")
    return(tibble())
  }
  
  resultat <- purrr::map_dfr(kampr√¶kker, function(r√¶kke) {
    
    dato_raw <- r√¶kke |>
      html_element(".match-program__row__date") |>
      html_text2()
    
    if (length(dato_raw) == 0 ||
        is.na(dato_raw) ||
        str_squish(dato_raw) == "") {
      return(tibble())
    }
    dato_raw <- dato_raw |> str_squish()
    
    hold_tekster <- r√¶kke |>
      html_elements(".match-program__row__team") |>
      html_text2() |>
      str_squish()
    
    if (length(hold_tekster) < 2) {
      return(tibble())
    }
    
    hjemmehold_tekst <- hold_tekster[1]
    udehold_tekst    <- hold_tekster[2]
    
    tibble(
      kamp_dato_raw = dato_raw,
      hjemmehold    = hjemmehold_tekst,
      udehold       = udehold_tekst
    )
  })
  
  resultat
}
```

#### Hj√¶lpefunktion ‚Äì hent kampprogram for √©n s√¶son

Funktionen henter kampprogrammet for en given s√¶son og et specifikt hold via et HTTP-kald. HTML-svaret parses efterf√∏lgende til et RAW-datas√¶t ved hj√¶lp af den dedikerede parse-funktion.

```{r}

# ====================================================================
# Hj√¶lpefunktion ‚Äì hent √©n s√¶sons kampprogram
# ====================================================================
hent_kampprogram_saeson <- function(saeson_id,
                                    hold_id,
                                    saeson_label,
                                    hjemmekamp = 1,
                                    udekamp   = 0) {
  url <- byg_kampprogram_url(
    saeson_id  = saeson_id,
    hold_id    = hold_id,
    hjemmekamp = hjemmekamp,
    udekamp    = udekamp
  )
  
  message("Henter: ", url, "  (", saeson_label, ")")
  
  svar <- httr::GET(url)
  if (status_code(svar) != 200) {
    warning("Fejl ved hentning af URL (status ",
            status_code(svar), "): ", url)
    return(tibble())
  }
  
  side_html <- svar |>
    content(as = "text") |>
    read_html()
  
  parse_kampr√¶kker(side_html)
}
```

#### Opsamling af SAH-s√¶soner til samlet RAW-datas√¶t

Her defineres de relevante SAH-s√¶soner og tilh√∏rende hold, hvorefter kampprogrammet hentes s√¶son for s√¶son.\
Resultatet samles i √©t konsistent RAW-datas√¶t, som danner grundlag for videre rensning og analyse.

```{r}
# ====================================================================
# Oversigt over SAH-s√¶soner + loop ‚Üí RAW-datas√¶t
# ====================================================================
sah_saesoner <- tribble(
  ~saeson_id, ~saeson_label, ~hold_id,
  9,          "2025/2026",    68,
  8,          "2024/2025",    68,
  7,          "2023/2024",    68,
  6,          "2022/2023",    68,
  5,          "2021/2022",    68,
  4,          "2020/2021",    11,
  3,          "2019/2020",    11,
  2,          "2018/2019",    11,
  1,          "2017/2018",    11
)

sah_kampprogram_raw <- sah_saesoner |>
  mutate(
    kampdata = purrr::pmap(
      list(saeson_id, hold_id, saeson_label),
      ~ hent_kampprogram_saeson(
        saeson_id    = ..1,
        hold_id      = ..2,
        saeson_label = ..3,
        hjemmekamp   = 1,
        udekamp      = 0
      )
    )
  ) |>
  unnest(cols = kampdata)

str(sah_kampprogram_raw)
view(sah_kampprogram_raw)
```

![](images/2026-01-04_00h17_45.png)

#### Milj√∏variabler og sikker ops√¶tning

Dette afsnit sikrer, at alle n√∏dvendige login-oplysninger til Azure SQL er korrekt sat via .Renviron. Hvis der mangler variabler, stoppes scriptet tidligt for at undg√• fejl og ufuldst√¶ndige dataindl√¶sninger.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Safeguard .Renviron (.Renviron skal v√¶re sat korrekt op)
# ====================================================================
required_vars <- c(
  "AZURE_SQL_SERVER",
  "AZURE_SQL_DB",
  "AZURE_SQL_UID",
  "AZURE_SQL_PWD"
)

values <- Sys.getenv(required_vars)
missing_vars <- required_vars[values == ""]

if (length(missing_vars) > 0) {
  cat("‚ö†Ô∏è F√∏lgende milj√∏variabler mangler i .Renviron:\n")
  print(missing_vars)
  cat("\n√Öbner ~/.Renviron ‚Äì udfyld v√¶rdierne og gem filen.\n")
  
  file.edit("~/.Renviron")
  cat("üîÑ R genstartes nu via RStudio ‚Äì k√∏r scriptet igen bagefter.\n")
  
  if (rstudioapi::isAvailable()) {
    rstudioapi::restartSession()
  } else {
    stop("rstudioapi er ikke tilg√¶ngelig ‚Äì genstart R manuelt og k√∏r scriptet igen.")
  }
} else {
  cat("‚úî Alle n√∏dvendige .Renviron-variabler er sat.\n\n")
}


```

#### Azure SQL-forbindelse med retry-logik

Dette afsnit opretter forbindelse til Azure SQL med automatisk retry og gradvist stigende timeout. Form√•let er at g√∏re dataindl√¶sningen robust over for midlertidige netv√¶rks- eller serverproblemer.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Azure forbindelse med automatisk retry
# ====================================================================
fors√∏g_max  <- 6
timeouts    <- c(60, 180, 200, 260, 360, 600)
delay_sec   <- 10
fors√∏g      <- 1
con         <- NULL

while (fors√∏g <= fors√∏g_max && is.null(con)) {
  
  timeout_brug <- timeouts[min(fors√∏g, length(timeouts))]
  
  cat(
    "Fors√∏g", fors√∏g,
    "p√• at forbinde (ConnectionTimeout =", timeout_brug, "sekunder)...\n"
  )
  
  con_try <- try(
    dbConnect(
      odbc::odbc(),
      driver   = "ODBC Driver 18 for SQL Server",
      server   = Sys.getenv("AZURE_SQL_SERVER"),
      database = Sys.getenv("AZURE_SQL_DB"),
      uid      = Sys.getenv("AZURE_SQL_UID"),
      pwd      = Sys.getenv("AZURE_SQL_PWD"),
      port     = 1433,
      Encrypt  = "yes",
      TrustServerCertificate = "no",
      ConnectionTimeout      = timeout_brug
    ),
    silent = TRUE
  )
  
  if (!inherits(con_try, "try-error")) {
    con <- con_try
    cat(
      "‚úî Forbundet til:", Sys.getenv("AZURE_SQL_DB"),
      "p√• fors√∏g", fors√∏g, "\n\n"
    )
  } else {
    cat("‚ùå Forbindelsen fejlede p√• fors√∏g", fors√∏g, "‚Äì pr√∏ver igen.\n")
    
    if (fors√∏g == fors√∏g_max) {
      stop("Kunne ikke forbinde til Azure SQL efter ", fors√∏g_max, " fors√∏g.\n")
    }
    
    cat("Venter", delay_sec, "sekunder f√∏r n√¶ste fors√∏g...\n\n")
    Sys.sleep(delay_sec)
    fors√∏g <- fors√∏g + 1
  }
}


```

#### F√∏rste load og inkrementel opdatering af SAH-kampprogram

Dette afsnit h√•ndterer b√•de f√∏rste indl√¶sning og efterf√∏lgende inkrementelle opdateringer af kampprogrammet i RAW-laget. Kun nye kampe inds√¶ttes ved senere k√∏rsler, s√• dubletter undg√•s, og tabellen holdes konsistent.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# F√∏rste load + incremental update til PBA01_Raw.fact_kampprogram_SAH_raw
#           (ingen r√• SQL, ingen dupletter)
# ====================================================================
table_id <- Id(
  schema = "PBA01_Raw",
  table  = "fact_kampprogram_SAH_raw"
)

cat("üîç Tjekker adgang til schema 'PBA01_Raw' og tabellen via dbExistsTable...\n")

exists_try <- try(
  dbExistsTable(con, table_id),
  silent = TRUE
)

if (inherits(exists_try, "try-error")) {
  dbDisconnect(con)
  stop(
    "Kan ikke tilg√• 'PBA01_Raw.fact_kampprogram_SAH_raw'.\n",
    "Tjek at schemaet 'PBA01_Raw' findes, og at brugeren har rettigheder."
  )
}

tabel_findes <- isTRUE(exists_try)

if (!tabel_findes) {
  # ---------------------------------------------------------------
  # F√∏rste load: tabellen findes ikke ‚Üí opret og inds√¶t alle r√¶kker
  # ---------------------------------------------------------------
  cat("üÜï F√∏rste load: Tabel findes ikke, opretter og inds√¶tter alle r√¶kker...\n")
  
  dbWriteTable(
    conn      = con,
    name      = table_id,
    value     = sah_kampprogram_raw,
    overwrite = FALSE,
    append    = FALSE
  )
  
  antal_r√¶kker  <- nrow(sah_kampprogram_raw)
  antal_v√¶rdier <- nrow(sah_kampprogram_raw) * ncol(sah_kampprogram_raw)
  
  cat("‚úÖ F√∏rste load gennemf√∏rt.\n")
  cat("üìä Antal r√¶kker indsat:", antal_r√¶kker, "\n")
  cat("üìä Antal v√¶rdier i tabellen:", antal_v√¶rdier, "\n\n")
  
} else {
  # ---------------------------------------------------------------
  # Incremental update: inds√¶t kun nye kampe
  # N√∏gle = (saeson_id, kamp_dato_raw, hjemmehold, udehold)
  # ---------------------------------------------------------------
  cat("üîÑ Tabel findes ‚Äì laver incremental update.\n")
  
  fact_tbl <- tbl(
    con,
    in_schema("PBA01_Raw", "fact_kampprogram_SAH_raw")
  )
  
  # Hent eksisterende n√∏gler fra SQL
  eksisterende_n√∏gler <- fact_tbl |>
    select(saeson_id, kamp_dato_raw, hjemmehold, udehold) |>
    distinct() |>
    collect()
  
  # Find de r√¶kker i det nye scrape, som IKKE findes i SQL-tabellen
  nye_r√¶kker <- sah_kampprogram_raw |>
    anti_join(
      eksisterende_n√∏gler,
      by = c("saeson_id", "kamp_dato_raw", "hjemmehold", "udehold")
    )
  
  if (nrow(nye_r√¶kker) == 0) {
    cat("‚ÑπÔ∏è Ingen nye kampe at inds√¶tte ‚Äì alt er allerede i databasen.\n\n")
  } else {
    cat("‚¨Ü Inds√¶tter kun nye kampe (antal):", nrow(nye_r√¶kker), "\n")
    
    dbWriteTable(
      conn      = con,
      name      = table_id,
      value     = nye_r√¶kker,
      append    = TRUE,
      overwrite = FALSE
    )
    
    cat("‚úÖ Incremental insert gennemf√∏rt.\n\n")
  }
}


# ====================================================================
# Luk forbindelsen p√¶nt
# ====================================================================
dbDisconnect(con)
cat("üîå Forbindelsen til Azure SQL er nu lukket.\n")

```

### API Danske helligdage Raw (Nager.Date)

I dette afsnit hentes officielle danske helligdage via Nager.Date API og samles til √©n RAW-dimension for perioden 2000‚Äì2026. Datas√¶ttet afspejler kilden direkte og indeholder kun let formatering/overs√¶ttelse, s√• det senere kan bruges som stabilt input til clean- og join-ready lag.

#### Pakker

```{r}
# Pakker:
#  - httr2/jsonlite  ‚Üí API-kald og JSON-parsing
#  - tibble/dplyr    ‚Üí datarammer og transformationer
#  - purrr           ‚Üí map-funktioner til √•rsl√∏kke
#  - lubridate       ‚Üí datoh√•ndtering
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  
  pacman::p_load(
    httr2, jsonlite, tibble, dplyr, purrr, lubridate
  )
}); cat("Pakker klar god arbejdslystüôÇ\n")

```

#### Funktion til hentning og standardisering af helligdage

Her defineres en genbrugelig funktion, der henter helligdage for √©t √•r via Nager.Date, validerer API-svaret og returnerer en standardiseret RAW-dimension med f√¶lles n√∏gler og navnefelter til senere joins.

```{r}

# ------------------------------------------------------------
# Funktion: hent og formater helligdage for √©t √•r
# ------------------------------------------------------------

get_public_holidays_dim <- function(year, country = "DK") {
  
  base_url <- "https://date.nager.at/api/v3/publicholidays"
  
  # Byg request og send til API‚Äôet
  resp <- request(base_url) |>
    req_url_path_append(paste0(year, "/", country)) |>
    req_perform()
  
  # Simpel fejlh√•ndtering: vi stopper scriptet, hvis API svarer med fejl
  if (resp_status(resp) >= 400) {
    stop("API-fejl for √•r: ", year, " land: ", country)
  }
  
  # JSON ‚Üí tekst ‚Üí liste ‚Üí tibble
  dat_raw <- resp |>
    resp_body_string() |>
    fromJSON(flatten = TRUE) |>
    as_tibble()
  
  # Formatering til vores √∏nskede RAW-struktur
  out <- dat_raw |>
    mutate(
      # API-feltet "date" er tekst ‚Äì vi laver det om til rigtig Date
      dato_tmp = as.Date(date)
    ) |>
    transmute(
      # datekey som ddmm√•√•√•√•, s√• den matcher dim_dato og andre dimensioner
      datekey        = format(dato_tmp, "%d%m%Y"),
      
      # Behold R-dato (bruges ofte i analyser)
      dato           = dato_tmp,
      
      # Navne direkte fra API
      helligdag_navn = localName,   # dansk navn
      english_name   = name,        # engelsk navn
      
      # Type: overs√¶t alle API-typer til dansk tekst
      type = purrr::map_chr(
        types,
        \(x) {
          # recode mapper de enkelte kodeord til danske beskrivelser
          x_dk <- dplyr::recode(
            x,
            "Public"      = "Officiel helligdag",
            "Bank"        = "Bankferie",
            "School"      = "Skoleferie",
            "Authorities" = "Myndigheder lukket",
            "Optional"    = "Valgfri fridag",
            "Observance"  = "H√∏jtid (ingen betalt fridag)",
            .default      = x
          )
          # hvis der er flere type-v√¶rdier, samles de i √©n tekststreng
          paste(x_dk, collapse = ", ")
        }
      )
    )
  
  out
}

```

#### Hent og saml alle helligdage (2000‚Äì2026)

Her hentes helligdage for hvert √•r i perioden 2000‚Äì2026 ved hj√¶lp af den definerede funktion, og alle √•rsresultater bindes sammen til √©n samlet RAW-dimension, som efterf√∏lgende valideres med simple QA-udskrifter.

```{r}
# ------------------------------------------------------------
# Hent alle √•r 2000‚Äì2026 og bind til √©n samlet dim-tabel
# ------------------------------------------------------------
# years:
#   - definerer den periode, vi vil d√¶kke med helligdage.
#   - kan nemt justeres senere (fx 1990:2030).
#
# purrr::map_dfr:
#   - kalder get_public_holidays_dim for hvert √•r
#   - r binder alle √•rs-tabeller sammen (row-bind) til √©n samlet tabel.
years <- 2000:2026

dim_helligdage_dkk_raw <- purrr::map_dfr(
  years,
  \(y) get_public_holidays_dim(y, country = "DK")
)

cat("Antal r√¶kker i dim_helligdage_dkk_raw:", nrow(dim_helligdage_dkk_raw), "\n")

# Hurtigt tjek i R ‚Äì kun til udvikling / debugging
print(head(dim_helligdage_dkk_raw, 10))
str(dim_helligdage_dkk_raw)
```

![](images/2026-01-04_00h29_33.png)

#### Fuld load til PBA01_Raw ‚Äì overwrite af helligdagsdimension

Vi sikrer at vi ikke mangler en pakke.

```{r}
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr)
}); cat("Pakker loaded og klar \n")

```

#### Forbindelse til Azure SQL (RAW-lag)

I dette trin oprettes en sikker databaseforbindelse til Azure SQL ved hj√¶lp af loginoplysninger fra milj√∏variabler. Forbindelsen konfigureres specifikt til RAW-laget, s√• efterf√∏lgende upload af helligdagetabel sker i det korrekte schema.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)
# ------------------------------------------------------------
# Forbindelse til Azure SQL (login fra .Renviron)
# ------------------------------------------------------------
# Vi henter loginoplysninger fra milj√∏variabler:
#   AZURE_SQL_SERVER
#   AZURE_SQL_DB
#   AZURE_SQL_UID
#   AZURE_SQL_PWD
#
# Schema:
#   - s√¶ttes nu til PBA01_Raw for at matche dit schema-design for RAW-laget.
server   <- Sys.getenv("AZURE_SQL_SERVER")
database <- Sys.getenv("AZURE_SQL_DB")
uid      <- Sys.getenv("AZURE_SQL_UID")
pwd      <- Sys.getenv("AZURE_SQL_PWD")
schema   <- "PBA01_Raw"
table    <- "dim_helligdage_dkk_raw"

if (any(!nzchar(c(server, database, uid, pwd)))) {
  stop("En eller flere milj√∏variable til Azure SQL mangler. Tjek .Renviron og genstart R.")
}

con <- DBI::dbConnect(
  odbc::odbc(),
  driver   = "ODBC Driver 18 for SQL Server",
  server   = server,
  database = database,
  uid      = uid,
  pwd      = pwd,
  port     = 1433,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  ConnectionTimeout      = 30
)

cat("Forbundet til:", database, "\n")

```

#### Upload til Azure SQL Database

Her overskrives hele tabellen i PBA01_Raw med dim_helligdage_dkk_raw efter et kort sikkerhedstjek, s√• vi undg√•r at uploade et tomt/ikke-eksisterende objekt.\
Efter upload valideres antallet af r√¶kker direkte i SQL, og forbindelsen lukkes p√¶nt for at afslutte load-trinnet.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false

# ------------------------------------------------------------
# Fuld load ‚Äì overskriv hele tabellen i PBA01_Raw
# ------------------------------------------------------------
# Vi sikkerhedstjekker f√∏rst, at objektet dim_helligdage_dkk_raw findes i R.
# Det forhindrer, at vi uploader en tom/ikke-eksisterende tabel ved en fejl.

if (!exists("dim_helligdage_dkk_raw")) {
  DBI::dbDisconnect(con)
  stop("Objektet 'dim_helligdage_dkk_raw' findes ikke ‚Äì k√∏r SCRIPT 1 f√∏rst.")
}

cat(
  "Uploader dim_helligdage_dkk_raw til ",
  schema, ".", table, " ...\n",
  sep = ""
)

DBI::dbWriteTable(
  conn      = con,
  name      = DBI::Id(schema = schema, table = table),
  value     = dim_helligdage_dkk_raw,
  overwrite = TRUE,   # VIGTIGT: vi overskriver altid ‚Üí ingen dubletter
  append    = FALSE
)

cat("Fuld load f√¶rdig. Antal r√¶kker uploadet:", nrow(dim_helligdage_dkk_raw), "\n")

# Simpelt tjek direkte fra SQL ‚Äì kun til validering
check_tbl <- DBI::dbReadTable(con, DBI::Id(schema = schema, table = table))
cat("Antal r√¶kker i SQL efter upload:", nrow(check_tbl), "\n")
print(head(check_tbl, 10))

# Luk forbindelsen, n√•r vi er f√¶rdige
DBI::dbDisconnect(con)
cat("Forbindelse lukket.\n")


```

### API Temperatur DMI Karup station - Raw temp_dry

I dette script etableres det r√• datagrundlag for lufttemperaturm√•linger (temp_dry, 2 m) fra DMI for Flyvestation Karup (station 06060). Data hentes via metObs-API‚Äôet og gemmes ufortolket i RAW-laget i Azure SQL som et faktalayer. Der foretages kun let teknisk formatering, mens al fortolkning og sammenkobling med √∏vrige datalag udskydes. Processen underst√∏tter b√•de et f√∏rste fuldt historisk load og efterf√∏lgende inkrementelle opdateringer for at sikre et konsistent og dubletfrit datas√¶t.

#### Pakker

```{r}

# ====================================================================
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(httr2, jsonlite, dplyr, tibble, purrr, lubridate)
}); cat("Pakker klar til brugüëç \n")

```

#### API-n√∏gle

Her indl√¶ses DMI API-n√∏glen sikkert fra milj√∏variabler, og scriptet afbrydes tidligt, hvis n√∏glen mangler, s√• der ikke foretages uautoriserede eller fejlbeh√¶ftede API-kald.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# DMI_API_KEY l√¶ses fra .Renviron. 
# Hvis den ikke findes, stopper vi, s√• vi ikke kalder API‚Äôet uden n√∏gle.

dmi_api_key <- Sys.getenv("DMI_API_KEY")
if (dmi_api_key == "") {
  stop("DMI_API_KEY er ikke sat ‚Äì tjek .Renviron og genstart R.")
}
```

#### Ops√¶tning af API-parametre (temp_dry)

I dette trin fastl√¶gges de f√¶lles API-parametre for alle kald, herunder endpoint, stationskode og den valgte temperaturparameter, s√• indsamlingen kan gennemf√∏res konsistent p√• tv√¶rs af hele perioden.

```{r}
# Faste parametre til alle kald:
#   - base_url: metObs observation-endpoint
#   - station_id: DMI-station 06060 (Karup)
#   - param_id: temp_dry (2 m lufttemperatur)
base_url   <- "https://dmigw.govcloud.dk/v2/metObs/collections/observation/items"
station_id <- "06060"        # Flyvestation Karup
param_id   <- "temp_dry"     # 2 m lufttemperatur
```

#### Funktion til r√• API-kald pr. datointerval

Her defineres en hj√¶lpefunktion, der opbygger et tidsinterval i UTC, kalder DMI metObs-API‚Äôet for det p√•g√¶ldende interval og oms√¶tter svaret til en ensartet tibble. Funktionen h√•ndterer tilf√¶lde uden data ved at returnere et tomt datas√¶t, s√• den kan bruges sikkert i l√∏kker over l√¶ngere perioder.

```{r}

# R√• kald-funktion for √©t interval ----------------------------------------
# do_call_temp:
#   - bygger datetime-interval i ISO-format (UTC)
#   - kalder DMI API for det interval
#   - mapper JSON-features til en tibble
#   - returnerer tom tibble, hvis der ingen features er

do_call_temp <- function(start_date, end_date) {
  
  start_txt <- paste0(format(start_date, "%Y-%m-%d"), "T00:00:00Z")
  end_txt   <- paste0(format(end_date,   "%Y-%m-%d"), "T23:59:59Z")
  dt_range  <- paste0(start_txt, "/", end_txt)
  
  cat("  Kald DMI API for interval:", start_txt, "‚Üí", end_txt, "\n")
  
  req <- request(base_url) |>
    req_url_query(
      stationId   = station_id,
      parameterId = param_id,
      datetime    = dt_range,
      limit       = 100000   # rigeligt pr. √•r for √©n parameter
    ) |>
    req_headers(
      "X-Gravitee-Api-Key" = dmi_api_key
    )
  
  resp <- req_perform(req)
  x    <- resp_body_json(resp, simplifyVector = FALSE)
  
  if (is.null(x$features) || length(x$features) == 0) {
    cat("    Ingen features i dette interval.\n")
    return(tibble())
  }
  
  out <- purrr::map_dfr(
    x$features,
    function(feat) {
      tibble(
        stationId    = feat$properties$stationId,
        parameterId  = feat$properties$parameterId,
        observed_raw = feat$properties$observed,
        value        = feat$properties$value,
        lon          = feat$geometry$coordinates[[1]],
        lat          = feat$geometry$coordinates[[2]]
      )
    }
  )
  
  cat("    R√¶kker hentet i interval:", nrow(out), "\n")
  out
}

```

#### Robust hentefunktion med retry-logik

Dette trin indf√∏rer en wrapper omkring API-kaldet, som h√•ndterer midlertidige fejl ved at fors√∏ge igen et fast antal gange med pauser imellem. P√• den m√•de √∏ges stabiliteten ved st√∏rre datatr√¶k, uden at hele processen afbrydes, hvis enkelte intervaller fejler.

```{r}
#  Robust wrapper med retry (h√•ndterer fx 504) -----------------------------
# fetch_temp_interval:
#   - kalder do_call_temp indenfor en while-l√∏kke
#   - hvis kaldet fejler (try-error), fors√∏ger vi igen op til max_retries
#   - indl√¶gger 5 sekunders pause mellem fors√∏g
#   - returnerer tom tibble, hvis alle fors√∏g fejler
fetch_temp_interval <- function(start_date, end_date, max_retries = 3) {
  attempt <- 1
  while (attempt <= max_retries) {
    cat("  Fors√∏g", attempt, "af", max_retries, "\n")
    
    res <- try(
      do_call_temp(start_date, end_date),
      silent = TRUE
    )
    
    if (!inherits(res, "try-error")) {
      return(res)
    }
    
    cat("    Fejl ved kald ‚Äì pr√∏ver igen\n")
    
    if (attempt < max_retries) {
      cat("    Venter 5 sekunder\n")
      Sys.sleep(5)
    }
    
    attempt <- attempt + 1
  }
  
  cat("    GIVER OP for dette interval efter", max_retries, "fors√∏g ‚Äì returnerer tomt tibble.\n")
  tibble()                                                            
}
```

#### Opbygning af √•rvise hentningsintervaller

Her opdeles hele perioden fra √•r 2000 til dags dato i √•rvise intervaller, s√• API-kald kan gennemf√∏res kontrolleret og robust. Denne struktur g√∏r det muligt at hente data systematisk og h√•ndtere fejl uden at p√•virke resten af perioden.

```{r}

# Lav √•rs-intervaller fra 2000-01-01 til i dag ----------------------------
# For at g√∏re loadet robust laver vi √©t kald pr. √•r:
#   - start_total / end_total definerer hele perioden
#   - years = 2000‚Ä¶nu
#   - intervals indeholder start- og slutdato for hvert √•r

start_total <- as.Date("2000-01-01")
end_total   <- Sys.Date()

years <- seq(lubridate::year(start_total), lubridate::year(end_total))

intervals <- purrr::map(years, function(y) {
  start_y <- as.Date(paste0(y, "-01-01"))
  end_y   <- as.Date(paste0(y, "-12-31"))
  
  tibble(
    year  = y,
    start = max(start_y, start_total),
    end   = min(end_y,   end_total)
  )
}) |>
  bind_rows()

cat("Intervaller der hentes (√•r for √•r):\n")
print(intervals)

```

![](images/2026-01-04_01h21_54.png)

#### Hentning og samling af temp_dry-data

I dette trin hentes temperaturdata √•r for √•r ved hj√¶lp af den robuste hentefunktion, hvorefter alle delresultater samles i √©t samlet r√•t datas√¶t. Processen logger fremdrift pr. √•r og afsluttes med en samlet opt√¶lling af hentede observationer.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# Hent alle √•r og bind sammen ---------------------------------------------
# Vi looper over alle √•rsintervaller med fetch_temp_interval
# og binder resultatet sammen til √©n stor tibble: temp_dry_karup_raw.
temp_list <- purrr::map(
  seq_len(nrow(intervals)),
  function(i) {
    this_year  <- intervals$year[i]
    start_i    <- intervals$start[i]
    end_i      <- intervals$end[i]
    
    cat("\n============================================\n")
    cat("√Ör:", this_year, "‚Äì henter temp_dry\n")
    cat("============================================\n")
    
    fetch_temp_interval(start_i, end_i, max_retries = 3)
  }
)

temp_dry_karup_raw <- bind_rows(temp_list)

cat("\nSamlet antal r√¶kker hentet (temp_dry):", nrow(temp_dry_karup_raw), "\n")

tibble(temp_dry_karup_raw)
```

![](images/2026-01-04_01h13_29.png)

#### Teknisk klarg√∏ring af temp_dry RAW-data

Her udf√∏res udelukkende teknisk rensning af de hentede temperaturdata, hvor tidsstempler standardiseres, kolonner afgr√¶nses, og observationerne sorteres kronologisk. Datas√¶ttet forbliver et rent RAW-lag uden fortolkning eller mapping til andre datadimensioner.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# Vi laver kun ‚Äúteknisk‚Äù rensning:
#   - observed_raw ‚Üí POSIXct med UTC
#   - v√¶lger kun relevante kolonner
#   - sorterer kronologisk

# Dette er stadig RAW-laget (ingen tolkning, ingen mapping).
temp_dry_karup <- temp_dry_karup_raw |>
  mutate(
    observed = lubridate::ymd_hms(observed_raw, tz = "UTC")
  ) |>
  select(
    stationId,
    parameterId,
    observed,
    value,
    lon,
    lat
  ) |>
  arrange(observed)

cat("Dataset 'temp_dry_karup' er nu klar i R.\n")
print(head(temp_dry_karup))
View(temp_dry_karup, title = "temp_dry_karup ‚Äì temp_dry for Karup")
```

![](images/2026-01-04_01h14_55-01.png)

#### F√∏rste fulde load af temp_dry til Azure SQL

Denne del dokumenterer, hvordan det f√¶rdige RAW-datas√¶t for temp_dry flyttes fra R til Azure SQL som en etablering af faktalaget PBA01_Raw.fact_temp_dry_raw. Scriptet opretter forbindelse til databasen, klarg√∏r data med et load-timestamp og anvender en dummy-sikring, der afbryder, hvis der allerede findes data for station 06060 og parameteren temp_dry, s√• dubletter undg√•s. Selve uploaden sker som et append, og efter f√∏rste historiske indl√¶sning er tanken, at tabellen vedligeholdes via en separat inkrementel opdatering. I rapportkontekst vises scriptet som dokumentation for drift/ETL og afvikles ikke n√∏dvendigvis ved rendering.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# SCRIPT A ‚Äì F√òRSTE FULDE LOAD AF TEMP_DRY_KARUP TIL Azure SQL 
# (RAW-LAG)
#            ‚Üí PBA01_Raw.fact_temp_dry_raw
#            (med dummy-sikring mod dublet-upload)
# ====================================================================

# Script A flytter data fra R til Azure SQL (RAW-schema):
#   - opretter forbindelse til Azure
#   - sikrer at tabellen findes i PBA01_Raw
#   - tjekker om der allerede findes data for 06060/temp_dry
#   - uploader hele temp_dry_karup f√∏rste gang (fuld historisk load)
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, lubridate)
})

cat("=== TEMP_DRY ‚Äì fuld load til Azure SQL ===\n")

# L√¶s login til Azure SQL fra .Renviron -----------------------------------
# Login l√¶ses fra milj√∏variabler:
#   - AZURE_SQL_SERVER
#   - AZURE_SQL_DB
#   - AZURE_SQL_UID
#   - AZURE_SQL_PWD
server <- Sys.getenv("AZURE_SQL_SERVER")
db     <- Sys.getenv("AZURE_SQL_DB")
uid    <- Sys.getenv("AZURE_SQL_UID")
pwd    <- Sys.getenv("AZURE_SQL_PWD")
schema_raw <- "PBA01_Raw"

if (server == "" || db == "" || uid == "" || pwd == "") {
  stop("AZURE_SQL_* milj√∏variable er ikke sat korrekt.")
}

# Opret forbindelse til Azure SQL -----------------------------------------
# Vi √•bner ODBC-forbindelsen til Azure SQL. 
# Denne bruges kun til selve uploaden i Script A.
cat("Opretter forbindelse til Azure SQL\n")

con <- dbConnect(
  drv   = odbc::odbc(),
  Driver   = "ODBC Driver 18 for SQL Server",
  Server   = server,
  Database = db,
  UID      = uid,
  PWD      = pwd,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  Authentication = "SqlPassword",
  Port     = 1433
)

cat("Forbindelse oprettet.\n")

# Tjek at temp_dry_karup findes -------------------------------------------
# Vi sikrer os, at Script 1 er k√∏rt f√∏rst, ellers giver det ingen mening
# at uploade.
if (!exists("temp_dry_karup")) {
  dbDisconnect(con)
  stop("Objektet 'temp_dry_karup' findes ikke i R ‚Äì k√∏r Script 1 (API) f√∏rst.")
}

cat("Objekt 'temp_dry_karup' fundet i environment.\n")

# Forbered data til upload -------------------------------------------------
# Vi tilf√∏jer load_timestamp, s√• vi kan se, hvorn√•r r√¶kken blev indl√¶st
# i databasen (klassisk datalager-praksis).
temp_dry_upload <- temp_dry_karup |>
  mutate(
    load_timestamp = Sys.time()
  )

cat("Antal r√¶kker klar til upload:", nrow(temp_dry_upload), "\n")

# Opret tabel i SQL hvis den ikke findes ----------------------------------
# Her arbejder vi nu i RAW-schemaet PBA01_Raw (ikke l√¶ngere dbo).
# Strukturen matcher 1:1 de kolonner, vi uploader fra R.
cat("Sikrer at PBA01_Raw.fact_temp_dry_raw findes\n")

dbExecute(con, sprintf("
IF NOT EXISTS (
  SELECT 1
  FROM INFORMATION_SCHEMA.TABLES
  WHERE TABLE_SCHEMA = '%s'
    AND TABLE_NAME   = 'fact_temp_dry_raw'
)
BEGIN
  CREATE TABLE %s.fact_temp_dry_raw (
    temp_id        bigint IDENTITY(1,1) PRIMARY KEY,
    stationId      varchar(10),
    parameterId    varchar(50),
    observed       datetime2,
    value          float,
    lon            float,
    lat            float,
    load_timestamp datetime2
  );
END;
", schema_raw, schema_raw))

cat("Tabel tjek/creation f√¶rdig.\n")

# DUMMY-SIKRING ‚Äì tjek om der allerede er data for denne station/parameter
# Dummy-sikringen s√∏rger for, at vi ikke kommer til at fuld-loade oven i
# eksisterende data for 06060/temp_dry. 
# Tanken:
#   - fuld-load ‚Üí kun p√• en tom tabel
#   - derefter bruges Script B (inkrementel) fremover.
antal_eksisterende <- dbGetQuery(con, sprintf("
  SELECT COUNT(1) AS n
  FROM %s.fact_temp_dry_raw
  WHERE stationId   = '06060'
    AND parameterId = 'temp_dry';
", schema_raw))$n[1]

if (antal_eksisterende > 0) {
  cat("\n*** DUMMY-SIKRING AKTIVERET ***\n")
  cat("Tabellen ", schema_raw, ".fact_temp_dry_raw indeholder allerede ",
      antal_eksisterende,
      " r√¶kker for station 06060 / temp_dry.\n", sep = "")
  cat("Fuld-load afbrydes for at undg√• dubletter.\n")
  dbDisconnect(con)
  stop("Fuld-load m√• kun k√∏res p√• en tom tabel for denne station/parameter. Brug inkrementel script i stedet.")
}

cat("Dummy-sikring: ingen eksisterende data for 06060 / temp_dry ‚Äì fuld-load forts√¶tter.\n\n")

# Upload data (f√∏rste fulde load) -----------------------------------------
# Vi uploader alle r√¶kker fra temp_dry_upload til PBA01_Raw.fact_temp_dry_raw.
# append = TRUE, overwrite = FALSE er sikkert, fordi vi lige har konstateret,
# at der ikke ligger data for 06060/temp_dry i forvejen.
cat("Uploader data til PBA01_Raw.fact_temp_dry_raw\n")

dbWriteTable(
  con,
  name      = DBI::Id(schema = schema_raw, table = "fact_temp_dry_raw"),
  value     = temp_dry_upload,
  append    = TRUE,
  overwrite = FALSE
)

cat("F√∏rste fulde load f√¶rdig. Antal r√¶kker uploadet:", nrow(temp_dry_upload), "\n")

dbDisconnect(con)
cat("Forbindelse til Azure SQL lukket.\n")
```

#### Inkrementel opdatering af temp_dry (RAW-lag)

Denne del beskriver den l√∏bende vedligeholdelse af temperaturdata efter f√∏rste fulde indl√¶sning. Scriptet identificerer seneste registrerede observation i RAW-tabellen og henter herefter kun nye m√•linger fra DMI, som appenderes til databasen. Form√•let er at sikre et opdateret og konsistent datas√¶t uden genindl√¶sning af historiske data eller risiko for dubletter.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# SCRIPT B ‚Äì TEMP_DRY ‚Äì INKREMENTEL OPDATERING (KUN NYE R√ÜKKER)
# ====================================================================

# Script B bruges kun efter f√∏rste fuld-load:
#   - finder seneste observed i PBA01_Raw.fact_temp_dry_raw
#   - henter data fra DMI API fra (seneste + 1 sekund) til nu
#   - appender kun de nye r√¶kker til RAW-tabellen
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, lubridate, httr2, tibble, purrr)
})

cat("=== TEMP_DRY ‚Äì inkrementel opdatering ===\n")

# L√¶s login til Azure SQL --------------------------------------------------
# Igen l√¶ser vi loginoplysninger fra .Renviron.
server <- Sys.getenv("AZURE_SQL_SERVER")
db     <- Sys.getenv("AZURE_SQL_DB")
uid    <- Sys.getenv("AZURE_SQL_UID")
pwd    <- Sys.getenv("AZURE_SQL_PWD")
schema_raw <- "PBA01_Raw"

if (server == "" || db == "" || uid == "" || pwd == "") {
  stop("AZURE_SQL_* milj√∏variable er ikke sat korrekt.")
}

# Opret forbindelse til Azure SQL -----------------------------------------
# Vi √•bner en ny forbindelse, dedikeret til denne inkrementelle opdatering.
cat("Opretter forbindelse til Azure SQL\n")

con <- dbConnect(
  drv   = odbc::odbc(),
  Driver   = "ODBC Driver 18 for SQL Server",
  Server   = server,
  Database = db,
  UID      = uid,
  PWD      = pwd,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  Authentication = "SqlPassword",
  Port     = 1433
)

cat("Forbindelse oprettet.\n")

# Find seneste observed i SQL for Karup temp_dry 
# Vi finder den maksimale observed for 06060/temp_dry.
# Dette tidspunkt styrer, hvorfra vi henter nye r√¶kker i API‚Äôet.
cat("Finder seneste observed for station 06060, parameter 'temp_dry'\n")

last_obs_df <- dbGetQuery(con, sprintf("
  SELECT MAX(observed) AS last_obs
  FROM %s.fact_temp_dry_raw
  WHERE stationId   = '06060'
    AND parameterId = 'temp_dry';
", schema_raw))

last_obs <- last_obs_df$last_obs[1]

if (is.na(last_obs)) {
  dbDisconnect(con)
  stop("Der er ingen data i PBA01_Raw.fact_temp_dry_raw endnu ‚Äì k√∏r FULD LOAD f√∏rst.")
}

cat("Seneste observed i SQL (UTC):", as.character(last_obs), "\n")

# Ops√¶t DMI API ------------------------------------------------------
# Vi genbruger samme endpoint og parametre:
#   - station_id = 06060
#   - param_id   = temp_dry
dmi_api_key <- Sys.getenv("DMI_API_KEY")
if (dmi_api_key == "") {
  dbDisconnect(con)
  stop("DMI_API_KEY er ikke sat ‚Äì tjek .Renviron.")
}

base_url   <- "https://dmigw.govcloud.dk/v2/metObs/collections/observation/items"
station_id <- "06060"
param_id   <- "temp_dry"

# Defin√©r tidsinterval for nye data ---------------------------------------
# Start = seneste observed + 1 sekund, slut = nu (UTC).
# P√• den m√•de undg√•r vi dubletter men mister heller ikke data.
start_time <- with_tz(last_obs, "UTC") + seconds(1)
end_time   <- with_tz(Sys.time(), "UTC")

start_txt <- format(start_time, "%Y-%m-%dT%H:%M:%SZ")
end_txt   <- format(end_time,   "%Y-%m-%dT%H:%M:%SZ")

dt_range  <- paste0(start_txt, "/", end_txt)

cat("Henter nye data i interval:", dt_range, "\n")

# Kald DMI API for nye observationer --------------------------------------
# Vi kalder DMI med det dynamiske datetime-interval.
# Hvis API‚Äôet ikke returnerer features, stopper vi stille og roligt.
req <- request(base_url) |>
  req_url_query(
    stationId   = station_id,
    parameterId = param_id,
    datetime    = dt_range,
    limit       = 300000
  ) |>
  req_headers(
    "X-Gravitee-Api-Key" = dmi_api_key
  )

resp <- req_perform(req)
x    <- resp_body_json(resp, simplifyVector = FALSE)

if (is.null(x$features) || length(x$features) == 0) {
  cat("Ingen nye observationer fundet i DMI API.\n")
  dbDisconnect(con)
  cat("Forbindelse til Azure SQL lukket.\n")
} else {
  
  #  Map nye observationer til tibble --------------------------------------
  # Vi bygger new_temp i samme struktur som RAW-tabellen og tilf√∏jer
  # load_timestamp, s√• vi kan se, hvorn√•r opdateringen blev indl√¶st.
  new_temp <- purrr::map_dfr(
    x$features,
    function(feat) {
      tibble(
        stationId    = feat$properties$stationId,
        parameterId  = feat$properties$parameterId,
        observed     = lubridate::ymd_hms(feat$properties$observed, tz = "UTC"),
        value        = feat$properties$value,
        lon          = feat$geometry$coordinates[[1]],
        lat          = feat$geometry$coordinates[[2]]
      )
    }
  ) |>
    arrange(observed) |>
    mutate(load_timestamp = Sys.time())
  
  cat("Antal nye r√¶kker hentet:", nrow(new_temp), "\n")
  
  # Upload nye r√¶kker til RAW-tabellen ------------------------------------
  # Hvis der er nye r√¶kker, appender vi dem til PBA01_Raw.fact_temp_dry_raw.
  if (nrow(new_temp) > 0) {
    cat("Uploader nye r√¶kker til PBA01_Raw.fact_temp_dry_raw\n")
    
    dbWriteTable(
      con,
      name      = DBI::Id(schema = schema_raw, table = "fact_temp_dry_raw"),
      value     = new_temp,
      append    = TRUE,
      overwrite = FALSE
    )
    
    cat("Nye r√¶kker er nu indsat i PBA01_Raw.fact_temp_dry_raw.\n")
  } else {
    cat("Der var ingen nye r√¶kker at uploade.\n")
  }
  
  dbDisconnect(con)
  cat("Forbindelse til Azure SQL lukket.\n")
}

```

### API Vejret DMI Karup station - Weather (Karup 06060)

I dette afsnit etableres det r√• datagrundlag for vejrobservationer fra DMI for Flyvestation Karup (station 06060). Data hentes via metObs-API‚Äôet og gemmes ufortolket i RAW-laget i Azure SQL for at bevare en stabil og sporbar sandhedskilde. Der foretages kun n√∏dvendige tekniske tilpasninger, mens al egentlig fortolkning uds√¶ttes til senere datalag. Processen underst√∏tter b√•de et f√∏rste fuldt historisk load og efterf√∏lgende inkrementelle opdateringer.

#### Pakker og setup

```{r}
# Dette script:
#   - s√¶tter DMI-API‚Äôet op (endpoint, station, parameter),
#   - definerer en hj√¶lpefunktion til at hente √©t datointerval,
#   - laver √©n r√¶kke √•r-intervaller fra 2000 til i dag,
#   - looper √•r for √•r og binder alle resultater sammen til weather_karup.
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(httr2, jsonlite, dplyr, tibble, purrr, lubridate)
})

```

#### API-n√∏gle og adgangskontrol (DMI)

Dette trin sikrer, at DMI_API_KEY er korrekt sat i .Renviron, f√∏r der foretages kald til DMI‚Äôs API. Hvis n√∏glen mangler, stoppes scriptet tidligt for at undg√• fejlslagne eller uautoriserede API-kald.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# API-n√∏gle -------------------------------------------------------
# DMI_API_KEY skal ligge i .Renviron.
# Hvis den ikke findes, stopper vi tidligt med en klar fejl.
dmi_api_key <- Sys.getenv("DMI_API_KEY")
if (dmi_api_key == "") {
  stop("DMI_API_KEY er ikke sat ‚Äì tjek .Renviron og genstart R.")
}
```

#### Ops√¶tning af faste API-parametre (DMI metObs)

Her defineres de faste parametre, der anvendes ved alle API-kald til DMI‚Äôs metObs-endpoint, herunder station, parameter og base-URL. Det sikrer konsistens i dataudtr√¶kket og g√∏r senere √¶ndringer nemme og kontrollerede.

```{r}

# Ops√¶tning -------------------------------------------------------
# Faste parametre for alle kald:
#   - base_url: metObs observation-endpoint.
#   - station_id: 06060 (Karup).
#   - param_id: weather (present weather).
base_url   <- "https://dmigw.govcloud.dk/v2/metObs/collections/observation/items"
station_id <- "06060"      # Flyvestation Karup
param_id   <- "weather"    # present weather


```

#### Funktion: hent vejrobservationer pr. datointerval (DMI metObs)

Denne funktion bygger et ISO-datetime-interval og henter alle observationer for den valgte station/parameter via DMI‚Äôs API. Svaret foldes ud fra GeoJSON ‚Äúfeatures‚Äù til en ensartet tibble, og der returneres en tom tibble, hvis intervallet ikke giver data.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false

# Funktion til at hente √©t datointerval -----------------------------------
# fetch_weather_interval:
#   - bygger et datetime-interval i ISO-format (UTC),
#   - kalder DMI API for det interval,
#   - mapper hver feature til en r√¶kke i en tibble,
#   - returnerer tom tibble, hvis der ingen features er.
fetch_weather_interval <- function(start_date, end_date) {
  
  start_txt <- paste0(format(start_date, "%Y-%m-%d"), "T00:00:00Z")
  end_txt   <- paste0(format(end_date,   "%Y-%m-%d"), "T23:59:59Z")
  dt_range  <- paste0(start_txt, "/", end_txt)
  
  cat("  Henter interval:", start_txt, "‚Üí", end_txt, "\n")
  
  req <- request(base_url) |>
    req_url_query(
      stationId   = station_id,
      parameterId = param_id,
      datetime    = dt_range,
      limit       = 300000   # max pr. kald
    ) |>
    req_headers(
      "X-Gravitee-Api-Key" = dmi_api_key
    )
  
  resp <- req_perform(req)
  x    <- resp_body_json(resp, simplifyVector = FALSE)
  
  if (is.null(x$features) || length(x$features) == 0) {
    cat("    Ingen features i dette interval.\n")
    return(tibble())
  }
  
  out <- map_dfr(
    x$features,
    function(feat) {
      tibble(
        stationId    = feat$properties$stationId,
        parameterId  = feat$properties$parameterId,
        observed_raw = feat$properties$observed,
        value        = feat$properties$value,
        lon          = feat$geometry$coordinates[[1]],
        lat          = feat$geometry$coordinates[[2]]
      )
    }
  )
  
  cat("    R√¶kker hentet i dette interval:", nrow(out), "\n")
  out
}

```

#### √Örsopdeling af datointervaller (robuste API-kald)

Den samlede periode opdeles i √©t interval pr. √•r fra 2000 til dags dato. Det reducerer risiko for timeouts og g√∏r datatr√¶kket mere stabilt og reproducerbart.

```{r}

# Drift-/ETL-kode (k√∏res ikke ved render)


# Lav √•rs-intervaller fra 2000-01-01 til i dag ----------------------------
# For at g√∏re kaldene robuste laver vi √©t kald per √•r:
#   - start_total / end_total definerer hele perioden,
#   - years er listen over √•r,
#   - intervals indeholder start- og slutdato for hvert √•r.
start_total <- as.Date("2000-01-01")
end_total   <- Sys.Date()

years <- seq(year(start_total), year(end_total))

intervals <- map(years, function(y) {
  start_y <- as.Date(paste0(y, "-01-01"))
  end_y   <- as.Date(paste0(y, "-12-31"))
  
  tibble(
    start = max(start_y, start_total),
    end   = min(end_y,   end_total)
  )
}) |>
  bind_rows()

cat("Intervaller der hentes (√•r for √•r):\n")
print(intervals)

```

![](images/2026-01-04_02h46_06.png)

#### Indsamling af vejrdata p√• tv√¶rs af √•r

Her hentes vejrdata for hvert √•rsinterval via DMI-API‚Äôet og samles efterf√∏lgende i √©t samlet RAW-datas√¶t. Processen sikrer et fuldt og konsistent datagrundlag for hele perioden.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false

# Hent alle √•r og bind sammen ---------------------------------------------
# Vi looper over alle √•rs-intervaller med fetch_weather_interval
# og binder dem efterf√∏lgende til √©t samlet RAW-datas√¶t.
cat("üöÄ Starter hentning af weather for Karup...\n")

weather_list <- map2(intervals$start, intervals$end, fetch_weather_interval)

weather_karup_raw <- bind_rows(weather_list)

cat("Samlet antal r√¶kker hentet:", nrow(weather_karup_raw), "\n")
```

#### Rensning og samling af RAW-vejrdata

I dette trin foretages en let teknisk rensning af de hentede observationer og data samles i √©n kronologisk tabel. Datas√¶ttet forbliver i RAW-laget og anvendes som grundlag for senere overs√¶ttelser og feature-engineering.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)


# Rens og lav endelig (RAW) tabel -----------------------------------------
# Vi laver kun ‚Äúteknisk‚Äù rensning:
#   - observed_raw ‚Üí POSIXct (UTC),
#   - v√¶lger kun relevante kolonner,
#   - sorterer kronologisk.
# Det er stadig et RAW-lag ‚Äì ingen overs√¶ttelser til kode-tabeller endnu.
weather_karup <- weather_karup_raw |>
  mutate(
    observed = ymd_hms(observed_raw, tz = "UTC")
  ) |>
  select(
    stationId,
    parameterId,
    observed,
    value,
    lon,
    lat
  ) |>
  arrange(observed)

# Kig p√• data (udvikling / QA)
cat("F√∏rste r√¶kker af weather_karup:\n")
print(head(weather_karup))
View(weather_karup, title = "weather_karup ‚Äì weather for Karup")
glimpse(weather_karup)
# Valgfrit: gem til senere brug lokalt
# saveRDS(weather_karup, "weather_karup_2000_nu.rds")



```

![](images/2026-01-04_02h50_01.png)

#### RAW-load til Azure SQL: PBA01_Raw.fact_weather_raw

Dette script forbinder til Azure SQL og gennemf√∏rer et f√∏rste fuldt historisk load af `weather_karup` til RAW-tabellen. Inden upload sikrer det, at tabellen findes, og en dummy-sikring stopper k√∏rslen, hvis der allerede ligger data for station 06060 og parameter `weather` (s√• vi undg√•r dubletter).

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)


```

#### F√∏rste fulde upload af weather-data

I dette trin indl√¶ses alle r√¶kker fra det klargjorte RAW-datas√¶t i Azure SQL ved hj√¶lp af DBI. Uploaden foretages som et append, da dummy-sikringen p√• forh√•nd har sikret, at der ikke findes eksisterende data for den p√•g√¶ldende station og parameter.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)


# ====================================================================
# SCRIPT A ‚Äì F√òRSTE FULDE LOAD AF WEATHER_KARUP TIL Azure SQL (RAW-LAG)
#            ‚Üí PBA01_Raw.fact_weather_raw
#            (med dummy-sikring mod dublet-upload)
# ====================================================================
# Dette script:
#   - opretter forbindelse til Azure SQL,
#   - sikrer at PBA01_Raw.fact_weather_raw findes,
#   - tjekker om der allerede er data for 06060/weather,
#   - uploader hele weather_karup f√∏rste gang (fuld historisk load).
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, lubridate)
})

cat("üîß Starter fuld load af weather_karup til Azure SQL...\n")

# L√¶s login til Azure SQL fra .Renviron -----------------------------------
# Loginoplysninger l√¶ses fra milj√∏variabler, s√• de ikke hardcodes i scriptet.
server <- Sys.getenv("AZURE_SQL_SERVER")
db     <- Sys.getenv("AZURE_SQL_DB")
uid    <- Sys.getenv("AZURE_SQL_UID")
pwd    <- Sys.getenv("AZURE_SQL_PWD")
schema_raw <- "PBA01_Raw"

if (server == "" || db == "" || uid == "" || pwd == "") {
  stop("AZURE_SQL_* milj√∏variable er ikke sat korrekt.")
}

# Opret forbindelse til Azure SQL -----------------------------------------
# Vi √•bner en ODBC-forbindelse til Azure. 
# Denne bruges kun til fuld-load-delen.
cat("üåê Opretter forbindelse til Azure SQL...\n")

con <- dbConnect(
  drv   = odbc::odbc(),
  Driver   = "ODBC Driver 18 for SQL Server",
  Server   = server,
  Database = db,
  UID      = uid,
  PWD      = pwd,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  Authentication = "SqlPassword",
  Port     = 1433
)

cat("‚úÖ Forbindelse oprettet.\n")

# Forbered data til upload -------------------------------------------------
# Vi sikrer, at weather_karup findes (API-scriptet skal v√¶re k√∏rt f√∏rst).
if (!exists("weather_karup")) {
  dbDisconnect(con)
  stop("Objektet 'weather_karup' findes ikke i R ‚Äì k√∏r API-scriptet f√∏rst.")
}

# Vi tilf√∏jer load_timestamp, s√• vi kan se, hvorn√•r r√¶kkerne blev indl√¶st.
weather_upload <- weather_karup |>
  mutate(
    load_timestamp = Sys.time()
  )

cat("üì¶ Klar til upload. Antal r√¶kker i weather_upload:", nrow(weather_upload), "\n")

# Opret tabel i SQL hvis den ikke findes ----------------------------------
# Her arbejder vi nu i RAW-schemaet PBA01_Raw.
# Tabellen oprettes kun, hvis den ikke allerede findes.
cat("üß± Sikrer at PBA01_Raw.fact_weather_raw findes...\n")

dbExecute(con, sprintf("
IF NOT EXISTS (
  SELECT 1
  FROM INFORMATION_SCHEMA.TABLES
  WHERE TABLE_SCHEMA = '%s'
    AND TABLE_NAME   = 'fact_weather_raw'
)
BEGIN
  CREATE TABLE %s.fact_weather_raw (
    weather_id      bigint IDENTITY(1,1) PRIMARY KEY,
    stationId       varchar(10),
    parameterId     varchar(50),
    observed        datetime2,
    value           float,
    lon             float,
    lat             float,
    load_timestamp  datetime2
  );
END;
", schema_raw, schema_raw))

cat("‚úÖ Tabel", paste0(schema_raw, ".fact_weather_raw"), "klar.\n")

# DUMMY-SIKRING ‚Äì tjek om der allerede er data for denne station/parameter
# Dummy-sikringen sikrer, at vi kun fuld-loader √©n gang for 06060/weather.
antal_eksisterende <- dbGetQuery(con, sprintf("
  SELECT COUNT(1) AS n
  FROM %s.fact_weather_raw
  WHERE stationId   = '06060'
    AND parameterId = 'weather';
", schema_raw))$n[1]

if (antal_eksisterende > 0) {
  cat("\n*** DUMMY-SIKRING AKTIVERET ***\n")
  cat("Tabellen ", schema_raw, ".fact_weather_raw indeholder allerede ",
      antal_eksisterende,
      " r√¶kker for station 06060 / weather.\n", sep = "")
  cat("Fuld-load afbrydes for at undg√• dubletter.\n")
  dbDisconnect(con)
  stop("Fuld-load m√• kun k√∏res p√• en tom tabel for denne station/parameter. Brug inkrementel script i stedet.")
}

cat("Dummy-sikring: ingen eksisterende data for 06060 / weather ‚Äì fuld-load forts√¶tter.\n\n")

# Upload data (f√∏rste fulde load) -----------------------------------------
# Vi uploader alle r√¶kker fra weather_upload til PBA01_Raw.fact_weather_raw.
# append = TRUE, overwrite = FALSE er sikkert, fordi vi lige har verificeret,
# at der ikke ligger data for denne station/parameter.
cat("‚¨ÜÔ∏è Uploader weather-data til PBA01_Raw.fact_weather_raw...\n")

dbWriteTable(
  con,
  name      = DBI::Id(schema = schema_raw, table = "fact_weather_raw"),
  value     = weather_upload,
  append    = TRUE,
  overwrite = FALSE
)

cat("‚úÖ F√∏rste fulde load f√¶rdig. Antal r√¶kker uploadet:", nrow(weather_upload), "\n")

dbDisconnect(con)
cat("üîö Forbindelse til Azure SQL lukket.\n")

```

#### Inkrementel opdatering af RAW weather-data

Dette script finder seneste registrerede observation i RAW-tabellen og henter kun nye DMI-m√•linger siden da, s√• tabellen kan vedligeholdes uden dubletter. De nye r√¶kker transformeres til samme RAW-struktur og appenderes til PBA01_Raw.fact_weather_raw.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# =============================================================================
# SCRIPT B ‚Äì INKREMENTEL OPDATERING AF WEATHER-DATA I Azure SQL
#            ‚Üí PBA01_Raw.fact_weather_raw
# =============================================================================
# Dette script:
#   - finder seneste observed i PBA01_Raw.fact_weather_raw,
#   - bygger et datetime-interval fra (seneste + 1 sekund) til nu,
#   - henter kun de nye observationer fra DMI API,
#   - appender de nye r√¶kker til RAW-tabellen.
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, lubridate, httr2, tibble, purrr)
})

cat("üîÑ Starter inkrementel opdatering af weather for Karup (06060)...\n")

# 1) L√¶s login til Azure SQL --------------------------------------------------
server <- Sys.getenv("AZURE_SQL_SERVER")
db     <- Sys.getenv("AZURE_SQL_DB")
uid    <- Sys.getenv("AZURE_SQL_UID")
pwd    <- Sys.getenv("AZURE_SQL_PWD")
schema_raw <- "PBA01_Raw"

if (server == "" || db == "" || uid == "" || pwd == "") {
  stop("AZURE_SQL_* milj√∏variable er ikke sat korrekt.")
}

# 2) Opret forbindelse til Azure SQL -----------------------------------------
cat("üåê Opretter forbindelse til Azure SQL...\n")

con <- dbConnect(
  drv   = odbc::odbc(),
  Driver   = "ODBC Driver 18 for SQL Server",
  Server   = server,
  Database = db,
  UID      = uid,
  PWD      = pwd,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  Authentication = "SqlPassword",
  Port     = 1433
)

cat("‚úÖ Forbindelse oprettet.\n")

# 3) Find seneste observed i SQL for Karup weather ---------------------------
# Vi finder den maksimale observed for 06060/weather ‚Äì det er vores ‚Äúcut‚Äù.
last_obs_df <- dbGetQuery(con, sprintf("
  SELECT MAX(observed) AS last_obs
  FROM %s.fact_weather_raw
  WHERE stationId   = '06060'
    AND parameterId = 'weather';
", schema_raw))

last_obs <- last_obs_df$last_obs[1]

if (is.na(last_obs)) {
  dbDisconnect(con)
  stop("Der er ingen data i PBA01_Raw.fact_weather_raw endnu ‚Äì k√∏r fuld-load-scriptet f√∏rst.")
}

cat("üìå Seneste observed i SQL:", as.character(last_obs), "\n")

# 4) Ops√¶t DMI API -----------------------------------------------------------
dmi_api_key <- Sys.getenv("DMI_API_KEY")
if (dmi_api_key == "") {
  dbDisconnect(con)
  stop("DMI_API_KEY er ikke sat ‚Äì tjek .Renviron.")
}

base_url   <- "https://dmigw.govcloud.dk/v2/metObs/collections/observation/items"
station_id <- "06060"
param_id   <- "weather"

# 5) Defin√©r tidsinterval for nye data ---------------------------------------
# Start = seneste observed + 1 sekund, slut = nu (UTC),
# s√• vi undg√•r dubletter men ikke misser m√•linger.
start_time <- with_tz(last_obs, "UTC") + seconds(1)
end_time   <- with_tz(Sys.time(), "UTC")

start_txt <- format(start_time, "%Y-%m-%dT%H:%M:%SZ")
end_txt   <- format(end_time,   "%Y-%m-%dT%H:%M:%SZ")

dt_range  <- paste0(start_txt, "/", end_txt)

cat("üïí Henter nye data i interval:", dt_range, "\n")

# 6) Kald DMI API for nye observationer --------------------------------------
req <- request(base_url) |>
  req_url_query(
    stationId   = station_id,
    parameterId = param_id,
    datetime    = dt_range,
    limit       = 300000
  ) |>
  req_headers(
    "X-Gravitee-Api-Key" = dmi_api_key
  )

resp <- req_perform(req)
x    <- resp_body_json(resp, simplifyVector = FALSE)

if (is.null(x$features) || length(x$features) == 0) {
  cat("‚ÑπÔ∏è Ingen nye observationer fundet i DMI API.\n")
  dbDisconnect(con)
  cat("üîö Forbindelse lukket.\n")
} else {
  
  # Map nye observationer til tibble i samme struktur som RAW-tabellen
  new_weather <- map_dfr(
    x$features,
    function(feat) {
      tibble(
        stationId    = feat$properties$stationId,
        parameterId  = feat$properties$parameterId,
        observed     = ymd_hms(feat$properties$observed, tz = "UTC"),
        value        = feat$properties$value,
        lon          = feat$geometry$coordinates[[1]],
        lat          = feat$geometry$coordinates[[2]]
      )
    }
  ) |>
    arrange(observed) |>
    mutate(load_timestamp = Sys.time())
  
  cat("üì¶ Antal nye r√¶kker hentet:", nrow(new_weather), "\n")
  
  if (nrow(new_weather) > 0) {
    # Append nye r√¶kker til RAW-tabellen i PBA01_Raw
    dbWriteTable(
      con,
      name      = DBI::Id(schema = schema_raw, table = "fact_weather_raw"),
      value     = new_weather,
      append    = TRUE,
      overwrite = FALSE
    )
    cat("‚úÖ Nye r√¶kker er nu indsat i", paste0(schema_raw, ".fact_weather_raw"), "\n")
  } else {
    cat("‚ÑπÔ∏è Der var ingen nye r√¶kker at uploade.\n")
  }
  
  dbDisconnect(con)
  cat("üîö Forbindelse lukket.\n")
}

cat("‚úÖ Inkrementel opdatering af weather f√¶rdig.\n")


```

### Vejrkode DMI dimension

Scraping-koden til DMI‚Äôs weather-koder er oprindeligt udviklet p√• baggrund af en dokumentationsside, hvor indholdet blev leveret som klassiske, server-renderede HTML-tabeller. I slutningen af 2025 √¶ndrede DMI den tekniske implementering, s√• weather-koderne nu leveres som r√• tekst (TSV) i `<textarea>`-elementer og f√∏rst renderes via JavaScript i browseren.

I dette projekt er SQl/dimensionen opbygget p√• det dav√¶rende HTML-grundlag (f√∏r √¶ndringen). I QMD-filen anvendes derimod en opdateret scraping-metode, som passer til den nuv√¶rende version af DMI‚Äôs side, s√• koden fortsat kan k√∏res og dokumenteres reproducerbart.

#### Pakker og datakilde

Dette afsnit indl√¶ser de n√∏dvendige pakker til webscraping og datarensning samt definerer URL‚Äôen til DMI‚Äôs dokumentationsside for weather-koder.

```{r}

suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(
    rvest, dplyr, purrr, stringr, tidyr, tibble,
    readr, janitor
  )
}); cat("Pakker klar og loadedüôÇ\n")

url <- "https://www.dmi.dk/friedata/dokumentation/meteorological-observations-data#weather"
```

#### Udvidelse af kodeintervaller

Denne hj√¶lpefunktion normaliserer og udvider weather-kodeintervaller (fx 20‚Äì29) til enkeltkoder. Det sikrer en ensartet og entydig repr√¶sentation af alle koder i RAW-dimensionen

```{r}
# --------------------------------------------------------------------# Hj√¶lper: udvid intervaller (20‚Äì29, 20-29, 106‚àí109) -> 20..29
# --------------------------------------------------------------------
expand_code_range <- function(x) {
  x <- stringr::str_squish(as.character(x))
  x <- stringr::str_replace_all(x, "‚Äì", "-")
  x <- stringr::str_replace_all(x, "‚àí", "-")
  
  a <- suppressWarnings(as.integer(stringr::str_extract(x, "^\\d+")))
  b <- suppressWarnings(as.integer(stringr::str_extract(x, "(?<=-)\\d+$")))
  
  if (is.na(a)) return(integer(0))
  if (is.na(b)) return(a)
  seq.int(a, b)
}

```

#### Parser TSV-blok fra DMI-dokumentation

Funktionen omdanner √©n TSV-blok fra `<textarea>` til en ensartet tabel med `code_raw` og engelsk tekst, og logger eventuelle parsing-advarsler. Det g√∏r scraping robust over for DMI‚Äôs ‚Äúpseudo-tags‚Äù og sm√• √¶ndringer i tabellernes struktur.

```{r}

# --------------------------------------------------------------------
# Parser √©n TSV-blok fra textarea -> tibble(code_raw, weather_text_en)
#    + fanger parsing warnings og printer dem via cat()
# --------------------------------------------------------------------
parse_code_tsv_block <- function(txt, block_id = NA_character_) {
  txt <- as.character(txt)
  
  # Fjern DMI pseudo-tags inde i celler: [colspan=2], [rowspan=4] osv.
  txt <- stringr::str_replace_all(txt, "\\[[^\\]]+\\]", "")
  txt <- stringr::str_replace_all(txt, "\r", "")
  txt <- stringr::str_trim(txt)
  
  if (!nzchar(txt)) {
    return(tibble::tibble(block_id = block_id, code_raw = character(), weather_text_en = character()))
  }
  
  warn_msgs <- character(0)
  
  df <- withCallingHandlers(
    {
      readr::read_tsv(
        I(txt),
        col_types = readr::cols(.default = readr::col_character()),
        show_col_types = FALSE,
        progress = FALSE
      ) |>
        janitor::clean_names()
    },
    warning = function(w) {
      warn_msgs <<- c(warn_msgs, conditionMessage(w))
      invokeRestart("muffleWarning")
    }
  )
  
  # Print warnings p√¶nt (hvis nogen)
  if (length(warn_msgs) > 0) {
    cat("‚ö†Ô∏è Parsing-advarsler i TSV-blok", block_id, "(", length(warn_msgs), "):\n", sep = "")
    cat("   - ", paste(unique(warn_msgs), collapse = "\n   - "), "\n", sep = "")
  }
  
  # readr kan ogs√• have konkrete parsing-problemer (vroom)
  probs <- tryCatch(readr::problems(df), error = function(e) tibble::tibble())
  if (nrow(probs) > 0) {
    cat("‚ö†Ô∏è Konkrete parsing-problemer i TSV-blok", block_id, ":", nrow(probs), "\n", sep = "")
    print(utils::head(probs, 10))
  }
  
  # Vi leder efter en kodekolonne
  code_col <- intersect(names(df), c("code", "codes"))
  if (length(code_col) == 0) {
    return(tibble::tibble(block_id = block_id, code_raw = character(), weather_text_en = character()))
  }
  code_col <- code_col[1]
  
  value_cols <- setdiff(names(df), code_col)
  
  if (length(value_cols) == 0) {
    out <- df |>
      dplyr::transmute(
        block_id = block_id,
        code_raw = .data[[code_col]],
        weather_text_en = ""
      )
  } else {
    out <- df |>
      dplyr::mutate(
        weather_text_en = purrr::pmap_chr(
          dplyr::across(dplyr::all_of(value_cols)),
          ~{
            vals <- c(...)
            vals <- vals[!is.na(vals) & vals != ""]
            if (length(vals) == 0) "" else stringr::str_squish(paste(vals, collapse = " - "))
          }
        )
      ) |>
      dplyr::transmute(
        block_id = block_id,
        code_raw = .data[[code_col]],
        weather_text_en = weather_text_en
      )
  }
  
  out |>
    dplyr::mutate(
      code_raw = stringr::str_squish(as.character(code_raw)),
      weather_text_en = stringr::str_squish(as.character(weather_text_en))
    )
}

```

#### Hent WeatherCode via DMI‚Äôs `<textarea>`

Funktionen finder alle `textarea.dmi-input`, filtrerer til blokke med `Code/Codes`, parser dem og udvider intervaller til enkeltkoder, s√• vi ender med en komplet tabel for weather_code 0‚Äì199 inkl. kontrol af manglende koder.

```{r}
# --------------------------------------------------------------------
# NY METODE (2025+): hent alle textarea.dmi-input og filtr√©r til WeatherCode
# --------------------------------------------------------------------
try_get_weather_from_dmi_textarea <- function(url) {
  page <- rvest::read_html(url)
  
  textareas <- page |> rvest::html_elements("textarea.dmi-input")
  if (length(textareas) == 0) stop("Fandt ingen textarea.dmi-input. DMI kan have √¶ndret siden igen.")
  
  tsv_blocks <- purrr::map_chr(textareas, ~ rvest::html_text(.x, trim = TRUE))
  tsv_blocks <- tsv_blocks[nzchar(tsv_blocks)]
  
  cat("‚úÖ textarea-blokke fundet:", length(tsv_blocks), "\n")
  
  # Heuristik: behold kun blokke der ‚Äúligner‚Äù kode-tabeller (har Code/Codes header-linje)
  # (vi undg√•r parameterlisten med Name/Unit/Description osv.)
  first_line <- function(x) strsplit(x, "\n", fixed = TRUE)[[1]][1]
  keep <- vapply(tsv_blocks, function(x) {
    fl <- first_line(x)
    stringr::str_detect(fl, "^(Code|Codes)\\b")
  }, logical(1))
  
  tsv_blocks_codes <- tsv_blocks[keep]
  cat("‚úÖ Kandidat-blokke med Code/Codes header:", length(tsv_blocks_codes), "\n")
  
  parsed_all <- purrr::imap_dfr(
    tsv_blocks_codes,
    ~ parse_code_tsv_block(.x, block_id = as.character(.y))
  )
  
  if (nrow(parsed_all) == 0) stop("Kunne ikke parse nogen kode-tabeller fra textarea-blokkene (0 r√¶kker).")
  
  weather_code_0_199 <- parsed_all |>
    dplyr::mutate(
      code_raw_clean = stringr::str_replace_all(code_raw, "\\s", ""),
      code_raw_clean = stringr::str_replace_all(code_raw_clean, "‚Äì", "-"),
      code_raw_clean = stringr::str_replace_all(code_raw_clean, "‚àí", "-"),
      code_min = suppressWarnings(as.integer(stringr::str_extract(code_raw_clean, "^\\d+"))),
      code_max = suppressWarnings(as.integer(stringr::str_extract(code_raw_clean, "(?<=-)\\d+"))),
      code_max = dplyr::if_else(is.na(code_max), code_min, code_max)
    ) |>
    dplyr::filter(!is.na(code_min)) |>
    dplyr::rowwise() |>
    dplyr::mutate(weather_code = list(seq.int(code_min, code_max))) |>
    tidyr::unnest(weather_code) |>
    dplyr::ungroup() |>
    dplyr::filter(weather_code >= 0, weather_code <= 199) |>
    dplyr::arrange(weather_code, block_id) |>
    dplyr::distinct(weather_code, .keep_all = TRUE) |>
    dplyr::transmute(
      weather_code = as.integer(weather_code),
      weather_text_en = weather_text_en,
      code_raw = code_raw,
      block_id = block_id
    )
  
  cat("‚úÖ Weather codes (0‚Äì199) r√¶kker:", nrow(weather_code_0_199), "\n")
  
  missing_codes <- setdiff(0:199, weather_code_0_199$weather_code)
  cat("‚úÖ Manglende koder i 0‚Äì199:", length(missing_codes), "\n")
  if (length(missing_codes) > 0) print(missing_codes)
  
  weather_code_0_199
}

```

#### Legacy fallback: parse WeatherCode fra HTML-tabeller (f√∏r 2025)

Denne funktion fors√∏ger den gamle html_table()-baserede scraping, men forventes ofte at give 0 r√¶kker nu, fordi DMI‚Äôs WeatherCode-sektion typisk renderes via JavaScript i stedet for server-renderet HTML.

```{r}

# --------------------------------------------------------------------
# LEGACY METODE (f√∏r 2025): parse HTML-tabeller (dokumentation/fallback)
#    Forventning nu: ofte 0 r√¶kker pga JS-rendering p√• DMI.dk
# --------------------------------------------------------------------
try_get_weather_from_html_tables_legacy <- function(url) {
  suppressPackageStartupMessages(pacman::p_load(rvest, dplyr, purrr, stringr, tidyr, tibble))
  
  page <- rvest::read_html(url)
  
  # Fors√∏g at finde tabeller under Weather og f√∏r n√¶ste sektion
  # (denne XPath kan v√¶re ‚Äúrigtig‚Äù historisk, men giver ofte 0 p√• den nuv√¶rende side)
  weather_tables_nodes <- page |>
    rvest::html_elements(
      xpath = "//h3[@id='weather']/following::table
               [preceding::h3[@id='weather'] and following::h2[@id='stations']]"
    )
  
  cat("‚ÑπÔ∏è Legacy: HTML-tabeller fundet:", length(weather_tables_nodes), "\n")
  
  if (length(weather_tables_nodes) == 0) {
    return(tibble::tibble(weather_code = integer(), weather_text_en = character(), code_raw = character()))
  }
  
  parse_weather_table <- function(tbl_node) {
    tbl <- rvest::html_table(tbl_node, fill = TRUE)
    tbl <- tibble::as_tibble(tbl)
    names(tbl) <- make.unique(names(tbl))
    
    tbl <- tbl |>
      dplyr::rename(code_raw = 1) |>
      dplyr::mutate(code_raw = stringr::str_squish(as.character(code_raw)))
    
    if (ncol(tbl) == 1) {
      tbl <- tbl |> dplyr::mutate(weather_text_en = "")
    } else {
      value_cols <- names(tbl)[2:ncol(tbl)]
      tbl <- tbl |>
        dplyr::mutate(
          weather_text_en = purrr::pmap_chr(
            dplyr::across(dplyr::all_of(value_cols)),
            ~{
              vals <- c(...)
              vals <- vals[!is.na(vals) & vals != ""]
              if (length(vals) == 0) "" else stringr::str_squish(paste(vals, collapse = " - "))
            }
          )
        )
    }
    
    tbl |>
      dplyr::select(code_raw, weather_text_en)
  }
  
  weather_codes_raw <- purrr::map_dfr(weather_tables_nodes, parse_weather_table)
  
  weather_codes_raw |>
    dplyr::mutate(
      code_raw_clean = stringr::str_replace_all(code_raw, "\\s", ""),
      code_raw_clean = stringr::str_replace_all(code_raw_clean, "‚Äì", "-"),
      code_raw_clean = stringr::str_replace_all(code_raw_clean, "‚àí", "-"),
      code_min = suppressWarnings(as.integer(stringr::str_extract(code_raw_clean, "^\\d+"))),
      code_max = suppressWarnings(as.integer(stringr::str_extract(code_raw_clean, "(?<=-)\\d+"))),
      code_max = dplyr::if_else(is.na(code_max), code_min, code_max)
    ) |>
    dplyr::filter(!is.na(code_min)) |>
    dplyr::rowwise() |>
    dplyr::mutate(weather_code = list(seq.int(code_min, code_max))) |>
    tidyr::unnest(weather_code) |>
    dplyr::ungroup() |>
    dplyr::filter(weather_code >= 0, weather_code <= 199) |>
    dplyr::arrange(weather_code) |>
    dplyr::distinct(weather_code, .keep_all = TRUE) |>
    dplyr::transmute(
      weather_code = as.integer(weather_code),
      weather_text_en = weather_text_en,
      code_raw = code_raw
    )
}


```

#### K√∏rsel: hent WeatherCode 0‚Äì199 med ny metode (textarea/TSV)

Her eksekveres den opdaterede scraping, som udl√¶ser TSV-blokkene fra DMI-sidens `textarea.dmi-input` og samler dem til √©n tabel med WeatherCode 0‚Äì199 til videre brug i projektet.

```{r}
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# --------------------------------------------------------------------
# K√òR: Prim√¶rt ny metode. Legacy er kun dokumentation/fallback.
# --------------------------------------------------------------------
weather_code_0_199 <- try_get_weather_from_dmi_textarea(url)

# Hurtigt view
glimpse(weather_code_0_199)
```

![](images/2026-01-04_03h07_33.png)

#### Upload af RAW WeatherCode til Azure SQL

Her uploades den f√¶rdige RAW-dimension til PBA01_Raw.dim_weather_code_raw, som overskrives ved hver k√∏rsel. Dermed er RAW-laget altid synkroniseret med den seneste dokumentation fra DMI.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ============================================================
# Upload til Azure SQL (RAW ‚Üí PBA01_Raw.dim_weather_code_raw)
# ====================================================================
# Til sidst uploader vi RAW-dimensionen til Azure SQL. 
# Loginoplysninger hentes som milj√∏variable fra .Renviron, s√• scriptet kan
# ligge p√• GitHub uden at afsl√∏re credentials.
#
# Tabellen skrives til PBA01_Raw.dim_weather_code_raw og overskrives ved hver k√∏rsel,
# s√• RAW-laget altid er synkroniseret med den seneste dokumentation fra DMI.

# Hent login fra .Renviron
server   <- Sys.getenv("AZURE_SQL_SERVER")
database <- Sys.getenv("AZURE_SQL_DB")
uid      <- Sys.getenv("AZURE_SQL_UID")
pwd      <- Sys.getenv("AZURE_SQL_PWD")

if (any(!nzchar(c(server, database, uid, pwd)))) {
  stop("Manglende milj√∏variable: Tjek .Renviron og genstart R.")
}

schema <- "PBA01_Raw"          # ‚¨ÖÔ∏è SKIFTET FRA 'dbo' TIL 'PBA01_Raw'
table  <- "dim_weather_code_raw"

# Opret SQL-forbindelse
con_azure <- DBI::dbConnect(
  odbc::odbc(),
  .connection_string = paste0(
    "Driver={ODBC Driver 18 for SQL Server};",
    "Server=", server, ";",
    "Database=", database, ";",
    "Uid=", uid, ";",
    "Pwd=", pwd, ";",
    "Encrypt=yes;",
    "TrustServerCertificate=no;",
    "Connection Timeout=30;"
  )
)

# Upload tabel (RAW)
DBI::dbWriteTable(
  conn       = con_azure,
  name       = DBI::Id(schema = schema, table = table),
  value      = weather_code_0_199,
  overwrite  = TRUE
)

# Validering
head(DBI::dbReadTable(con_azure, DBI::Id(schema = schema, table = table)))

DBI::dbDisconnect(con_azure)
```

### API Befolkning Viborg (DST)

#### Pakker

Her loader vi de n√∏dvendige pakker til at hente befolkningsdata via API, l√¶se dem ind i R og lave et hurtigt RAW-tjek. Samtidig klarg√∏r vi DBI/ODBC-v√¶rkt√∏jerne, s√• datas√¶ttet senere kan flyttes videre til databasen uden at blande analyse og indl√¶sning.

```{r}
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(
    httr, rjstat, dplyr, DBI, odbc, tibble, rstudioapi, dbplyr, rlang
  )
});  cat("Pakker klar og loadedüôÇ\n")

```

#### Oprettelse af tidsdimension (kvartaler)

I dette afsnit konstrueres en samlet tidsvektor med alle kvartaler fra 2008K1 til 2025K4. Strengen bruges som input til API-kaldet, s√• hele perioden kan hentes konsistent i √©n samlet foresp√∏rgsel.

```{r}
√•r      <- 2008:2025
kvartal <- paste0("K", 1:4)

tid_v√¶rdier <- as.vector(t(outer(√•r, kvartal, paste0)))
tid_streng  <- paste(tid_v√¶rdier, collapse = ",")

```

#### Hent befolkningsdata fra Danmarks Statistik

Kalder FOLK1A-API‚Äôet for Viborg Kommune og henter kvartalsvise befolkningstal. Resultatet gemmes som RAW-data til senere rensning og brug i analyser.

```{r}

# ====================================================================
# Defin√©r API-endpoint og query-parametre
# ====================================================================

dst_url <- "https://api.statbank.dk/v1/data/FOLK1A/JSONSTAT"

dst_query <- list(
  "OMR√ÖDE"     = "791",           # Viborg
  "K√òN"        = "TOT,1,2",       # I alt, m√¶nd, kvinder
  "ALDER"      = "IALT",          # Alder i alt
  "CIVILSTAND" = "TOT,U,G,E,F",   # Civilstand
  "Tid"        = tid_streng
)

dst_response <- GET(
  url   = dst_url,
  query = dst_query
)

kode <- status_code(dst_response)
cat("HTTP-status fra DST:", kode, "\n")
if (kode != 200) {
  stop("Fejl ved kald til Danmarks Statistik (status = ", kode, ").")
}

dst_json <- dst_response |>
  content(as = "text", encoding = "UTF-8")

dst_parsed <- fromJSONstat(dst_json)

befolkning_viborg_raw <- dst_parsed[[1]] |>
  as_tibble()

cat("\nüìå Strukturen af befolkning_viborg_raw:\n")
str(befolkning_viborg_raw)

cat("\nüîç F√∏rste 10 r√¶kker:\n")
print(head(befolkning_viborg_raw, 10))


```

#### Sikring af database-login (.Renviron)

Tjekker at alle n√∏dvendige Azure SQL-milj√∏variabler er sat korrekt. Stopper processen og guider til ops√¶tning, hvis der mangler loginoplysninger.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# --------------------------------------------------------------------
# Safeguard .Renviron 
# --------------------------------------------------------------------

required_vars <- c(
  "AZURE_SQL_SERVER",
  "AZURE_SQL_DB",
  "AZURE_SQL_UID",
  "AZURE_SQL_PWD"
)

values <- Sys.getenv(required_vars)
missing_vars <- required_vars[values == ""]

if (length(missing_vars) > 0) {
  cat("‚ö†Ô∏è F√∏lgende milj√∏variabler mangler i .Renviron:\n")
  print(missing_vars)
  cat("\n√Öbner ~/.Renviron ‚Äì udfyld v√¶rdierne og gem filen.\n")
  
  file.edit("~/.Renviron")
  cat("üîÑ R genstartes nu via RStudio ‚Äì k√∏r scriptet igen bagefter.\n")
  
  if (rstudioapi::isAvailable()) {
    rstudioapi::restartSession()
  } else {
    stop("rstudioapi er ikke tilg√¶ngelig ‚Äì genstart R manuelt og k√∏r scriptet igen.")
  }
} else {
  cat("‚úî Alle n√∏dvendige .Renviron-variabler er sat.\n\n")
}

```

#### Azure SQL-forbindelse med retry

Opretter forbindelse til Azure SQL med automatisk genfors√∏g og stigende timeout. Sikrer robust forbindelse selv ved midlertidige netv√¶rks- eller timeout-fejl.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# --------------------------------------------------------------------
# Azure forbindelse med automatisk retry 
# --------------------------------------------------------------------

fors√∏g_max  <- 6
timeouts    <- c(60, 180, 200, 260, 360, 600)
delay_sec   <- 10
fors√∏g      <- 1
con         <- NULL

while (fors√∏g <= fors√∏g_max && is.null(con)) {
  
  timeout_brug <- timeouts[min(fors√∏g, length(timeouts))]
  
  cat(
    "Fors√∏g", fors√∏g,
    "p√• at forbinde (ConnectionTimeout =", timeout_brug, "sekunder)...\n"
  )
  
  con_try <- try(
    dbConnect(
      odbc::odbc(),
      driver   = "ODBC Driver 18 for SQL Server",
      server   = Sys.getenv("AZURE_SQL_SERVER"),
      database = Sys.getenv("AZURE_SQL_DB"),
      uid      = Sys.getenv("AZURE_SQL_UID"),
      pwd      = Sys.getenv("AZURE_SQL_PWD"),
      port     = 1433,
      Encrypt  = "yes",
      TrustServerCertificate = "no",
      ConnectionTimeout      = timeout_brug
    ),
    silent = TRUE
  )
  
  if (!inherits(con_try, "try-error")) {
    con <- con_try
    cat(
      "‚úî Forbundet til:", Sys.getenv("AZURE_SQL_DB"),
      "p√• fors√∏g", fors√∏g, "\n\n"
    )
  } else {
    cat("‚ùå Forbindelsen fejlede p√• fors√∏g", fors√∏g, "‚Äì pr√∏ver igen.\n")
    
    if (fors√∏g == fors√∏g_max) {
      stop("Kunne ikke forbinde til Azure SQL efter ", fors√∏g_max, " fors√∏g.\n")
    }
    
    cat("Venter", delay_sec, "sekunder f√∏r n√¶ste fors√∏g...\n\n")
    Sys.sleep(delay_sec)
    fors√∏g <- fors√∏g + 1
  }
}
```

#### F√∏rste load + incremental update (uden r√• SQL)

Her tjekker vi, om RAW-tabellen allerede findes i PBA01_Raw, og v√¶lger automatisk korrekt strategi. Hvis tabellen ikke findes, oprettes den og fuld-loades; hvis den findes, inds√¶ttes kun nye kvartaler for at undg√• dubletter.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# --------------------------------------------------------------------
# F√∏rste load + incremental update UDEN r√• SQL 
# --------------------------------------------------------------------

# Vi antager schemaet hedder pr√¶cist "PBA01_Raw".
# I stedet for et separat schema-tjek bruger vi dbExistsTable
# og fanger fejl med try(): hvis schema ikke findes, vil det fejle.

table_id <- Id(
  schema = "PBA01_Raw",
  table  = "fact_befolkning_Viborg_raw"
)

cat("üîç Tjekker adgang til schema 'PBA01_Raw' og tabellen via dbExistsTable...\n")

exists_try <- try(
  dbExistsTable(con, table_id),
  silent = TRUE
)

if (inherits(exists_try, "try-error")) {
  dbDisconnect(con)
  stop(
    "Kan ikke tilg√• 'PBA01_Raw.fact_befolkning_Viborg_raw'.\n",
    "Tjek at schemaet 'PBA01_Raw' findes, og at brugeren har rettigheder."
  )
}

tabel_findes <- isTRUE(exists_try)

if (!tabel_findes) {
  # ---------------------------------------------------------------
  # Tabel findes ikke ‚Üí f√∏rste fulde load
  # ---------------------------------------------------------------
  cat("üÜï F√∏rste load: Tabel findes ikke, opretter og inds√¶tter alle r√¶kker...\n")
  
  dbWriteTable(
    conn      = con,
    name      = table_id,
    value     = befolkning_viborg_raw,
    overwrite = FALSE,
    append    = FALSE
  )
  
  antal_r√¶kker  <- nrow(befolkning_viborg_raw)
  antal_v√¶rdier <- nrow(befolkning_viborg_raw) * ncol(befolkning_viborg_raw)
  
  cat("‚úÖ F√∏rste load gennemf√∏rt.\n")
  cat("üìä Antal r√¶kker indsat:", antal_r√¶kker, "\n")
  cat("üìä Antal v√¶rdier i tabellen:", antal_v√¶rdier, "\n\n")
  
} else {
  # ---------------------------------------------------------------
  # Tabel findes ‚Üí incremental update (ingen dupletter)
  # ---------------------------------------------------------------
  cat("üîÑ Tabel findes ‚Äì laver incremental update.\n")
  
  # dbplyr-reference til tabellen
  fact_tbl <- tbl(
    con,
    in_schema("PBA01_Raw", "fact_befolkning_Viborg_raw")
  )
  
  # hent et lille sample til at identificere Tid-kolonnen i SQL
  sample_sql <- fact_tbl |>
    head(5) |>
    collect()
  
  # find kolonnen der ligner YYYYKQ, fx 2008K1
  tid_kolonne_sql <- names(sample_sql)[sapply(sample_sql, function(col) {
    any(grepl("^[0-9]{4}K[1-4]$", as.character(col)))
  })][1]
  
  if (is.na(tid_kolonne_sql) || tid_kolonne_sql == "") {
    dbDisconnect(con)
    stop("Kunne ikke identificere Tid-kolonnen i SQL-tabellen (m√∏nster YYYYKQ).")
  }
  
  # find seneste kvartal i databasen (max p√• tid-kolonnen, uden NA-advarsel)
  seneste_tid_df <- fact_tbl |>
    summarise(max_tid = max(.data[[tid_kolonne_sql]], na.rm = TRUE)) |>
    collect()
  
  seneste_tid <- seneste_tid_df$max_tid[1]
  cat("Sidst kendte kvartal i databasen:", seneste_tid, "\n")
  
  # find tilsvarende Tid-kolonne i R-datas√¶ttet
  sample_r <- head(befolkning_viborg_raw, 5)
  tid_kolonne_r <- names(sample_r)[sapply(sample_r, function(col) {
    any(grepl("^[0-9]{4}K[1-4]$", as.character(col)))
  })][1]
  
  if (is.na(tid_kolonne_r) || tid_kolonne_r == "") {
    dbDisconnect(con)
    stop("Kunne ikke identificere Tid-kolonnen i R-datas√¶ttet (m√∏nster YYYYKQ).")
  }
  
  # v√¶lg nye r√¶kker i R (Tid > seneste_tid)
  if (is.na(seneste_tid)) {
    cat("‚ÑπÔ∏è Tabellen findes men er tom ‚Äì inds√¶tter alle r√¶kker fra API.\n")
    nye_r√¶kker <- befolkning_viborg_raw
  } else {
    nye_r√¶kker <- befolkning_viborg_raw |>
      filter(.data[[tid_kolonne_r]] > seneste_tid)
  }
  
  if (nrow(nye_r√¶kker) == 0) {
    cat("‚ÑπÔ∏è Ingen nye kvartaler at inds√¶tte ‚Äì alt er allerede i databasen.\n\n")
  } else {
    cat("‚¨Ü Inds√¶tter kun nye r√¶kker (antal):", nrow(nye_r√¶kker), "\n")
    
    dbWriteTable(
      conn      = con,
      name      = table_id,
      value     = nye_r√¶kker,
      append    = TRUE,
      overwrite = FALSE
    )
    
    cat("‚úÖ Incremental insert gennemf√∏rt.\n\n")
  }
}

# --------------------------------------------------------------------
# Luk forbindelsen p√¶nt 
# --------------------------------------------------------------------

dbDisconnect(con)
cat("üîå Forbindelsen til Azure SQL er nu lukket.\n")

```

## PBA02_Clean ‚Äì Transformation og klarg√∏ring af data

Fra og med PB02 p√•begyndes den egentlige datatransformation og -behandling. Data tr√¶kkes direkte fra et adgangsbegr√¶nset og betalt SQL-baseret datalager, hvorfor koden ikke kan afvikles via Quarto-rendering uden s√¶rskilt systemadgang. Dokumentationen fokuserer derfor p√• at p√•vise udf√∏rte transformationer gennem strukturelle outputs og screenshots. I denne fase renses data, datatyper behandles og standardiseres, og datas√¶ttene klarg√∏res til videre joins og analyse.

## Dimension vejrkode transformation

#### Pakker og milj√∏

Dette afsnit indl√¶ser de n√∏dvendige pakker til databaseadgang og datamanipulation. `pacman` anvendes for at sikre reproducerbarhed og et rent konsol-output uden un√∏dvendig st√∏j.

```{r}
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(
    DBI, odbc, dplyr, tibble, stringr
  )
}); cat("Pakker klar og loadedüôÇ\n")
```

#### Azure-forbindelse

Her oprettes forbindelsen til Azure SQL-databasen. Loginoplysninger hentes fra `.Renviron`, s√• credentials ikke indg√•r direkte i koden og l√∏sningen forbliver sikker og reproducerbar.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Azure forbindelse
# ====================================================================

# Vi forbinder til den samme Azure-database som i de √∏vrige scripts.
# Alle credentials hentes fra .Renviron, s√• der ikke st√•r passwords i koden.

con <- dbConnect(
  odbc::odbc(),
  driver   = "ODBC Driver 18 for SQL Server",
  server   = Sys.getenv("AZURE_SQL_SERVER"),
  database = Sys.getenv("AZURE_SQL_DB"),
  uid      = Sys.getenv("AZURE_SQL_UID"),
  pwd      = Sys.getenv("AZURE_SQL_PWD"),
  port     = 1433,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  ConnectionTimeout      = 30
); cat("Forbundet til:", Sys.getenv("AZURE_SQL_DB"), "\n\n")

```

#### Indl√¶s RAW-dimension fra PBA01_Raw.dim_weather_code_raw

Her hentes weather-kode-dimensionen direkte fra RAW-laget i Azure SQL (PBA01_Raw). Datas√¶ttet danner input til CLEAN-laget, hvor koderne efterf√∏lgende standardiseres og forsynes med danske beskrivelser.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)


# ====================================================================
# Hent RAW-dimension fra PBA01_Raw
# ====================================================================

# RAW-tabellen indeholder:
#   - weather_code     ‚Üí DMI‚Äôs vejrkode (0‚Äì199)
#   - weather_text_en  ‚Üí engelsk beskrivelse
#
# I CLEAN-laget √∏nsker vi:
#   - vejrkode         (samme kode)
#   - vejrbeskrivelse  (REN dansk tekst, ingen engelsk)
dim_weather_code_raw <- dbReadTable(
  con,
  DBI::Id(schema = "PBA01_Raw", table = "dim_weather_code_raw")
) |>
  select(vejrkode = weather_code) |>
  arrange(vejrkode)

cat("Antal koder i RAW:", nrow(dim_weather_code_raw), "\n\n")

```

#### Opslagstabel for danske vejrbeskrivelser (CLEAN)

De danske overs√¶ttelser af DMI‚Äôs engelske vejrbeskrivelser er udarbejdet manuelt med st√∏tte fra GitHub Copilot som sprogligt hj√¶lpemiddel. MS Copilot er udelukkende anvendt til formulering og konsistens i teksten ‚Äì den faglige fortolkning og den endelige validering af hver vejrkode er foretaget manuelt.

```{r}
# ====================================================================
# Opslagstabel med danske overs√¶ttelser
# ====================================================================
# Her hardcoder vi EN entydig dansk tekst pr. vejrkode (0‚Äì199).
# Denne tabel er ‚Äúsandheden‚Äù i CLEAN-laget og g√∏r, at vi ikke l√¶ngere
# er afh√¶ngige af den engelske tekst fra dokumentationen.


vejr_oversaettelse <- tibble::tribble(
  ~vejrkode, ~vejrbeskrivelse,
  
  0,  "Skyudvikling ikke observeret eller kan ikke observeres ‚Äì karakteristisk √¶ndring i himlens tilstand i den seneste time",
  1,  "Skyer er generelt ved at opl√∏ses eller blive mindre udviklede ‚Äì karakteristisk √¶ndring i himlens tilstand i den seneste time",
  2,  "Himlens tilstand stort set u√¶ndret i den seneste time",
  3,  "Skyer er generelt ved at dannes eller udvikles ‚Äì karakteristisk √¶ndring i himlens tilstand i den seneste time",
  4,  "Sigtbarhed reduceret af r√∏g (fx steppe- eller skovbrande, industrir√∏g eller vulkansk aske)",
  5,  "Dis",
  6,  "Udbredt st√∏v i luften, ikke l√∏ftet af vinden ved eller n√¶r stationen p√• observationstidspunktet",
  7,  "St√∏v eller sand l√∏ftet af vinden ved eller n√¶r stationen, men ingen veludviklede st√∏v- eller sandhvirvler og ingen st√∏v- eller sandstorm",
  8,  "Veludviklede st√∏v- eller sandhvirvler set ved eller n√¶r stationen i den seneste time eller p√• observationstidspunktet, men ingen st√∏v- eller sandstorm",
  9,  "St√∏v- eller sandstorm i sigte p√• observationstidspunktet eller ved stationen i den seneste time",
  
  10, "T√•ge",
  11, "Plettet lav t√•ge eller is-t√•ge ved stationen (p√• land ‚â§ ca. 2 m, til s√∏s ‚â§ ca. 10 m)",
  12, "Mere eller mindre sammenh√¶ngende lav t√•ge eller is-t√•ge ved stationen (p√• land ‚â§ ca. 2 m, til s√∏s ‚â§ ca. 10 m)",
  13, "Lyn synligt, men ingen torden h√∏res",
  14, "Nedb√∏r i sigte, men n√•r ikke jordoverfladen eller havoverfladen",
  15, "Nedb√∏r i sigte, n√•r jorden eller havet, men fjernt (ansl√•et > 5 km fra stationen)",
  16, "Nedb√∏r i sigte, n√•r jorden eller havet, t√¶t p√• men ikke ved selve stationen",
  17, "Tordenvejr, men ingen nedb√∏r p√• observationstidspunktet",
  18, "Byger (squalls) ved eller i n√¶rheden af stationen i den seneste time eller p√• observationstidspunktet",
  19, "Skyl√∏b/funnel cloud (tornado- eller vandsky) ved eller i n√¶rheden af stationen i den seneste time eller p√• observationstidspunktet",
  
  20, "St√∏vregn (ikke underafk√∏let) eller snefnug-korn, ikke som byger",
  21, "Regn (ikke underafk√∏let), ikke som byger",
  22, "Sne, ikke som byger",
  23, "Regn og sne eller iskorn, ikke som byger",
  24, "Underafk√∏let st√∏vregn eller underafk√∏let regn, ikke som byger",
  25, "Regnbyge(r)",
  26, "Snebyge(r) eller byge(r) af regn og sne",
  27, "Haglbyge(r) eller byge(r) af regn og hagl",
  28, "T√•ge eller is-t√•ge",
  29, "Tordenvejr (med eller uden nedb√∏r)",
  
  30, "Svag eller moderat st√∏v- eller sandstorm ‚Äì aftaget i den seneste time",
  31, "Svag eller moderat st√∏v- eller sandstorm ‚Äì ingen v√¶sentlig √¶ndring i den seneste time",
  32, "Svag eller moderat st√∏v- eller sandstorm ‚Äì begyndt eller tiltager i den seneste time",
  33, "Kraftig st√∏v- eller sandstorm ‚Äì aftaget i den seneste time",
  34, "Kraftig st√∏v- eller sandstorm ‚Äì ingen v√¶sentlig √¶ndring i den seneste time",
  35, "Kraftig st√∏v- eller sandstorm ‚Äì begyndt eller tiltager i den seneste time",
  36, "Svag eller moderat f√∏get sne, generelt lavt (under √∏jenh√∏jde)",
  37, "Kraftig f√∏gesne, generelt lavt (under √∏jenh√∏jde)",
  38, "Svag eller moderat f√∏get sne, generelt h√∏jt (over √∏jenh√∏jde)",
  39, "Kraftig f√∏gesne, generelt h√∏jt (over √∏jenh√∏jde)",
  
  40, "T√•ge eller is-t√•ge i det fjerne p√• observationstidspunktet, men ikke ved stationen i den seneste time ‚Äì t√•gen str√¶kker sig over observat√∏rens niveau",
  41, "T√•ge eller is-t√•ge i banker/pletter",
  42, "T√•ge eller is-t√•ge, himlen synlig ‚Äì er blevet tyndere i den seneste time",
  43, "T√•ge eller is-t√•ge, himlen usynlig ‚Äì er blevet tyndere i den seneste time",
  44, "T√•ge eller is-t√•ge, himlen synlig ‚Äì ingen v√¶sentlig √¶ndring i den seneste time",
  45, "T√•ge eller is-t√•ge, himlen usynlig ‚Äì ingen v√¶sentlig √¶ndring i den seneste time",
  46, "T√•ge eller is-t√•ge, himlen synlig ‚Äì er begyndt eller blevet t√¶ttere i den seneste time",
  47, "T√•ge eller is-t√•ge, himlen usynlig ‚Äì er begyndt eller blevet t√¶ttere i den seneste time",
  48, "T√•ge, der afs√¶tter rim, himlen synlig",
  49, "T√•ge, der afs√¶tter rim, himlen usynlig",
  
  50, "St√∏vregn, ikke underafk√∏let, periodisk ‚Äì svag p√• observationstidspunktet",
  51, "St√∏vregn, ikke underafk√∏let, vedvarende ‚Äì svag p√• observationstidspunktet",
  52, "St√∏vregn, ikke underafk√∏let, periodisk ‚Äì moderat p√• observationstidspunktet",
  53, "St√∏vregn, ikke underafk√∏let, vedvarende ‚Äì moderat p√• observationstidspunktet",
  54, "St√∏vregn, ikke underafk√∏let, periodisk ‚Äì kraftig (t√¶t) p√• observationstidspunktet",
  55, "St√∏vregn, ikke underafk√∏let, vedvarende ‚Äì kraftig (t√¶t) p√• observationstidspunktet",
  56, "Underafk√∏let st√∏vregn, svag",
  57, "Underafk√∏let st√∏vregn, moderat eller kraftig (t√¶t)",
  58, "St√∏vregn og regn, svag",
  59, "St√∏vregn og regn, moderat eller kraftig",
  
  60, "Regn, ikke underafk√∏let, periodisk ‚Äì svag p√• observationstidspunktet",
  61, "Regn, ikke underafk√∏let, vedvarende ‚Äì svag p√• observationstidspunktet",
  62, "Regn, ikke underafk√∏let, periodisk ‚Äì moderat p√• observationstidspunktet",
  63, "Regn, ikke underafk√∏let, vedvarende ‚Äì moderat p√• observationstidspunktet",
  64, "Regn, ikke underafk√∏let, periodisk ‚Äì kraftig p√• observationstidspunktet",
  65, "Regn, ikke underafk√∏let, vedvarende ‚Äì kraftig p√• observationstidspunktet",
  66, "Underafk√∏let regn, svag",
  67, "Underafk√∏let regn, moderat eller kraftig (t√¶t)",
  68, "Regn eller st√∏vregn og sne, svag",
  69, "Regn eller st√∏vregn og sne, moderat eller kraftig",
  
  70, "Periodisk snefald ‚Äì svagt p√• observationstidspunktet",
  71, "Vedvarende snefald ‚Äì svagt p√• observationstidspunktet",
  72, "Periodisk snefald ‚Äì moderat p√• observationstidspunktet",
  73, "Vedvarende snefald ‚Äì moderat p√• observationstidspunktet",
  74, "Periodisk snefald ‚Äì kraftigt p√• observationstidspunktet",
  75, "Vedvarende snefald ‚Äì kraftigt p√• observationstidspunktet",
  76, "Iskrystaller (diamond dust) med eller uden t√•ge",
  77, "Snefnug-korn (snow grains) med eller uden t√•ge",
  78, "Isolerede stjerneformede snekrystaller med eller uden t√•ge",
  79, "Iskorn (ice pellets)",
  
  80, "Regnbyge(r), svag",
  81, "Regnbyge(r), moderat eller kraftig",
  82, "Regnbyge(r), meget kraftig/voldsom",
  83, "Byge(r) af regn og sne blandet, svag",
  84, "Byge(r) af regn og sne blandet, moderat eller kraftig",
  85, "Snebyge(r), svag",
  86, "Snebyge(r), moderat eller kraftig",
  87, "Byge(r) af snefnug-korn eller sm√• hagl, med eller uden regn eller regn/sne blandet ‚Äì svag",
  88, "Byge(r) af snefnug-korn eller sm√• hagl, med eller uden regn eller regn/sne blandet ‚Äì moderat eller kraftig",
  89, "Haglbyge(r), med eller uden regn eller regn/sne blandet, ikke forbundet med torden ‚Äì svag",
  90, "Haglbyge(r), med eller uden regn eller regn/sne blandet, ikke forbundet med torden ‚Äì moderat eller kraftig",
  
  91, "Svag regn p√• observationstidspunktet ‚Äì tordenvejr i den seneste time men ikke nu",
  92, "Moderat eller kraftig regn p√• observationstidspunktet ‚Äì tordenvejr i den seneste time men ikke nu",
  93, "Svag sne, regn og sne blandet eller hagl p√• observationstidspunktet ‚Äì tordenvejr i den seneste time men ikke nu",
  94, "Moderat eller kraftig sne, regn og sne blandet eller hagl p√• observationstidspunktet ‚Äì tordenvejr i den seneste time men ikke nu",
  95, "Tordenvejr, svagt eller moderat, uden hagl men med regn og/eller sne p√• observationstidspunktet",
  96, "Tordenvejr, svagt eller moderat, med hagl p√• observationstidspunktet",
  97, "Tordenvejr, kraftigt, uden hagl men med regn og/eller sne p√• observationstidspunktet",
  98, "Tordenvejr sammen med st√∏v- eller sandstorm p√• observationstidspunktet",
  99, "Tordenvejr, kraftigt, med hagl p√• observationstidspunktet",
  
  100, "Ingen v√¶sentligt vejr observeret",
  101, "Skyer er generelt ved at opl√∏ses eller blive mindre udviklede i den seneste time",
  102, "Himlens tilstand p√• det hele u√¶ndret i den seneste time",
  103, "Skyer er generelt ved at dannes eller udvikles i den seneste time",
  104, "Dis eller r√∏g, eller st√∏v i luften ‚Äì sigtbarhed ‚â• 1 km",
  105, "Dis eller r√∏g, eller st√∏v i luften ‚Äì sigtbarhed < 1 km",
  106, "Reserveret kode (ikke anvendt)",
  107, "Reserveret kode (ikke anvendt)",
  108, "Reserveret kode (ikke anvendt)",
  109, "Reserveret kode (ikke anvendt)",
  110, "T√•ge",
  111, "Iskrystaller (diamond dust)",
  112, "Fjernt lyn",
  113, "Reserveret kode (ikke anvendt)",
  114, "Reserveret kode (ikke anvendt)",
  115, "Reserveret kode (ikke anvendt)",
  116, "Reserveret kode (ikke anvendt)",
  117, "Reserveret kode (ikke anvendt)",
  118, "Byger (squalls)",
  119, "Reserveret kode (ikke anvendt)",
  120, "T√•ge",
  121, "Nedb√∏r",
  122, "St√∏vregn (ikke underafk√∏let) eller snefnug-korn",
  123, "Regn (ikke underafk√∏let)",
  124, "Sne",
  125, "Underafk√∏let st√∏vregn eller underafk√∏let regn",
  126, "Tordenvejr (med eller uden nedb√∏r)",
  127, "F√∏get eller drivende sne eller sand",
  128, "F√∏get eller drivende sne eller sand, sigtbarhed ‚â• 1 km",
  129, "F√∏get eller drivende sne eller sand, sigtbarhed < 1 km",
  130, "T√•ge",
  131, "T√•ge eller is-t√•ge i banker/pletter",
  132, "T√•ge eller is-t√•ge ‚Äì er blevet tyndere i den seneste time",
  133, "T√•ge eller is-t√•ge ‚Äì ingen v√¶sentlig √¶ndring i den seneste time",
  134, "T√•ge eller is-t√•ge ‚Äì er begyndt eller blevet t√¶ttere i den seneste time",
  135, "T√•ge, der afs√¶tter rim",
  136, "Reserveret kode (ikke anvendt)",
  137, "Reserveret kode (ikke anvendt)",
  138, "Reserveret kode (ikke anvendt)",
  139, "Reserveret kode (ikke anvendt)",
  140, "Nedb√∏r",
  141, "Nedb√∏r, svag eller moderat",
  142, "Nedb√∏r, kraftig",
  143, "Flydende nedb√∏r, svag eller moderat",
  144, "Flydende nedb√∏r, kraftig",
  145, "Fast nedb√∏r, svag eller moderat",
  146, "Fast nedb√∏r, kraftig",
  147, "Underafk√∏let nedb√∏r, svag eller moderat",
  148, "Underafk√∏let nedb√∏r, kraftig",
  149, "Reserveret kode (ikke anvendt)",
  150, "St√∏vregn",
  151, "St√∏vregn, ikke underafk√∏let, svag",
  152, "St√∏vregn, ikke underafk√∏let, moderat",
  153, "St√∏vregn, ikke underafk√∏let, kraftig",
  154, "Underafk√∏let st√∏vregn, svag",
  155, "Underafk√∏let st√∏vregn, moderat",
  156, "Underafk√∏let st√∏vregn, kraftig",
  157, "St√∏vregn og regn, svag",
  158, "St√∏vregn og regn, moderat eller kraftig",
  159, "Reserveret kode (ikke anvendt)",
  160, "Regn",
  161, "Regn, ikke underafk√∏let, svag",
  162, "Regn, ikke underafk√∏let, moderat",
  163, "Regn, ikke underafk√∏let, kraftig",
  164, "Underafk√∏let regn, svag",
  165, "Underafk√∏let regn, moderat",
  166, "Underafk√∏let regn, kraftig",
  167, "Regn (eller st√∏vregn) og sne, svag",
  168, "Regn (eller st√∏vregn) og sne, moderat eller kraftig",
  169, "Reserveret kode (ikke anvendt)",
  170, "Sne",
  171, "Sne, svag",
  172, "Sne, moderat",
  173, "Sne, kraftig",
  174, "Iskorn, svage",
  175, "Iskorn, moderate",
  176, "Iskorn, kraftige",
  177, "Snefnug-korn",
  178, "Iskrystaller",
  179, "Reserveret kode (ikke anvendt)",
  180, "Byger eller periodisk nedb√∏r",
  181, "Regnbyge(r) eller periodisk regn, svag",
  182, "Regnbyge(r) eller periodisk regn, moderat",
  183, "Regnbyge(r) eller periodisk regn, kraftig",
  184, "Regnbyge(r) eller periodisk regn, meget kraftig/voldsom",
  185, "Snebyge(r) eller periodisk sne, svag",
  186, "Snebyge(r) eller periodisk sne, moderat",
  187, "Snebyge(r) eller periodisk sne, kraftig",
  188, "Reserveret kode (ikke anvendt)",
  189, "Hagl",
  190, "Tordenvejr",
  191, "Tordenvejr, svagt eller moderat, uden nedb√∏r",
  192, "Tordenvejr, svagt eller moderat, med regn- og/eller snebyger",
  193, "Tordenvejr, svagt eller moderat, med hagl",
  194, "Tordenvejr, kraftigt, uden nedb√∏r",
  195, "Tordenvejr, kraftigt, med regn- og/eller snebyger",
  196, "Tordenvejr, kraftigt, med hagl",
  197, "Reserveret kode (ikke anvendt)",
  198, "Reserveret kode (ikke anvendt)",
  199, "Tornado"
)

cat("Antal koder i overs√¶ttelsestabel:", nrow(vejr_oversaettelse), "\n\n")

```

#### Sammensmeltning af RAW-vejrkoder med danske beskrivelser

Her sammenkobles RAW-dimensionen fra PBA01_Raw med den danske overs√¶ttelsestabel for at danne en fuld CLEAN-dimension. Der udf√∏res samtidig et kvalitetstjek for at sikre, at alle vejrkoder har en entydig dansk beskrivelse.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)


# ====================================================================
# Join RAW-koder med dansk tekst ‚Üí CLEAN-dimension
# ====================================================================
dim_vejrkode_clean <- dim_weather_code_raw |>
  left_join(vejr_oversaettelse, by = "vejrkode")

# Kvalitetstjek: findes der koder i RAW uden dansk overs√¶ttelse?
mangler_da <- dim_vejrkode_clean |>
  filter(is.na(vejrbeskrivelse))

cat("Antal koder uden dansk tekst:", nrow(mangler_da), "\n\n")
if (nrow(mangler_da) > 0) {
  cat("‚ö†Ô∏è  F√∏lgende vejrkoder mangler overs√¶ttelse:\n")
  print(mangler_da$vejrkode)
}; glimpse(dim_vejrkode_clean);tibble(dim_vejrkode_clean)


```

![](images/2026-01-04_03h34_28.png)

#### Load til PBA02_Clean

Den f√¶rdigbearbejdede vejrkode-dimension gemmes nu i CLEAN-laget som PBA02_Clean.dim_vejrkode. Tabellen overskrives ved k√∏rsel, s√• CLEAN-laget altid afspejler den seneste, validerede version.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Skriv CLEAN-dimension til PBA02_Clean
# ====================================================================

# Vi l√¶gger nu den f√¶rdige dimension i CLEAN-skemaet:
#   PBA02_Clean.dim_vejrkode
#
# Schema + tabelnavn angives eksplicit med DBI::Id.
DBI::dbWriteTable(
  conn      = con,
  name      = DBI::Id(schema = "PBA02_Clean", table = "dim_vejrkode"),
  value     = dim_vejrkode_clean,
  overwrite = TRUE
)

cat("dim_vejrkode er nu uploadet til PBA02_Clean.dim_vejrkode ‚úî\n\n")

```

#### Afslutning og oprydning

Til sidst lukkes databaseforbindelsen korrekt for at frigive ressourcer og sikre en ren afslutning p√• scriptet.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Luk forbindelse
# ====================================================================
dbDisconnect(con)
cat("üîö Forbindelse lukket.\n")
```

## Dimension klubinfo transformation

I dette afsnit hentes klubinformation fra SQL-databasen og transformeres med henblik p√• at identificere og korrigere eventuelle fejl eller inkonsistenser. Form√•let er at sikre et konsistent og p√•lideligt datagrundlag til det videre forl√∏b, da klubdata ‚Äì herunder klubID ‚Äì udg√∏r en central n√∏gle, som anvendes p√• tv√¶rs af flere analyser i projektet.

#### Pakker

I dette afsnit indl√¶ses de n√∏dvendige R-pakker til databaseforbindelse og datamanipulation. suppressPackageStartupMessages() anvendes for at holde konsol-output ryddeligt og fokuseret.

```{r}
# ===========================================
# Pakker
# ===========================================
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, tibble, stringr)
}); cat("Pakker klar og loadedüôÇ\n")
```

#### Forbindelse til Azure SQL

Her oprettes forbindelsen til projektets Azure SQL-database ved brug af ODBC. Loginoplysninger hentes fra .Renviron for at sikre en sikker og reproducerbar ops√¶tning uden credentials i koden.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ================================================
# Azure forbindelse
# ================================================
con <- dbConnect(
  odbc::odbc(),
  driver   = "ODBC Driver 18 for SQL Server",
  server   = Sys.getenv("AZURE_SQL_SERVER"),
  database = Sys.getenv("AZURE_SQL_DB"),
  uid      = Sys.getenv("AZURE_SQL_UID"),
  pwd      = Sys.getenv("AZURE_SQL_PWD"),
  port     = 1433,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  ConnectionTimeout      = 30
)
cat("Forbundet til:", Sys.getenv("AZURE_SQL_DB"), "\n\n")
```

#### Hent RAW-klubdimension fra Azure (PBA01_Raw)

I dette trin indl√¶ses den r√• klubdimension direkte fra RAW-laget i Azure SQL. Datas√¶ttet danner grundlag for efterf√∏lgende rensning og standardisering af kluboplysninger, som anvendes bredt i analyserne.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

dim_vff_klubber_raw <- dbReadTable(
  con,
  DBI::Id(schema = "PBA01_Raw", table = "dim_vff_klubber_raw")
)

cat("R√¶kker i dim_vff_klubber_raw:", nrow(dim_vff_klubber_raw), "\n\n")
glimpse(dim_vff_klubber_raw)
```

![](images/2026-01-04_03h49_49.png)

#### Opbygning af CLEAN-klubdimension (stabilt KLUB-ID)

Her transformeres RAW-klubdata til en renset og konsistent klubdimension med et stabilt, entydigt klub-ID. Dimensionen standardiserer navne- og stadionoplysninger og fungerer som f√¶lles reference p√• tv√¶rs af analyser og joins.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# =============================================================================
# 3) Byg CLEAN-dimension med stabilt KLUB-ID
# =============================================================================
dim_vff_klubber_clean <- dim_vff_klubber_raw %>%
  arrange(kort) %>%                            # stabil r√¶kkef√∏lge
  mutate(
    klub_nr_l√∏bende = row_number(),           # 1, 2, 3, ...
    klub_id         = paste0(
      "KLUB",
      stringr::str_pad(klub_nr_l√∏bende, width = 3, pad = "0")
    ),
    klub_kort          = kort,
    klub_lang          = langt,
    klub_navn_officiel = navn,
    stadion_navn       = stadion,
    stadion_sponsor    = sponsor_stadion_navn,
    stadion_kapacitet  = tilskuere
  ) %>%
  select(
    klub_id,
    klub_kort,
    klub_lang,
    klub_navn_officiel,
    stadion_navn,
    stadion_sponsor,
    stadion_kapacitet
  )

cat("CLEAN-klubber f√∏r ekstra klubber:", nrow(dim_vff_klubber_clean), "\n\n")
view(dim_vff_klubber_clean)
```

![](images/2026-01-04_03h51_40.png)

#### Validering mod Superliga-program (manglende klubber og inkon)

Her hentes Superliga-kampprogrammet fra PBA02_Clean og bruges som reference til at validere, om alle hjemme-/udeklubkoder findes i dim_vff_klubber_clean. Outputtet identificerer eventuelle manglende klubkoder, s√• klubdimensionen kan suppleres f√∏r n√¶ste join-/analyse-lag.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Sammenlign med Superliga-programmet og find manglende klubkoder
# ====================================================================
fact_sl_clean <- dbReadTable(
  con,
  DBI::Id(schema = "PBA02_Clean", table = "fact_superliga_program_clean")
)

cat("R√¶kker i fact_superliga_program_clean:", nrow(fact_sl_clean), "\n\n")

alle_koder_program <- union(
  unique(fact_sl_clean$hjemmehold),
  unique(fact_sl_clean$udehold)
)
alle_koder_program <- alle_koder_program[!is.na(alle_koder_program)]

manglende_koder <- setdiff(alle_koder_program, dim_vff_klubber_clean$klub_kort)
manglende_koder <- sort(manglende_koder)

cat("Antal klubkoder i Superliga-programmet:", length(alle_koder_program), "\n")
cat("Antal klubkoder der mangler i dim_vff_klubber_clean:", length(manglende_koder), "\n\n")
print(manglende_koder)
```

![](images/2026-01-04_03h53_23.png)

#### Tilf√∏j dummy-klubber (supplering af manglende koder)

Hvis der findes klubkoder i Superliga-programmet, som ikke ligger i dim_vff_klubber_clean, oprettes der midlertidige ‚Äúdummy‚Äù-r√¶kker med nye KLUB-id‚Äôer. Det sikrer fuld d√¶kning i joins, s√• kampprogram og andre tabeller ikke mister r√¶kker pga. manglende klubdimension.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# =============================================================================
# Opret dummy-klubber for manglende koder (fx KBK)
# =============================================================================
if (length(manglende_koder) > 0) {
  
  # start n√¶ste l√∏benummer efter de eksisterende
  n√¶ste_nr <- nrow(dim_vff_klubber_clean) + 1L
  
  supplerende_klubber <- tibble(
    klub_nr_l√∏bende   = seq_len(length(manglende_koder)) + (n√¶ste_nr - 1L),
    klub_id           = paste0(
      "KLUB",
      stringr::str_pad(klub_nr_l√∏bende, width = 3, pad = "0")
    ),
    klub_kort         = manglende_koder,
    klub_lang         = manglende_koder,   # indtil vi evt. ved mere
    klub_navn_officiel= manglende_koder,   # indtil vi evt. ved mere
    stadion_navn      = NA_character_,
    stadion_sponsor   = NA_character_,
    stadion_kapacitet = NA_integer_
  ) %>%
    select(
      klub_id,
      klub_kort,
      klub_lang,
      klub_navn_officiel,
      stadion_navn,
      stadion_sponsor,
      stadion_kapacitet
    )
  
  cat("Tilf√∏jer", nrow(supplerende_klubber), "dummy-klub(ber):\n")
  print(supplerende_klubber)
  
  dim_vff_klubber_clean <- bind_rows(
    dim_vff_klubber_clean,
    supplerende_klubber
  )
}

cat("\nSamlet antal klubber i CLEAN-dimension:", nrow(dim_vff_klubber_clean), "\n\n")
glimpse(dim_vff_klubber_clean)
```

![](images/2026-01-04_03h54_12.png)

![](images/2026-01-04_03h55_24.png)

#### Upload af CLEAN-klubdimension og afslutning

Den f√¶rdige klubdimension skrives nu til PBA02_Clean.dim_vff_klubber_clean, s√• alle klubber har stabile og konsistente KLUB-ID‚Äôer. Herefter lukkes forbindelsen til Azure SQL.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# =============================================================================
# Upload til PBA02_Clean.dim_vff_klubber_clean + luk forbindelse 
# =============================================================================
dbWriteTable(
  conn      = con,
  name      = DBI::Id(schema = "PBA02_Clean", table = "dim_vff_klubber_clean"),
  value     = dim_vff_klubber_clean,
  overwrite = TRUE
)

cat("dim_vff_klubber_clean er nu uploadet til PBA02_Clean med opdaterede klubkoder og KLUB-ID.\n")

dbDisconnect(con)
cat("üîö Forbindelse lukket.\n")
```

## Superliga-program: PBA01_Raw ‚Üí PBA02_Clean Transformation

Her renses Superliga-programmet fra RAW-laget, s√• kun s√¶soner fra og med 2000/2001 medtages. Dato-feltet konverteres og fastholdes som Date, og der oprettes en date_key som tekst i format DDMM√Ö√Ö√Ö√Ö til sikre joins i de efterf√∏lgende lag.

#### Pakker og afh√¶ngigheder

Dette afsnit indl√¶ser de n√∏dvendige R-pakker til databaseforbindelse, datamanipulation og datoh√•ndtering. Pakkerne sikres installeret og loadet samlet for at g√∏re scriptet reproducerbart og overskueligt.

```{r}
# ====================================================================
# Pakker
# ====================================================================
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, tibble, stringr, lubridate, purrr, rlang)
}) ; cat("Pakker klar og loadedüôÇ\n")

```

#### Milj√∏- og credential-validering (.Renviron)

Dette afsnit fungerer som en sikkerhedskontrol, der verificerer, at alle n√∏dvendige Azure-milj√∏variabler er korrekt sat i .Renviron. Scriptet stopper tidligt med en klar fejl, hvis credentials mangler, for at undg√• mislykkede forbindelser senere i pipeline.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Safeguard: tjek at .Renviron er sat korrekt op
# ====================================================================
required_vars <- c("AZURE_SQL_SERVER", "AZURE_SQL_DB", "AZURE_SQL_UID", "AZURE_SQL_PWD")
values <- Sys.getenv(required_vars)
missing_vars <- required_vars[values == ""]

if (length(missing_vars) > 0) {
  cat("‚ö†Ô∏è F√∏lgende milj√∏variabler mangler i .Renviron:\n")
  print(missing_vars)
  stop("Udfyld .Renviron og k√∏r scriptet igen.")
}

```

#### Azure SQL-forbindelse (PBA-milj√∏)

Her oprettes en sikker ODBC-forbindelse til Azure SQL ved brug af credentials fra `.Renviron`. Forbindelsen anvendes til at l√¶se RAW-data og skrive videre til CLEAN-laget.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# ====================================================================
# Azure forbindelse
# ====================================================================
con <- DBI::dbConnect(
  odbc::odbc(),
  driver   = "ODBC Driver 18 for SQL Server",
  server   = Sys.getenv("AZURE_SQL_SERVER"),
  database = Sys.getenv("AZURE_SQL_DB"),
  uid      = Sys.getenv("AZURE_SQL_UID"),
  pwd      = Sys.getenv("AZURE_SQL_PWD"),
  port     = 1433,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  ConnectionTimeout      = 30
)
cat("‚úÖ Forbundet til:", Sys.getenv("AZURE_SQL_DB"), "\n\n")

```

#### Hent RAW Superliga-program (PBA01_Raw.fact_superliga_program_raw)

I dette trin indl√¶ses Superliga-programmet direkte fra RAW-laget i Azure SQL. Datas√¶ttet danner grundlag for efterf√∏lgende rensning, datokonvertering og filtrering til CLEAN-laget.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# ====================================================================
# Hent RAW Superliga-program fra PBA01_Raw
# ====================================================================
fact_superliga_program_raw <- DBI::dbReadTable(
  con,
  DBI::Id(schema = "PBA01_Raw", table = "fact_superliga_program_raw")
) %>% as_tibble()

cat("‚úî R√¶kker i PBA01_Raw.fact_superliga_program_raw:", nrow(fact_superliga_program_raw), "\n\n")
tibble(fact_superliga_program_raw)

```

![](images/2026-01-04_04h49_42.png)

#### Transform√©r RAW ‚Üí CLEAN: Superliga-program (PBA01_Raw.fact_superliga_program_raw ‚Üí PBA02_Clean.fact_superliga_program_clean)

Her standardiserer vi s√¶sonlogik, dato/tid (dato som Date), ugedag, hold/resultat, tilskuertal samt beregner vinderfelter og opretter date_key. Til sidst filtreres der til s√¶soner fra og med 2000/2001, og der udv√¶lges kun de felter, som skal bruges stabilt i CLEAN-laget.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# ====================================================================
# Clean: s√¶son, runde, ugedag, dato (Date), date_key (tekst), tid, hold, m√•l, vinder, tilskuertal
# ====================================================================

fact_superliga_program_clean <- fact_superliga_program_raw %>%
  mutate(
    # S√¶son-start√•r: Superliga-s√¶son g√•r typisk fra sommer til for√•r
    # 2000/2001 ‚Üí season_value = 2001 ‚Üí start_aar = 2000
    start_aar = if_else(!is.na(season_value) & season_value > 1991L, season_value - 1L, season_value),
    
    # Parse dato + tid fra "dd/mm HH:MM"
    dt_match = stringr::str_match(dato_tid_raw, "^(\\d{2})/(\\d{2})\\s+(\\d{2}:\\d{2})$")
  ) %>%
  mutate(
    dag    = suppressWarnings(as.integer(dt_match[, 2])),
    maaned = suppressWarnings(as.integer(dt_match[, 3])),
    klok   = dt_match[, 4],
    
    # √Ör afh√¶nger af m√•ned: juli‚Äìdecember = start_aar, jan‚Äìjuni = start_aar + 1
    aar = if_else(
      !is.na(maaned) & maaned >= 7L,
      start_aar,
      start_aar + 1L
    ),
    
    # Byg rigtig dato og tid
    kamp_dato = suppressWarnings(lubridate::make_date(year = aar, month = maaned, day = dag)),
    kamp_tid  = if_else(!is.na(klok) & stringr::str_detect(klok, "^\\d{2}:\\d{2}$"),
                        paste0(klok, ":00"),
                        NA_character_)
  ) %>%
  # ‚úÖ Kun s√¶soner fra 2000/2001 og frem
  filter(!is.na(start_aar) & start_aar >= 2000L) %>%
  # Ugedag: fix encoding og brug fulde danske navne
  mutate(
    ugedag = case_when(
      stringr::str_starts(ugedag_raw, "Man") ~ "Mandag",
      stringr::str_starts(ugedag_raw, "Tir") ~ "Tirsdag",
      stringr::str_starts(ugedag_raw, "Ons") ~ "Onsdag",
      stringr::str_starts(ugedag_raw, "Tor") ~ "Torsdag",
      stringr::str_starts(ugedag_raw, "Fre") ~ "Fredag",
      stringr::str_starts(ugedag_raw, "L")   ~ "L√∏rdag",
      stringr::str_starts(ugedag_raw, "S")   ~ "S√∏ndag",
      TRUE                                   ~ ugedag_raw
    )
  ) %>%
  # Split hold og resultat + tilskuertal
  mutate(
    # Split hjemme/ude fra "BIF-AB"
    hold_split = stringr::str_split(hold_raw, "-", n = 2),
    hjemmehold = purrr::map_chr(hold_split, ~ stringr::str_squish(.x[1] %||% NA_character_)),
    udehold    = purrr::map_chr(hold_split, ~ stringr::str_squish(.x[2] %||% NA_character_)),
    
    # Split m√•l fra "3-0"
    res_split  = stringr::str_match(resultat_raw, "^(\\d+)-(\\d+)$"),
    m√•l_hjemme = suppressWarnings(as.integer(res_split[, 2])),
    m√•l_ude    = suppressWarnings(as.integer(res_split[, 3])),
    
    # Tilskuertal: fjern punktummer, mellemrum, osv.
    tilskuertal = tilskuertal_raw %>%
      stringr::str_replace_all("[^0-9]", "") %>%
      na_if("") %>%
      suppressWarnings(as.integer())
  ) %>%
  # Beregn vinder og vinder_type
  mutate(
    vinder = case_when(
      m√•l_hjemme > m√•l_ude ~ hjemmehold,
      m√•l_ude    > m√•l_hjemme ~ udehold,
      TRUE ~ "Uafgjort"
    ),
    vinder_type = case_when(
      m√•l_hjemme > m√•l_ude ~ "Hjemmehold",
      m√•l_ude    > m√•l_hjemme ~ "Udehold",
      TRUE ~ "Uafgjort"
    )
  ) %>%
  transmute(
    # S√¶son som tekst: "2000/2001" og frem
    s√¶son = paste0(start_aar, "/", start_aar + 1L),
    
    # Runde som rent tal
    runde = suppressWarnings(as.integer(runde_nr)),
    
    # P√¶n ugedag
    ugedag = ugedag,
    
    # ‚úÖ dato som Date (ikke tekst)
    dato = kamp_dato,
    
    # ‚úÖ date_key som tekst (DDMM√Ö√Ö√Ö√Ö)
    date_key = format(kamp_dato, "%d%m%Y"),
    
    # Tid i HH:MM:SS
    tid = kamp_tid,
    
    # Hold og m√•l
    hjemmehold,
    udehold,
    m√•l_hjemme,
    m√•l_ude,
    
    # Vinder + om det er hjemme/ude/uafgjort
    vinder,
    vinder_type,
    
    # Tilskuere
    tilskuertal,
    
    # Dommer og tv-kanal videref√∏res som de er
    dommer   = dommer_raw,
    tv_kanal = tv_kanal_raw
  )

cat("‚úÖ F√¶rdig-cleanet Superliga-program (r√¶kker):", nrow(fact_superliga_program_clean), "\n\n")
tibble(fact_superliga_program_clean)

if (interactive()) View(fact_superliga_program_clean);glimpse(fact_superliga_program_clean)

```

![](images/2026-01-04_04h47_31.png)

#### Upload af CLEAN Superliga-program til Azure SQL (PBA02_Clean)

I dette trin skrives det f√¶rdigbehandlede Superliga-program til CLEAN-laget i Azure SQL. Tabellen overskrives ved hver k√∏rsel, s√• PBA02_Clean altid afspejler den seneste og konsistente version af datas√¶ttet.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# ====================================================================
# Upload til PBA02_Clean.fact_superliga_program_clean
# ====================================================================
DBI::dbWriteTable(
  conn      = con,
  name      = DBI::Id(schema = "PBA02_Clean", table = "fact_superliga_program_clean"),
  value     = fact_superliga_program_clean,
  overwrite = TRUE
)
cat("‚úÖ fact_superliga_program_clean uploadet til PBA02_Clean ‚úî\n")


```

#### Luk forbindelse til Azure SQL

Til sidst lukkes forbindelsen til Azure SQL for at frigive ressourcer og sikre en korrekt afslutning af scriptet.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# ===============================================================
# Luk forbindelse
# ====================================================================
DBI::dbDisconnect(con)
cat("üîö Forbindelse lukket.\n")
```

## DMI vejr obsevationer transformation

CLEAN-faktatabel med √©n entydig DMI-vejr¬≠observation pr. time, hvor den seneste observation inden for hver time bevares til analysebrug.

#### Pakkeload

Vi loader og sikrer at vi har n√∏dvendige pakker til transformation

```{r}

# ====================================================================
# Pakker
# ===================================================================
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, tibble, stringr, lubridate)
})

options(scipen = 999)
cat("Pakker er klar ‚úÖ\n\n")

```

#### **Azure SQL-forbindelse (CLEAN-lag)**

Her oprettes forbindelsen til Azure SQL via ODBC med credentials fra `.Renviron`, s√• data kan l√¶ses og skrives sikkert i CLEAN-laget.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Azure forbindelse
# ====================================================================
con <- DBI::dbConnect(
  odbc::odbc(),
  driver   = "ODBC Driver 18 for SQL Server",
  server   = Sys.getenv("AZURE_SQL_SERVER"),
  database = Sys.getenv("AZURE_SQL_DB"),
  uid      = Sys.getenv("AZURE_SQL_UID"),
  pwd      = Sys.getenv("AZURE_SQL_PWD"),
  port     = 1433,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  ConnectionTimeout      = 30
)
cat("Forbundet til:", Sys.getenv("AZURE_SQL_DB"), "‚úÖ\n\n")
on.exit(try(DBI::dbDisconnect(con), silent = TRUE), add = TRUE)

```

#### **Hent RAW vejrdata fra Azure SQL (PBA01_Raw)**

I dette trin indl√¶ses de r√• vejr-observationer direkte fra PBA01_Raw.fact_weather_raw. Datas√¶ttet bruges som grundlag for efterf√∏lgende aggregering og rensning i CLEAN-laget.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# ====================================================================
# Hent RAW
# ====================================================================
fact_weather_raw <- DBI::dbReadTable(
  con,
  DBI::Id(schema = "PBA01_Raw", table = "fact_weather_raw")
)

cat("R√¶kker i PBA01_Raw.fact_weather_raw:", nrow(fact_weather_raw), "\n\n")
stopifnot(all(c("observed","value") %in% names(fact_weather_raw)))
glimpse(fact_weather_raw)

```

![](images/2026-01-04_05h23_08.png)

#### Opbyg CLEAN vejrdata med timesopl√∏sning

Her aggregeres RAW-observationerne til √©n repr√¶sentativ vejrkode pr. time ved at v√¶lge den senest registrerede observation inden for hver time. Datas√¶ttet afgr√¶nses til tidsrummet 08‚Äì22 og struktureres til det endelige CLEAN-format.

```{r}
#| eval: false
#| error: false
#| message: false
#| warning: false
# Drift-/ETL-kode (k√∏res ikke ved render)
# ====================================================================
# Byg CLEAN med 1 observation pr time
# ====================================================================
fact_vejr_dmi <- fact_weather_raw |>
  mutate(
    # robust datetime
    observed_dt = as.POSIXct(observed, tz = "Europe/Copenhagen"),
    
    # time-bucket
    observed_hour = lubridate::floor_date(observed_dt, unit = "hour"),
    
    # vejrkode som int
    vejrkode = as.integer(value),
    
    # output-format (samme som f√∏r, bare HH:00:00)
    obs_dato = format(as.Date(observed_hour), "%d-%m-%Y"),
    tid      = format(observed_hour, "%H:00:00")
  ) |>
  # forretningsfilter: 08‚Äì22
  filter(
    tid >= "08:00:00",
    tid <= "22:00:00"
  ) |>
  # v√¶lg seneste observation i timen
  group_by(observed_hour) |>
  slice_max(order_by = observed_dt, n = 1, with_ties = FALSE) |>
  ungroup() |>
  # behold samme kolonner og samme r√¶kkef√∏lge som f√∏r
  transmute(
    obs_dato = obs_dato,
    tid      = tid,
    vejrkode = vejrkode
  )

cat("CLEAN fact_vejr_dmi bygget ‚úÖ\n")
cat("R√¶kker:", nrow(fact_vejr_dmi), "\n\n")
glimpse(fact_vejr_dmi)

```

![](images/2026-01-04_05h24_54.png){width="680"}

#### Sanity check: entydighed pr. dato og time datakonsistens

Datas√¶ttet har ikke en ensartet tidsopl√∏sning over hele perioden: √¶ldre observationer er registreret med ca. 3-timers interval, mens nyere data forekommer med op til 10-minutters interval. Dette h√•ndteres ved at aggregere til √©n observation pr. time i CLEAN-laget.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)
# ====================================================================
# 3) Sanity: Ingen dubletter pr obs_dato+tid
# ====================================================================
dup <- fact_vejr_dmi |>
  count(obs_dato, tid, sort = TRUE) |>
  filter(n > 1)

cat("Dublet-check (obs_dato + tid):", nrow(dup), "\n")
if (nrow(dup) > 0) {
  print(head(dup, 50))
  stop("STOP: Der er dubletter i CLEAN pr obs_dato+tid. Det m√• ikke ske.")
}

stopifnot(!any(is.na(fact_vejr_dmi$obs_dato)))
stopifnot(!any(is.na(fact_vejr_dmi$tid)))
stopifnot(!any(is.na(fact_vejr_dmi$vejrkode)))

```

![](images/2026-01-04_05h26_03.png){width="682"}

#### Upload af CLEAN vejrdata (PBA02_Clean.fact_vejr_dmi)

Den f√¶rdige CLEAN-fact overskrives nu i PBA02_Clean med √©n observation pr. time. Tabellen bevarer samme struktur som f√∏r, s√• efterf√∏lgende JoinReady- og analysetrin kan k√∏re u√¶ndret.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)
# ====================================================================
# 4) Upload CLEAN-fact til PBA02_Clean (samme tabelnavn som f√∏r)
# ====================================================================

DBI::dbWriteTable(
  conn      = con,
  name      = DBI::Id(schema = "PBA02_Clean", table = "fact_vejr_dmi"),
  value     = fact_vejr_dmi,
  overwrite = TRUE
)

view(fact_vejr_dmi)

cat("\n‚úÖ CLEAN-fact 'PBA02_Clean.fact_vejr_dmi' er nu opdateret (1 obs pr time).\n")
cat("JoinReady kan k√∏re u√¶ndret.\n")


```

## PBA03_join_ready

I PBA03_JoinReady udf√∏res den sidste og mest omfattende samling af data, hvor transformationer gennemf√∏res med henblik p√• at g√∏re analysefasen s√• smertefri som muligt. Det er i dette lag, at dimensioner og faktatabeller sammenkobles via stabile n√∏gler, og datastrukturen bringes p√• en form, der er direkte anvendelig i modeller, analyser og visualiseringer. Dermed minimeres yderligere databehandling i analysefasen og sikrer konsistens p√• tv√¶rs af det videre forl√∏b.

### VFF billetsalg transformation + joinklar

I dette afsnit transformerer vi billetsalgsdata fra databasens PBA01_Raw-lag og klarg√∏r datas√¶ttet til videre behandling og analyse i de efterf√∏lgende lag.

#### Pakkeload

Vi loader her alle de n√∏dvendige pakker for transformationen.

```{r}
# --------------------------------------------------------------------
# Pakker
# --------------------------------------------------------------------
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, rstudioapi, stringr)
}); cat("Pakker er nu klar ‚Äì god arbejdslyst!\n\n")


```

#### Safeguard ‚Äì validering af milj√∏variabler (.Renviron)

Dette afsnit sikrer, at alle n√∏dvendige Azure SQL-credentials er korrekt sat i `.Renviron`. Hvis en eller flere variabler mangler, stoppes scriptet tidligt for at undg√• fejlbeh√¶ftede forbindelser og utilsigtede k√∏rsler.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# --------------------------------------------------------------------
# Safeguard: tjek at .Renviron er sat korrekt op
# --------------------------------------------------------------------
required_vars <- c(
  "AZURE_SQL_SERVER",
  "AZURE_SQL_DB",
  "AZURE_SQL_UID",
  "AZURE_SQL_PWD"
)

values <- Sys.getenv(required_vars)
missing_vars <- required_vars[values == ""]

if (length(missing_vars) > 0) {
  cat("‚ö†Ô∏è  F√∏lgende milj√∏variabler mangler i .Renviron:\n")
  print(missing_vars); cat("\n√Öbner ~/.Renviron ‚Äì udfyld v√¶rdierne og gem filen.\n")
  
  file.edit("~/.Renviron")
  cat("üîÑ R genstartes nu via RStudio ‚Äì k√∏r scriptet igen bagefter.\n")
  
  if (rstudioapi::isAvailable()) {
    rstudioapi::restartSession()
  } else {
    stop("rstudioapi er ikke tilg√¶ngelig ‚Äì genstart R manuelt og k√∏r scriptet igen.")
  }
} else {
  cat("‚úî Alle n√∏dvendige .Renviron-variabler er sat.\n\n")
}
```

#### Azure SQL-forbindelse med automatisk retry-mekanisme

Dette afsnit opretter forbindelse til Azure SQL med indbygget retry-logik og gradvist √∏gede timeouts. Form√•let er at sikre en robust forbindelse i tilf√¶lde af midlertidige netv√¶rks- eller belastningsproblemer i Azure.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)
# -------------------------------------------------------------------------
# Azure forbindelse med automatisk retry
# -------------------------------------------------------------------------
fors√∏g_max <- 6
timeouts  <- c(60, 180, 200, 260, 360, 600)
delay_sec <- 10
fors√∏g    <- 1
con       <- NULL

while (fors√∏g <= fors√∏g_max && is.null(con)) {
  
  timeout_brug <- timeouts[min(fors√∏g, length(timeouts))]
  
  cat(
    "Fors√∏g", fors√∏g,
    "p√• at forbinde (ConnectionTimeout =", timeout_brug, "sekunder)...\n"
  )
  
  con_try <- try(
    DBI::dbConnect(
      odbc::odbc(),
      driver   = "ODBC Driver 18 for SQL Server",
      server   = Sys.getenv("AZURE_SQL_SERVER"),
      database = Sys.getenv("AZURE_SQL_DB"),
      uid      = Sys.getenv("AZURE_SQL_UID"),
      pwd      = Sys.getenv("AZURE_SQL_PWD"),
      port     = 1433,
      Encrypt  = "yes",
      TrustServerCertificate = "no",
      ConnectionTimeout      = timeout_brug
    ),
    silent = TRUE
  )
  
  if (!inherits(con_try, "try-error")) {
    con <- con_try
    cat("‚úÖ Forbundet til:", Sys.getenv("AZURE_SQL_DB"), "p√• fors√∏g", fors√∏g, "\n\n")
  } else {
    cat("‚ùå Forbindelsen fejlede p√• fors√∏g", fors√∏g, "\n")
    
    if (fors√∏g == fors√∏g_max) {
      stop("Kunne ikke forbinde til Azure SQL efter ", fors√∏g_max, " fors√∏g.\n")
    }
    
    cat("Venter", delay_sec, "sekunder f√∏r n√¶ste fors√∏g...\n\n")
    Sys.sleep(delay_sec)
    fors√∏g <- fors√∏g + 1
  }
}

```

#### Indl√¶sning af n√∏dvendige tabeller fra Azure SQL

I dette afsnit indl√¶ses n√∏dvendige dimension- og faktatabeller fra Azure SQL. Data hentes fra henholdsvis CLEAN-laget (klubdimension) og RAW-laget (billetsalg) og danner grundlag for den efterf√∏lgende transformation, normalisering og opbygning af JoinReady-datas√¶ttet.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
#| # Drift-/ETL-kode (k√∏res ikke ved render)

# -------------------------------------------------------------------------
# Hent tabeller
# -------------------------------------------------------------------------
dim_vff_klubber_clean <- DBI::dbReadTable(
  conn = con,
  name = DBI::Id(schema = "PBA02_Clean", table = "dim_vff_klubber_clean")
) |> as_tibble()

fact_vff_billetsalg_raw_SQLite <- DBI::dbReadTable(
  conn = con,
  name = DBI::Id(schema = "PBA01_Raw", table = "fact_vff_billetsalg_raw_SQLite")
) |> as_tibble()

cat("‚úî Hentet dim_vff_klubber_clean:", nrow(dim_vff_klubber_clean), "r√¶kker\n")
cat("‚úî Hentet fact_vff_billetsalg_raw_SQLite:", nrow(fact_vff_billetsalg_raw_SQLite), "r√¶kker\n\n")

dim_vff_klubber_clean
fact_vff_billetsalg_raw_SQLite

```

![](images/2026-01-04_06h01_38.png)

![](images/2026-01-04_06h04_01.png)

#### Clean og n√∏gleopbygning af billetsalgsdata (JoinReady)

I dette afsnit opdeles og normaliseres holdbetegnelser fra billetsalgsdata, kendte inkonsistenser korrigeres (SDR ‚Üí SJF), og der foretages opslag mod klub-dimensionen for at tilknytte stabile klub-ID‚Äôer. P√• baggrund heraf konstrueres et entydigt kamp-ID, som danner grundlag for videre joins og analyser i JoinReady-laget.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)

# -------------------------------------------------------------------------
# Clean: split hold + SDR->SJF + join klub_id + kamp_id
# -------------------------------------------------------------------------
dim_klub_key <- dim_vff_klubber_clean |>
  transmute(
    klub_id,
    klub_kort_norm = str_to_upper(str_squish(klub_kort))
  )

fact_VFF_Billetsalg_join_ready <- fact_vff_billetsalg_raw_SQLite |>
  mutate(
    hold_raw = hold,
    hold     = str_to_upper(str_squish(hold)),
    
    hjemme_kort = str_trim(str_extract(hold, "^[^-]+")),
    ude_kort    = str_trim(str_extract(hold, "[^-]+$")),
    
    hjemme_kort = if_else(hjemme_kort == "SDR", "SJF", hjemme_kort),
    ude_kort    = if_else(ude_kort    == "SDR", "SJF", ude_kort),
    
    hjemme_kort_norm = str_to_upper(str_squish(hjemme_kort)),
    ude_kort_norm    = str_to_upper(str_squish(ude_kort))
  ) |>
  left_join(dim_klub_key, by = c("hjemme_kort_norm" = "klub_kort_norm")) |>
  rename(hjemme_klubID = klub_id) |>
  left_join(dim_klub_key, by = c("ude_kort_norm" = "klub_kort_norm")) |>
  rename(ude_klubID = klub_id) |>
  select(-hjemme_kort_norm, -ude_kort_norm) |>
  mutate(
    kamp_id = paste0(s√¶son, "R", runde, hjemme_klubID)
  )

# --------------------------------------------------------------------
# Tjek + view
# --------------------------------------------------------------------
cat("Manglende hjemme_klubID:", sum(is.na(fact_VFF_Billetsalg_join_ready$hjemme_klubID)), "\n")
cat("Manglende ude_klubID:   ", sum(is.na(fact_VFF_Billetsalg_join_ready$ude_klubID)), "\n\n")

if (interactive()) View(fact_VFF_Billetsalg_join_ready)

fact_VFF_Billetsalg_join_ready 
```

![](images/2026-01-04_06h06_24.png)

#### Upload til JoinReady ‚Äì f√∏rste load og inkrementel opdatering

I dette trin indl√¶ses de klargjorte billetsalgsdata i JoinReady-laget. Tabellen oprettes ved f√∏rste k√∏rsel med et fuldt load, mens efterf√∏lgende k√∏rsler udelukkende inds√¶tter nye r√¶kker baseret p√• kamp_id, s√• dubletter undg√•s og tabellen holdes opdateret.

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)
# --------------------------------------------------------------------
# Upload: f√∏rste load / incremental (kun nye r√¶kker)
# --------------------------------------------------------------------
target_id <- DBI::Id(schema = "PBA03_JoinReady", table = "fact_VFF_Billetsalg_join_ready")

tabel_finds <- DBI::dbExistsTable(con, target_id)

# N√∏glen for incremental (din beslutning)
key_cols <- c("kamp_id")

if (!tabel_finds) {
  
  DBI::dbWriteTable(
    conn      = con,
    name      = target_id,
    value     = fact_VFF_Billetsalg_join_ready,
    overwrite = FALSE,
    append    = FALSE
  )
  
  cat("üèÅ F√∏rste load: Tabellen er oprettet og fuldt indl√¶st.\n")
  
} else {
  
  eksisterende_keys <- DBI::dbGetQuery(
    con,
    "SELECT DISTINCT kamp_id FROM PBA03_JoinReady.fact_VFF_Billetsalg_join_ready"
  ) |> as_tibble()
  
  nye_r√¶kker <- fact_VFF_Billetsalg_join_ready |>
    anti_join(eksisterende_keys, by = key_cols)
  
  if (nrow(nye_r√¶kker) == 0) {
    cat("‚Ñπ Ingen nye r√¶kker ‚Äì tabellen er up-to-date.\n")
  } else {
    
    DBI::dbWriteTable(
      conn   = con,
      name   = target_id,
      value  = nye_r√¶kker,
      append = TRUE
    )
    
    cat("‚úÖ Incremental update ‚Äì indsatte", nrow(nye_r√¶kker), "nye r√¶kker.\n")
  }
}

```

#### Lukning af forbindelse til Azure SQL

```{r}
#| eval: false
#| message: false
#| warning: false
#| error: false
# Drift-/ETL-kode (k√∏res ikke ved render)
# --------------------------------------------------------------------
# Luk forbindelse
# --------------------------------------------------------------------
DBI::dbDisconnect(con)
cat("üîí Forbindelse lukket.\n")
```

### Superliga program JoinReady

I dette afsnit sammenkobles det cleanede Superliga-program med klubdimensionen for at danne et join-ready faktadatas√¶t. Der tilf√∏jes entydige klub-ID‚Äôer, date_key og kamp_id, hvorefter data indl√¶ses i PBA03_JoinReady til videre analyse og modellering.

#### Pakker

Her henter vi og loader de n√∏dvendige pakker

```{r}
# ====================================================================
# Pakker
# ====================================================================
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, tibble, stringr, lubridate)
})

```

#### Azure SQL-forbindelse

Her oprettes en sikker forbindelse til den f√¶lles Azure SQL-database ved brug af credentials fra `.Renviron`, s√• efterf√∏lgende data kan hentes og skrives uden hardcodede loginoplysninger.

```{r}
# ====================================================================
# Azure forbindelse
# ====================================================================
con <- dbConnect(
  odbc::odbc(),
  driver   = "ODBC Driver 18 for SQL Server",
  server   = Sys.getenv("AZURE_SQL_SERVER"),
  database = Sys.getenv("AZURE_SQL_DB"),
  uid      = Sys.getenv("AZURE_SQL_UID"),
  pwd      = Sys.getenv("AZURE_SQL_PWD"),
  port     = 1433,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  ConnectionTimeout      = 120
)
cat("Forbundet til:", Sys.getenv("AZURE_SQL_DB"), "\n\n")

```

#### Indl√¶sning af CLEAN Superliga-program og klub-dimension

I dette trin hentes de rensede datas√¶t fra PBA02_Clean-laget: Superliga-kampprogrammet samt klub-dimensionen. Disse danner grundlaget for efterf√∏lgende joins og opbygning af join-ready tabeller.

```{r}

# ====================================================================
# Hent CLEAN Superliga-program + CLEAN klub-dimension
# ====================================================================
fact_sl_clean <- dbReadTable(
  con,
  DBI::Id(schema = "PBA02_Clean", table = "fact_superliga_program_clean")
)

dim_klub <- dbReadTable(
  con,
  DBI::Id(schema = "PBA02_Clean", table = "dim_vff_klubber_clean")
)

cat("R√¶kker hentet fra Superliga CLEAN:", nrow(fact_sl_clean), "\n")
cat("R√¶kker hentet fra Klub-dimension:", nrow(dim_klub), "\n\n")
tibble(fact_superliga_program_clean)
view(dim_klub)
```

![](images/2026-01-04_06h54_23.png)

![](images/2026-01-04_06h55_47.png)

#### Join af klub-ID p√• hjemme- og udehold

Her kobles klub-ID‚Äôer fra klub-dimensionen p√• b√•de hjemme- og udehold i Superliga-programmet. Dermed sikres entydige og stabile n√∏gler, som kan anvendes konsekvent i de efterf√∏lgende join-ready og analyse-trin.

```{r}
# ====================================================================
# Join klub-ID p√• hjemmehold og udehold
# ====================================================================
fact_joined <- fact_sl_clean %>%
  left_join(
    dim_klub %>% dplyr::select(klub_kort, klub_id),
    by = c("hjemmehold" = "klub_kort")
  ) %>%
  rename(klub_id_hjemme = klub_id) %>%
  left_join(
    dim_klub %>% dplyr::select(klub_kort, klub_id),
    by = c("udehold" = "klub_kort")
  ) %>%
  rename(klub_id_ude = klub_id) %>%
  mutate(
    hjemme_klubID = if_else(is.na(klub_id_hjemme), hjemmehold, klub_id_hjemme),
    ude_klubID    = if_else(is.na(klub_id_ude),    udehold,    klub_id_ude)
  ) %>%
  dplyr::select(-klub_id_hjemme, -klub_id_ude)

```

![](images/2026-01-04_06h58_14.png)

#### Oprettelse af date_key

Her oprettes en standardiseret date_key i formatet DDMM√Ö√Ö√Ö√Ö baseret p√• kampdatoen. Denne n√∏gle anvendes senere til joins mod datodimensioner og sikrer konsistent datoh√•ndtering p√• tv√¶rs af datas√¶t.

```{r}

# ====================================================================
# 3) Opret date_key (ddmmyyyy som tekst)
# ====================================================================
fact_joined <- fact_joined %>%
  mutate(
    date_key = format(as.Date(dato), "%d%m%Y")
  )

```

#### Oprettelse af kamp_id

Her konstrueres et entydigt kamp_id baseret p√• s√¶son, runde og klubidentifikatorer. N√∏glen bruges som prim√¶r reference i JoinReady-laget og sikrer stabile og entydige joins p√• tv√¶rs af fakta- og dimensionsdata.

```{r}
# ====================================================================
# 4) Tilf√∏j kamp_id (s√¶son + "R" + runde + "KLUB046")
# ====================================================================
VFF_ID <- "KLUB046"


fact_joined <- fact_joined %>%
  mutate(
    kamp_id = paste0(s√¶son, "R", runde, VFF_ID, ude_klubID)  
    )
view(fact_joined)


```

![](images/2026-01-04_07h03_33.png)

#### Klarg√∏ring af join-ready tabel

Datas√¶ttet struktureres endeligt ved at placere centrale n√∏gler (kamp_id, date_key og klub-ID‚Äôer) konsekvent. Resultatet er en join-ready tabel, klar til brug i videre analyse og modellering uden yderligere transformationer.

```{r}
# ====================================================================
# 5) Endelig join-ready tabel (med kamp_id)
# ====================================================================
fact_superliga_program_join_ready <- fact_joined %>%
  relocate(date_key, .after = dato) %>%
  relocate(hjemme_klubID, ude_klubID, .after = udehold) %>%
  relocate(kamp_id, .before = s√¶son)

if (interactive()) View(fact_superliga_program_join_ready)
```

#### Load til PBA03_JoinReady (f√∏rste load og inkrementel opdatering)

I dette trin indl√¶ses den join-ready tabel i PBA03_JoinReady. Tabellen oprettes ved f√∏rste k√∏rsel og opdateres herefter inkrementelt, s√• kun nye kampe (baseret p√• kamp_id) inds√¶ttes, hvilket sikrer stabil drift uden dubletter.

```{r}

# ====================================================================
# 6) F√∏rste load + incremental update (PBA03_JoinReady)
# ====================================================================
target_schema <- "PBA03_JoinReady"
target_table  <- "fact_superliga_program_join_ready"

table_id <- DBI::Id(
  schema = target_schema,
  table  = target_table
)

tabel_findes <- DBI::dbExistsTable(con, table_id)

if (!tabel_findes) {
  
  cat("üÜï F√∏rste load ‚Äì opretter tabel...\n")
  
  DBI::dbWriteTable(
    conn      = con,
    name      = table_id,
    value     = fact_superliga_program_join_ready,
    overwrite = TRUE
  )
  
  cat("‚úÖ F√∏rste load gennemf√∏rt. R√¶kker:", nrow(fact_superliga_program_join_ready), "\n\n")
  
} else {
  
  cat("üîÑ Incremental update (kun nye kamp_id)...\n")
  
  eksisterende_ids <- DBI::dbReadTable(con, table_id) %>%
    dplyr::select(kamp_id) %>%
    mutate(kamp_id = as.character(kamp_id))
  
  nye_r√¶kker <- fact_superliga_program_join_ready %>%
    mutate(kamp_id = as.character(kamp_id)) %>%
    dplyr::anti_join(eksisterende_ids, by = "kamp_id")
  
  if (nrow(nye_r√¶kker) > 0) {
    
    DBI::dbWriteTable(
      conn      = con,
      name      = table_id,
      value     = nye_r√¶kker,
      append    = TRUE
    )
    
    cat("‚¨Ü Indsat nye r√¶kker:", nrow(nye_r√¶kker), "\n\n")
    
  } else {
    cat("‚ÑπÔ∏è Ingen nye r√¶kker at inds√¶tte.\n\n")
  }
}

# ====================================================================
# Luk forbindelse
# ====================================================================
DBI::dbDisconnect(con)
cat("üîí Forbindelse lukket.\n")

```

### Vejrdata - PBA03_JoinReady (join-ready og standardiseret)

I dette afsnit samles rensede vejrdata med tilh√∏rende vejrkode-dimension og standardiseres til √©t konsistent observationsniveau. Datas√¶ttet reduceres til √©n repr√¶sentativ r√¶kke pr. dato via et fast tids-grid med fallback-logik og klarg√∏res herefter til direkte brug i analyse og modellering i PBA03_JoinReady.

#### Pakker

Her indl√¶ses de n√∏dvendige R-pakker, som bruges til databaseforbindelse, datah√•ndtering og tekstbehandling i det efterf√∏lgende join-ready-workflow.

```{r}
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, tibble, stringr)
})
```

#### Forbindelse til Azure SQL

I dette trin oprettes en sikker ODBC-forbindelse til det adgangsbegr√¶nsede Azure SQL-datalager, som anvendes til at hente og skrive data i JoinReady-laget.

```{r}
con <- dbConnect(
  odbc::odbc(),
  driver   = "ODBC Driver 18 for SQL Server",
  server   = Sys.getenv("AZURE_SQL_SERVER"),
  database = Sys.getenv("AZURE_SQL_DB"),
  uid      = Sys.getenv("AZURE_SQL_UID"),
  pwd      = Sys.getenv("AZURE_SQL_PWD"),
  port     = 1433,
  Encrypt  = "yes",
  TrustServerCertificate = "no",
  ConnectionTimeout      = 30
)
cat("Forbundet til:", Sys.getenv("AZURE_SQL_DB"), "\n\n")

```

#### Indl√¶sning af CLEAN vejrdata og kode-dimension

Her indl√¶ses de rensede vejr-observationer samt tilh√∏rende vejrkode-dimension fra PBA02_Clean. Disse datas√¶t danner grundlaget for efterf√∏lgende kvalitetstjek, joins og standardisering i JoinReady-laget.

```{r}

# ====================================================================
# Hent CLEAN FACT + DIM
# ====================================================================
fact_vejr_dmi <- dbReadTable(con, DBI::Id(schema = "PBA02_Clean", table = "fact_vejr_dmi"))
dim_vejrkode  <- dbReadTable(con, DBI::Id(schema = "PBA02_Clean", table = "dim_vejrkode"))

cat("R√¶kker i FACT (PBA02_Clean.fact_vejr_dmi):", nrow(fact_vejr_dmi), "\n")
cat("R√¶kker i DIM  (PBA02_Clean.dim_vejrkode):", nrow(dim_vejrkode), "\n\n")
str(fact_vejr_dmi); str(dim_vejrkode)

```

![](images/2026-01-04_08h33_38.png)

#### Kvalitetstjek af vejrkoder f√∏r join

I dette trin kontrolleres konsistensen mellem FACT- og DIM-laget ved at identificere vejrkoder, som forekommer i observationerne, men ikke har en tilsvarende beskrivelse i kode-dimensionen. Form√•let er at sikre et fuldt d√¶kkende og korrekt join uden tab af information.

```{r}

# =============================================================================
# Kvalitetstjek f√∏r join ‚Äì manglende matches (vejrkode)
# =============================================================================
distinct_fact_koder <- fact_vejr_dmi |>
  distinct(vejrkode) |>
  arrange(vejrkode)

distinct_dim_koder <- dim_vejrkode |>
  distinct(vejrkode) |>
  arrange(vejrkode)

cat("Antal unikke vejrkoder i FACT:", nrow(distinct_fact_koder), "\n")
cat("Antal unikke vejrkoder i DIM :", nrow(distinct_dim_koder), "\n")

faktiske_uden_dim <- distinct_fact_koder |>
  anti_join(distinct_dim_koder, by = "vejrkode")

cat("Antal vejrkoder i FACT uden match i DIM:", nrow(faktiske_uden_dim), "\n")
if (nrow(faktiske_uden_dim) > 0) {
  cat("Koder uden match i DIM:\n")
  print(faktiske_uden_dim)
}
cat("\n")

```

![](images/2026-01-04_08h39_16.png)

#### Join af vejrdata og kode-dimension (r√• JoinReady)

Her sammenk√¶des vejr-observationerne fra FACT-laget med vejrkode-dimensionen for at berige data med beskrivende metadata. Samtidig oprettes tekniske n√∏gler (date_key og Date_time_key), s√• datas√¶ttet er klar til videre standardisering og brug i JoinReady-laget.

```{r}

# ====================================================================
# Join FACT + DIM ‚Üí (raw) fact_vejr_join_ready
# ====================================================================
fact_vejr_join_ready_raw <- fact_vejr_dmi |>
  left_join(dim_vejrkode, by = "vejrkode") |>
  mutate(
    obs_dato_date = as.Date(obs_dato, format = "%d-%m-%Y"),
    date_key      = format(obs_dato_date, "%d%m%Y"),
    Date_time_key = paste0(date_key, str_replace_all(tid, ":", ""))
  ) |>
  select(-obs_dato_date)

cat("R√¶kker i fact_vejr_join_ready_raw:", nrow(fact_vejr_join_ready_raw), "\n\n")

# Valgfri: kig p√• r√• join
View(fact_vejr_join_ready_raw)

```

![](images/2026-01-04_08h39_53.png)

#### Kvalitetstjek af join ‚Äì manglende vejrbeskrivelser

I dette trin kontrolleres resultatet af joinet for manglende v√¶rdier i *vejrbeskrivelse*. Form√•let er at identificere eventuelle vejrkoder, som findes i FACT-tabellen, men ikke har en tilsvarende beskrivelse i DIM-tabellen, s√• datakvaliteten kan sikres f√∏r videre behandling.

```{r}

# ====================================================================
# 5) Kvalitetstjek efter join ‚Äì NA i vejrbeskrivelse
# ====================================================================
mangler_beskrivelse <- fact_vejr_join_ready_raw |>
  filter(is.na(vejrbeskrivelse)) |>
  distinct(vejrkode) |>
  arrange(vejrkode)

cat("Antal forskellige koder uden vejrbeskrivelse efter join:", nrow(mangler_beskrivelse), "\n")
if (nrow(mangler_beskrivelse) > 0) {
  cat("Disse koder mangler tekst i DIM:\n")
  print(mangler_beskrivelse)
}
cat("\n")

```

#### Standardisering af vejrdata til √©n repr√¶sentativ observation pr. dato

I dette trin aggregeres vejrdata, s√• der kun forekommer √©n observation pr. dato. Observationen v√¶lges som den m√•ling, der ligger t√¶ttest p√• kl. 14:00, som anvendes som et fast og sammenligneligt referencepunkt p√• dagen. Der prioriteres f√∏rst faste observationstidspunkter kl. 08:00, 11:00, 14:00 og 17:00. For datoer, hvor disse observationer mangler, anvendes en kontrolleret fallback-strategi, hvor den n√¶rmeste tilg√¶ngelige observation v√¶lges. Processen sikrer et konsistent, fuldst√¶ndigt og metodisk gennemsigtigt datas√¶t med pr√¶cis √©n r√¶kke pr. dato.

```{r}
# Form√•l:
#   At sikre pr√¶cis √©n vejr-observation pr. dato ved at:
#   1) Prioritere faste observationstidspunkter (grid)
#   2) V√¶lge den observation, der ligger t√¶ttest p√• kl. 14:00
#   3) Anvende fallback-logik p√• dage uden grid-observationer
#
# Grid-tider: 08:00, 11:00, 14:00, 17:00
# M√•ltidspunkt (anker): 14:00
# ====================================================================

# Definerer de faste tidspunkter, som betragtes som strukturerede observationer
faste_tider <- c("08:00:00", "11:00:00", "14:00:00", "17:00:00")

# Definerer det tidspunkt p√• dagen, som alle observationer m√•les op imod
m√•l_tid     <- "14:00:00"

# Hj√¶lpefunktion:
# Konverterer et klokkesl√¶t (HH:MM:SS) til sekunder siden midnat
# Dette g√∏r det muligt at beregne numeriske tidsafstande
tid_til_sek <- function(x) {
  as.integer(substr(x, 1, 2)) * 3600 +
    as.integer(substr(x, 4, 5)) * 60 +
    as.integer(substr(x, 7, 8))
}

# Konverterer m√•ltidspunktet (14:00) til sekunder
m√•l_sec <- tid_til_sek(m√•l_tid)

# Udtr√¶kker alle unikke datoer i datas√¶ttet
# Denne liste fungerer som facit for, hvor mange r√¶kker slutdatas√¶ttet skal have
dato_liste <- fact_vejr_join_ready_raw |>
  distinct(obs_dato) |>
  arrange(obs_dato)

# Antal unikke datoer (bruges senere til konsistenskontrol)
n_dage <- nrow(dato_liste)

# Filtrerer datas√¶ttet ned til kun observationer p√• grid-tidspunkter
# Disse observationer prioriteres i f√∏rste omgang
vejr_grid <- fact_vejr_join_ready_raw |>
  filter(tid %in% faste_tider) |>
  arrange(obs_dato, tid)

# Sikkerhedstjek:
# Der m√• h√∏jst v√¶re √©n observation pr. dato + tid i grid
# Hvis ikke, stoppes scriptet for at undg√• tvetydighed
dup_grid <- vejr_grid |>
  count(obs_dato, tid) |>
  filter(n != 1)
stopifnot(nrow(dup_grid) == 0)

# For hver dato i grid:
# 1) Beregnes afstanden til kl. 14:00
# 2) Observationen t√¶ttest p√• 14:00 v√¶lges
# 3) R√¶kken m√¶rkes som "grid" for fuld sporbarhed
vejr_1_pr_dato_fra_grid <- vejr_grid |>
  mutate(
    tid_sec = tid_til_sek(tid),
    dist_sec = abs(tid_sec - m√•l_sec),
    dist_minutes_to_14 = dist_sec / 60,
    source = "grid"
  ) |>
  group_by(obs_dato) |>
  arrange(dist_sec, .by_group = TRUE) |>
  slice(1) |>
  ungroup() |>
  select(-tid_sec, -dist_sec)

# Identificerer de datoer, som ikke har nogen grid-observation
# Disse datoer kr√¶ver fallback-logik
mangler_datoer <- dato_liste |>
  anti_join(vejr_1_pr_dato_fra_grid |> distinct(obs_dato), by = "obs_dato")

# Fallback:
# For dage uden grid-observation v√¶lges den observation,
# der (uanset tidspunkt) ligger t√¶ttest p√• kl. 14:00
# Disse r√¶kker m√¶rkes eksplicit som "fallback"
vejr_fallback <- fact_vejr_join_ready_raw |>
  semi_join(mangler_datoer, by = "obs_dato") |>
  mutate(
    tid_sec = tid_til_sek(tid),
    dist_sec = abs(tid_sec - m√•l_sec),
    dist_minutes_to_14 = dist_sec / 60,
    source = "fallback"
  ) |>
  group_by(obs_dato) |>
  arrange(dist_sec, .by_group = TRUE) |>
  slice(1) |>
  ungroup() |>
  select(-tid_sec, -dist_sec)

# Samler grid-udvalg og fallback-udvalg til √©t endeligt datas√¶t
# Sorteres kronologisk
fact_vejr_join_ready <- bind_rows(vejr_1_pr_dato_fra_grid, vejr_fallback) |>
  arrange(obs_dato)

# Endelige sikkerhedstjek:
# 1) Antal r√¶kker skal svare til antal unikke datoer
# 2) Ingen dato m√• forekomme mere end √©n gang
stopifnot(nrow(fact_vejr_join_ready) == n_dage)
stopifnot(sum(duplicated(fact_vejr_join_ready$obs_dato)) == 0)

# Konsol-output som dokumenterer resultatet og datakvaliteten
cat("‚úÖ Endeligt datas√¶t (1 pr dato) klar.\n")
cat("R√¶kker:", nrow(fact_vejr_join_ready), " | Distinct datoer:", n_dage, "\n")
cat("Fordeling source:\n")
print(dplyr::count(fact_vejr_join_ready, source))
cat("\n")

# Mulighed for manuelt eftersyn f√∏r upload / videre brug
View(fact_vejr_join_ready)
```

![](images/2026-01-04_08h41_13.png)

#### Upload af f√¶rdigbehandlet vejrdata og lukning af databaseforbindelse

I dette trin uploades det endelige og validerede vejr-datas√¶t til databasen i laget PBA03_JoinReady. Tabellen overskrives bevidst for at sikre, at databasen altid indeholder den seneste og konsistente version af data med pr√¶cis √©n observation pr. dato. Efter uploaden lukkes databaseforbindelsen kontrolleret for at frigive ressourcer og afslutte ETL-processen korrekt.

```{r}
# ====================================================================
# Upload (overwrite) til PBA03_JoinReady.fact_vejr_join_ready
# ====================================================================
DBI::dbWriteTable(
  conn      = con,
  name      = DBI::Id(schema = "PBA03_JoinReady", table = "fact_vejr_join_ready"),
  value     = fact_vejr_join_ready,
  overwrite = TRUE
)

cat("‚úÖ Tabel 'PBA03_JoinReady.fact_vejr_join_ready' er nu skrevet til databasen (overwrite).\n\n")

# ====================================================================
# Luk forbindelse
# ====================================================================
dbDisconnect(con)
cat("üîö Forbindelse lukket.\n")
```

### Temperaturdata: klarg√∏ring fra RAW til join-ready

Dette afsnit beskriver behandlingen af r√• temperaturdata fra PBA01_Raw til en reduceret og analytisk anvendelig FACT-tabel i PBA03_JoinReady. Data opdeles i dato og tid, filtreres til et kamprelevant tidsvindue og reduceres fra h√∏j frekvens til udvalgte heltimer, som vurderes mest repr√¶sentative for kampafvikling. Processen er bevidst designet til at balancere datakvalitet, relevans og stabilitet i efterf√∏lgende joins og modeller, og afsluttes med upload af en konsistent join-ready tabel, der kan anvendes direkte i analyse- og modelarbejde.

#### Pakker

N√∏dvendige pakker loades og g√∏res klar til anvendelse.

```{r}
suppressPackageStartupMessages({
  if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
  pacman::p_load(DBI, odbc, dplyr, tibble, tidyverse, stringr)
}); cat("Pakker er klar ‚úÖ\n\n")


```

#### Sikkerhedstjek af databasekonfiguration via .Renviron

Dette trin fungerer som et tidligt safeguard, der sikrer, at alle n√∏dvendige milj√∏variabler til Azure SQL-forbindelsen er korrekt defineret i .Renviron-filen. Inden der oprettes forbindelse til databasen, kontrolleres det eksplicit, at servernavn, databasenavn, brugernavn og adgangskode er tilg√¶ngelige. Mangler √©n eller flere variabler, afbrydes processen bevidst for at undg√• skjulte forbindelsesfejl og uklare runtime-problemer senere i ETL-flowet.

```{r}

# ====================================================================
# Safeguard: .Renviron
# ====================================================================
ensure_renviron <- function() {
  required_vars <- c("AZURE_SQL_SERVER","AZURE_SQL_DB","AZURE_SQL_UID","AZURE_SQL_PWD")
  values <- Sys.getenv(required_vars)
  missing_vars <- required_vars[values == ""]
  if (length(missing_vars) > 0) {
    stop("‚ùå Manglende milj√∏variabler i .Renviron: ", paste(missing_vars, collapse = ", "))
  }
  cat("‚úî .Renviron OK\n\n")
}
ensure_renviron()


```

#### Stabil oprettelse af forbindelse til Azure SQL med retry-logik

Dette trin etablerer en robust databaseforbindelse til Azure SQL ved hj√¶lp af en eksplicit retry-mekanisme. Forbindelsen fors√∏ges oprettet flere gange med gradvist l√¶ngere timeouts for at h√•ndtere midlertidige netv√¶rksproblemer og kendte Azure-fejl som 08S01 communication link failure eller timeout. Processen sikrer, at ETL-flowet enten opn√•r en stabil forbindelse eller stopper kontrolleret med en klar fejlmeddelelse, frem for at fejle uforudsigeligt midt i k√∏rslen.

```{r}

# ====================================================================# Retry-funktion: Azure SQL (stabil connection)
# ====================================================================
connect_azure_retry <- function(
    fors√∏g_max = 6,
    timeouts  = c(60, 180, 200, 260, 360, 600),
    delay_sec = 10
) {
  fors√∏g <- 1
  con <- NULL
  
  while (fors√∏g <= fors√∏g_max && is.null(con)) {
    timeout_brug <- timeouts[min(fors√∏g, length(timeouts))]
    cat("Fors√∏g", fors√∏g, "- ConnectionTimeout =", timeout_brug, "sekunder\n")
    
    con_try <- try(
      DBI::dbConnect(
        odbc::odbc(),
        driver   = "ODBC Driver 18 for SQL Server",
        server   = Sys.getenv("AZURE_SQL_SERVER"),
        database = Sys.getenv("AZURE_SQL_DB"),
        uid      = Sys.getenv("AZURE_SQL_UID"),
        pwd      = Sys.getenv("AZURE_SQL_PWD"),
        port     = 1433,
        Encrypt  = "yes",
        TrustServerCertificate = "no",
        ConnectionTimeout = timeout_brug
      ),
      silent = TRUE
    )
    
    if (!inherits(con_try, "try-error")) {
      cat("‚úÖ Forbundet til Azure SQL p√• fors√∏g", fors√∏g, "\n\n")
      return(con_try)
    }
    
    if (fors√∏g == fors√∏g_max) stop("‚ùå Kunne ikke forbinde til Azure SQL efter ", fors√∏g_max, " fors√∏g.")
    Sys.sleep(delay_sec)
    fors√∏g <- fors√∏g + 1
  }
}

con <- connect_azure_retry()
on.exit(try(DBI::dbDisconnect(con), silent = TRUE), add = TRUE)


```

#### Indl√¶sning af r√• temperaturdata fra databasens RAW-lag

I dette trin indl√¶ses de r√• temperaturdata fra tabellen PBA01_Raw.fact_temp_dry_raw i databasen. Kun de n√∏dvendige kolonner, observationstidspunkt og temperaturv√¶rdi, udv√¶lges for at holde datas√¶ttet fokuseret og let at arbejde videre med. Antallet af indl√¶ste r√¶kker udskrives efterf√∏lgende som et simpelt kontrolpunkt, der bekr√¶fter, at data er hentet korrekt fra RAW-laget og st√•r klar til videre transformation.

```{r}

# ====================================================================
# Hent RAW temperaturdata fra PBA01_Raw.fact_temp_dry_raw
# ====================================================================
fact_temp_dry_raw1 <- DBI::dbReadTable(
  con,
  DBI::Id(schema = "PBA01_Raw", table = "fact_temp_dry_raw")
) |>
  dplyr::select(observed, value)

cat("Antal r√¶kker i PBA01_Raw.fact_temp_dry_raw:", nrow(fact_temp_dry_raw1), "\n\n"); str(fact_temp_dry_raw1)


```

![](images/2026-01-04_09h05_59.png)

#### Opdeling i dato og tid samt filtrering til kamprelevant tidsvindue

I dette trin opdeles observationstidspunktet i en separat dato- og tidskomponent, hvorefter data filtreres til tidsrummet kl. 08:00‚Äì22:00. Dette interval afspejler et bevidst forretningsvalg, hvor temperaturer uden for typiske publikums- og kamprelevante tidspunkter udelades. Samtidig omd√∏bes og struktureres kolonnerne til et mere forretningsn√¶rt format, som er direkte egnet til videre analyse og joins.

```{r}

# ====================================================================
# Dato, tid og filter 08‚Äì22 (forretningslogik)
# ====================================================================
fact_temperatur_dmi <- fact_temp_dry_raw1 |>
  dplyr::mutate(
    obs_dato_raw = as.Date(observed),
    tid          = format(observed, "%H:%M:%S")
  ) |>
  dplyr::filter(
    tid >= "08:00:00",
    tid <= "22:00:00"
  ) |>
  dplyr::transmute(
    obs_dato   = format(obs_dato_raw, "%d-%m-%Y"),
    tid        = tid,
    temperatur = value
  )

cat("Antal r√¶kker efter filter 08‚Äì22:", nrow(fact_temperatur_dmi), "\n\n"); view(fact_temperatur_dmi)


```

![](images/2026-01-04_09h07_02.png)

#### Overblik over d√¶kningsperiode i temperaturdata

Dette trin giver et hurtigt overblik over, hvilke datoer der er repr√¶senteret i temperaturdatas√¶ttet. Ved at konvertere datoen til et korrekt Date-format og udtr√¶kke unikke v√¶rdier skabes et klart billede af dataperiodens omfang. Antallet af distinkte datoer bruges som et simpelt, men effektivt kontrolpunkt til at vurdere datad√¶kning og konsistens, inden de videre reduktioner og aggregeringer gennemf√∏res.

```{r}
# =============================================================================
#Distinct datoer (obs_dato som rigtig Date) ‚Äì hurtig overblik
# =============================================================================
distinct_datoer <- fact_temperatur_dmi |>
  dplyr::mutate(obs_dato_date = as.Date(obs_dato, format = "%d-%m-%Y")) |>
  dplyr::distinct(obs_dato_date) |>
  dplyr::arrange(obs_dato_date)

cat("Antal distincte datoer i temperaturdata:", nrow(distinct_datoer), "\n\n")


```

#### Reduktion til observationer p√• hele timer

I dette trin fjernes al minutbaseret granularitet, s√• datas√¶ttet udelukkende best√•r af observationer foretaget p√• hele timer. Form√•let er at forenkle datastrukturen og sikre ensartede tidsintervaller, som er mere stabile og lettere at arbejde med i efterf√∏lgende joins og analyser. Antallet af tilbagev√¶rende r√¶kker udskrives som kontrol for at dokumentere effekten af reduktionen.

```{r}
# ====================================================================
# Smid minut-data v√¶k (kun hele timer)
# ====================================================================
temp_heltimer <- fact_temperatur_dmi |>
  dplyr::filter(substr(tid, 4, 5) == "00")

cat("Antal r√¶kker efter filter til HELTIMER:", nrow(temp_heltimer), "\n\n")

```

#### Diagnostisk overblik over observationer pr. time

Dette trin anvendes som et diagnostisk kontrolpunkt, hvor antallet af temperatur¬≠observationer opg√∏res for hver hele time i tidsrummet kl. 08:00‚Äì22:00. Form√•let er at vurdere datad√¶kning og identificere eventuelle sk√¶vheder eller mangler i bestemte timer. Resultatet giver et hurtigt, kvantitativt overblik over, hvilke tidspunkter der er bedst repr√¶senteret i datas√¶ttet, f√∏r der tr√¶ffes endelige valg om tidsreduktion.

```{r}

# =============================================================================
# 7) Diagnose: observationer pr. HEL time (08‚Äì22)
# =============================================================================
obs_pr_time <- temp_heltimer |>
  dplyr::mutate(time_hour = as.integer(substr(tid, 1, 2))) |>
  dplyr::count(time_hour, name = "antal_observationer") |>
  dplyr::arrange(dplyr::desc(antal_observationer))

cat("--- Observationer pr. HEL time (sorteret) ---\n")
print(obs_pr_time)
cat("\n")

```

![](images/2026-01-04_09h10_15.png)

#### Bevidst valg af faste, kamprelevante tidspunkter

I dette trin fastl√¶gges de endelige tidspunkter, som temperaturdata reduceres til: kl. 09:00, 12:00, 15:00 og 18:00. Valget er metodisk og forretningsm√¶ssigt begrundet i √∏nsket om at repr√¶sentere temperaturforhold f√∏r og omkring kampafvikling. Tidspunktet kl. 21:00 frav√¶lges bevidst, da det typisk ligger efter kampens afslutning og derfor vurderes mindre relevant for analyse af tilskuere og kamprelaterede forhold.

```{r}
# ====================================================================
# FASTE tidspunkter (BEVIDST VALG): 09, 12, 15, 18 (ikke 21)
# ====================================================================
valgte_tidspunkter <- c("09:00:00", "12:00:00", "15:00:00", "18:00:00")

cat("Bevidst valgte tidspunkter:", paste(valgte_tidspunkter, collapse = ", "), "\n")
cat("Note: 21:00:00 frav√¶lges fordi det typisk er efter de fleste kampe.\n\n")


```

#### Reduktion af temperaturdata til udvalgte faste tidspunkter

I dette trin reduceres temperaturdatas√¶ttet til udelukkende at indeholde observationer fra de fire p√• forh√•nd fastlagte tidspunkter kl. 09:00, 12:00, 15:00 og 18:00. Form√•let er at skabe et kompakt og konsistent datas√¶t, som afspejler temperaturforhold f√∏r og omkring kampafvikling. Samtidig kontrolleres det eksplicit, at datas√¶ttet kun indeholder de √∏nskede tidspunkter, hvilket sikrer metodisk stringens og forudsigelighed i de efterf√∏lgende joins og analyser.

```{r}
# ====================================================================
#  Reduc√©r datas√¶ttet: behold kun de 4 tidspunkter
# ====================================================================
fact_temperatur_reduceret <- temp_heltimer |>
  dplyr::filter(tid %in% valgte_tidspunkter) |>
  dplyr::transmute(
    obs_dato   = obs_dato,
    tid        = tid,
    temperatur = temperatur
  )

cat("Antal r√¶kker efter reduktion til 4 tidspunkter:", nrow(fact_temperatur_reduceret), "\n\n")

cat("Distinct tider i reduceret tabel:\n")
print(fact_temperatur_reduceret |> dplyr::distinct(tid) |> dplyr::arrange(tid))
cat("\n")

```

![](images/2026-01-04_09h11_40.png)

#### Kvalitetssikring af datad√¶kning p√• valgte tidspunkter

Dette trin fungerer som et eksplicit kvalitetssikringscheck, hvor antallet af temperatur¬≠observationer opg√∏res for hvert af de valgte tidspunkter. Form√•let er at bekr√¶fte, at alle fire tidspunkter er konsistent repr√¶senteret i datas√¶ttet, og at der ikke er utilsigtede udfald eller sk√¶vheder i datad√¶kningen. Resultatet giver et klart overblik, som kan anvendes til at vurdere datas√¶ttets stabilitet f√∏r upload og videre anvendelse.

```{r}
# ====================================================================
#  QA: observationer pr valgt tidspunkt
# ====================================================================
qa_valgte <- fact_temperatur_reduceret |>
  dplyr::count(tid, name = "antal_observationer") |>
  dplyr::arrange(tid)

cat("--- QA: observationer pr valgt tidspunkt ---\n")
print(qa_valgte)
cat("\n")


```

![](images/2026-01-04_09h12_28.png)

#### Visuelt eftersyn af reduceret temperatur-FACT

Dette trin giver mulighed for et manuelt, visuelt eftersyn af den reducerede temperatur-FACT-tabel. Ved at inspicere datas√¶ttet direkte kan man hurtigt verificere, at struktur, datoer, tidspunkter og temperaturv√¶rdier stemmer overens med de metodiske valg, der er foretaget. Selvom trinnet er valgfrit, er det et nyttigt supplement til de automatiske QA-checks, da det kan afsl√∏re √•benlyse uregelm√¶ssigheder, inden data uploades og anvendes videre.

```{r}
# ====================================================================
#  VIEW: se den reducerede fact direkte (valgfrit men nyttigt)
# ====================================================================
View(fact_temperatur_reduceret, title = "fact_temperatur_reduceret (09/12/15/18)")


```

![](images/2026-01-04_09h13_35.png)

#### Diagnose af fuldst√¶ndighed p√• dato- og tidsniveau

Dette trin opbygger et s√¶rskilt diagnose-datas√¶t, der viser antallet af observationer for hver kombination af dato og valgt tidspunkt. Form√•let er at skabe fuld transparens omkring datad√¶kningen og g√∏re det muligt at identificere dage, hvor √©n eller flere af de faste tidspunkter mangler. Derudover udtr√¶kkes eksplicit en oversigt over datoer, som ikke har alle fire tidspunkter repr√¶senteret, hvilket giver et klart beslutningsgrundlag for eventuel efterbehandling eller metodisk dokumentation.

```{r}

# ====================================================================
#  Diagnose-datas√¶t: dato + tid + antal observationer 
# ====================================================================
obs_pr_dato_tid <- fact_temperatur_reduceret |>
  dplyr::mutate(
    obs_dato_date = as.Date(obs_dato, format = "%d-%m-%Y")
  ) |>
  dplyr::count(obs_dato_date, tid, name = "antal_obs") |>
  dplyr::arrange(obs_dato_date, tid)

cat("Diagnose-datas√¶t (dato+tid+antal_obs) klar ‚úÖ  R√¶kker:", nrow(obs_pr_dato_tid), "\n\n")
View(obs_pr_dato_tid, title = "DIAGNOSE: dato + tid + antal_obs")

# Ekstra: datoer der mangler mindst √©t af de 4 tidspunkter
mangler_pr_dato <- obs_pr_dato_tid |>
  dplyr::group_by(obs_dato_date) |>
  dplyr::summarise(
    antal_tidspunkter = dplyr::n_distinct(tid),
    .groups = "drop"
  ) |>
  dplyr::filter(antal_tidspunkter < 4) |>
  dplyr::arrange(obs_dato_date)

cat("Antal datoer der IKKE har alle 4 tidspunkter:", nrow(mangler_pr_dato), "\n\n")
View(mangler_pr_dato, title = "Datoer med manglende tidspunkter (<4)")

```

![](images/2026-01-04_09h14_51.png)

#### Robust upload af temperaturdata til Azure SQL

I dette trin uploades den reducerede temperatur-FACT-tabel til Azure SQL i laget PBA03_JoinReady ved hj√¶lp af en robust retry-mekanisme. Uploaden er designet til at h√•ndtere kendte og midlertidige forbindelsesproblemer, herunder 08S01 communication link failure, ved automatisk at gentage fors√∏get med korte pauser. Processen sikrer, at tabellen enten skrives korrekt til databasen eller fejler kontrolleret med en klar fejlmeddelelse, s√• dataintegriteten bevares og ETL-flowet afsluttes p√•lideligt.

```{r}

# ====================================================================
#  Upload til Azure ‚Äì robust retry (h√•ndter 08S01 / link failure)
# ====================================================================
upload_retry <- function(df, fors√∏g_max = 5, delay_sec = 10) {
  fors√∏g <- 1
  repeat {
    cat("Upload-fors√∏g", fors√∏g, "...\n")
    ok <- try(
      DBI::dbWriteTable(
        conn      = con,
        name      = DBI::Id(schema = "PBA03_JoinReady", table = "fact_temperatur_join_ready"),
        value     = df,
        overwrite = TRUE
      ),
      silent = TRUE
    )
    
    if (!inherits(ok, "try-error")) {
      cat("‚úÖ Upload gennemf√∏rt\n\n")
      break
    }
    
    if (fors√∏g >= fors√∏g_max) {
      stop("‚ùå Upload fejlede efter ", fors√∏g_max, " fors√∏g.\n\nFejl:\n", ok)
    }
    
    cat("‚ö†Ô∏è Upload fejlede ‚Äì pr√∏ver igen om", delay_sec, "sekunder\n\n")
    Sys.sleep(delay_sec)
    fors√∏g <- fors√∏g + 1
  }
}

upload_retry(fact_temperatur_reduceret)


```

#### Luk forbindelsen til SQL Azure

```{r}
# ====================================================================
# 15) Luk forbindelse
# ====================================================================
DBI::dbDisconnect(con)
cat("üîö Forbindelse til Azure SQL lukket.\n")


```
